<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian analysis | Doug Johnson</title>
    <link>https://academic-demo.netlify.app/tag/bayesian-analysis/</link>
      <atom:link href="https://academic-demo.netlify.app/tag/bayesian-analysis/index.xml" rel="self" type="application/rss+xml" />
    <description>Bayesian analysis</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jan 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hua7e188933fe49dd53608be3028a25685_18646_512x512_fill_lanczos_center_3.png</url>
      <title>Bayesian analysis</title>
      <link>https://academic-demo.netlify.app/tag/bayesian-analysis/</link>
    </image>
    
    <item>
      <title>Stan vs PyMC3 vs Bean Machine</title>
      <link>https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I have been a light user of Stan and RStan for some time and while there are a lot of things I really like about the language (such as the awesome community you can turn to for support and ShinyStan for inspecting Stan output) there are also a few things that I find frustrating. My biggest gripes with Stan are…&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Installation can be buggy&lt;/strong&gt; – I think that every time I have installed RStan I have encountered some weird installation error. In some cases, the fix was an easy google search away but in other cases it took a lot more time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model compilation is frustratingly slow&lt;/strong&gt; – It generally takes quite a while (a few minutes) to compile a model in Stan. I find this frustrating because I often make really stupid mistakes that only get caught when a model is compiled or run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging is tough&lt;/strong&gt; - In some ways, a Stan model is a bit of a black box – you define your model, feed it some data, and out pops samples from the posterior (or the MAP or whatever). This makes it a bit tough to debug models, especially complicated ones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The documentation is comprehensive but dense&lt;/strong&gt; – The Stan documentation (the user manual, reference guide, and tutorials) are very comprehensive but not great for learning the language. Now that I have mastered the basics, this is less of a concern, but it still takes me quite a bit of time to find answers to my Stan questions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the release of Bean Machine a couple of weeks ago I figured it is high time I checked out other probabilistic programming languages (PPLs) so I attempted to fit a simple item response theory model in all 3 languages. I have included the raw code for all three languages at the end (you can find a Google colab notebook for the PyMC3 model &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt; and a Google colab notebook for the Bean Machine model &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt; and some quick thoughts about what I liked and disliked about PyMC3 and Bean Machine below. Despite the click-baity title, this isn’t intended to be a comprehensive comparison of the strengths and weaknesses of each language – that is a job for someone with a far better understanding of each language’s capabilities than me – but rather a quick summary of my impressions from trying to fit a simple model in each language.&lt;/p&gt;
&lt;div id=&#34;what-i-liked-and-disliked-about-pymc3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I Liked and Disliked about PyMC3&lt;/h2&gt;
&lt;p&gt;Overall, I really liked PyMC3. In fact, I liked it so much that in the future I think I will likely use PyMC3 rather than Stan for my modelling needs. Some of the things I really liked about the language include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Installation was extremely easy and no installation is required on Google Colab.&lt;/strong&gt; Installation of PyMC3 was very easy and I didn’t encounter any weird errors. Even better, it comes preinstalled on Google Colab!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sampling from the prior predictive is quick and easy which is really helpful for model checking.&lt;/strong&gt; In PyMC3, to sample from the prior predictive (i.e. the distribution you get if you sample from the priors and likelihood without considering the data) you simply add one line of code at the end of your model. I found this really useful for checking that there are no basic errors in my model code and that my priors pass a basic sniff test. By contrast, in Stan, sampling from the prior predictive requires more or less duplicating your model in the generated quantities block and, for reasons I don’t understand, is super slow.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The syntax is concise and intuitive.&lt;/strong&gt; PyMC3’s syntax is very similar to Stan except that you don’t have to declare your variables (variable types are inferred from their distributions) or your data (since the data comes from the python environment). I really like this since I often make stupid mistakes in the data and parameters blocks of Stan (e.g. confusing indices).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I haven’t spent enough time with PyMC3 to say much about the documentation but based on a quick glance it seemed kind of similar to that for Stan – i.e. very comprehensive but not great for those new to the language. Sampling speed was also very similar to Stan’s (though the fact that this can be done easily on Google Colab effectively saves me some time since I can’t do much else on my laptop while Stan is running and running Stan in the cloud is more of a pain that it is worth.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-i-liked-and-disliked-about-bean-machine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I Liked and Disliked about Bean Machine&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Update: After posting an issue on the Bean Machine Github repo, the Bean Machine team graciously took the time to figure out what was wrong with my implementation of the 3PL model in Bean Machine. The issue was that the dtype of the data that I was passing to the Bean Machine sampler was incorrect. I was passing a torch tensor of type int when Bean Machine was expecting a torch tensor of type float (since that is the output generated from the torch Bernoulli distribution). In the code below, I have commented out the incorrect line and replaced it with the new correct line.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I started this exercise thinking that I would probably like Bean Machine more than the other two languages. While it seems like Bean Machine has a lot of potential, I found it very hard to use despite spending far more time trying to learn Bean Machine than PyMC3. Some of the things I found challenging/disliked when using Bean Machine include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sampling each of the random variables separately is not possible (or, at least, I couldn’t figure out how to do it).&lt;/strong&gt; One of the reasons I was excited about Bean Machine at first is that, in Bean Machine, models are not monolithic black boxes but rather a set of collection of separately defined variables. Thus, I thought it would be really easy to first sample from the priors one by one and then sample from the likelihood – something I would find really useful for debugging. Unfortunately, I couldn’t figure out how to do this easily.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The syntax is a bit verbose.&lt;/strong&gt; In Bean Machine, each model variable requires a full function definition which means that there is a lot of cruft to sift through when looking at code. You also have to know a bit of Pytorch though with pytorch’s increasing popularity that is probably not a barrier for most people.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;del&gt;3. &lt;strong&gt;Inference seems to be a bit buggy.&lt;/strong&gt; I couldn’t get the NUTS or HMC samplers to work for my simple reference model despite the fact that, in theory, these should be well suited to the model and thus used plain old MH sampling instead.&lt;/del&gt; (See update above.)&lt;/p&gt;
&lt;p&gt;On the plus side, installation was super easy and I really liked the basic tutorials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-reference-model-the-3-paramater-logistic-item-response-theory-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Reference Model – The 3 Paramater Logistic Item Response Theory Model&lt;/h2&gt;
&lt;p&gt;Since I have been playing around a lot with learning assessment data, I used a 3 parameter logistic item response theory model (3PL) as . The 3PL model is often used when you have dichotomous response data for a set of students – i.e. for each student and question combination you have an indicator for whether the student got the question right. I use a sample dataset provided by Stata. The 3PL model assumes that the probability that student j gets answer k correct is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Pr(y_{kj}=1 |\theta_j)= c+(1-c)\frac{1}{1+e^{-(a_k(\theta_j-b_k))}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where c is a parameter which accounts for the fact that even if a student guesses there is some probability that they will get the answer right, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is the ability level for student j, &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; is the difficulty of question k, and &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt; is a parameter for how well question k discriminates between low and high ability students.&lt;/p&gt;
&lt;p&gt;I first fit this model in Stata using maximum likelihood (which I think uses some sort of EM algorithm under the hood) and generate predicted abilities for each student in the dataset. I use the estimates from Stata to inform the priors for the full Bayesian model and as a sanity check on the model output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;use http://www.stata-press.com/data/r14/masc1.dta
irt 3pl q1-q9
predict theta_hat, latent&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an aside, you might be wondering why you would want to bother with a PPL given that using Stata and an ML approach is so easy. IMO, you probably don’t. There are a few situations where using a PPL for IRT might be useful though. First, you might want to fit a model for which there is no existing software package (e.g. some fancy new model which incorporates student response time). Second, there are some instances where you might want to use a Bayesian rather than frequentist approach. For example, Zajonc and Das use a Bayesian approach to IRT to compare the full distribution of student learning outcomes in India versus other countries. The authors couldn’t have done this using a maximum likelihood approach.&lt;/p&gt;
&lt;p&gt;When fitting the model using the PPLs, I use the following priors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta_j \sim N(0,1);  a_k \sim Lognormal(.5,1);  b_k \sim N(0,10); c \sim Beta(3,30) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The prior on &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is standard for most IRT models. The prior on a is borrowed from the EdStan R package The prior on b is very diffuse and may be considered more or less an uninformative prior. The prior on c is tightly centered around the ML estimate of c from the Stata output (which helps ensure that the models fit quickly).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stan + R Code&lt;/h2&gt;
&lt;p&gt;Stan code for the 3PL model is included below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse); library(rstan)
df &amp;lt;- haven::read_dta(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)

# add column for student number
df[&amp;quot;student&amp;quot;] &amp;lt;- 1:nrow(df)

# reshape to long format
df &amp;lt;- df %&amp;gt;% pivot_longer(cols = -student, names_to = &amp;quot;question&amp;quot;, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(question = as.numeric(str_remove(question, &amp;quot;q&amp;quot;)))

stan_code &amp;lt;- &amp;quot;
data {
  int&amp;lt;lower=1&amp;gt; J;             // number of respondents
  int&amp;lt;lower=1&amp;gt; K;             // number of items
  int&amp;lt;lower=1&amp;gt; N;             // number of observations
  int&amp;lt;lower=1,upper=J&amp;gt; jj[N]; // respondent for observation n
  int&amp;lt;lower=1,upper=K&amp;gt; kk[N]; // item for observation n
  int&amp;lt;lower=0,upper=1&amp;gt; y[N];  // score for observation n
}
parameters {
  vector[J] theta;             // ability for student j
  vector[K] b;                 // difficulty of question k
  vector&amp;lt;lower=0&amp;gt;[K] a;        // discriminating param for question k
  real&amp;lt;lower=0,upper=1&amp;gt; c; // guessing param
}
model {
  vector[N] eta;
  
  // priors
  theta ~ normal(0, 1); // Typical assumption for theta.
  b ~ normal(0, 10); // Diffuse prior for b. Same as that used by edstan
  a ~ lognormal(0.5, 1); // Diffuse prior. Same as that used by edstan
  c ~ beta(3, 30); // Tight prior around the estimated value from Stata output
  
  // model
  for (n in 1:N) {
    eta[n] = c + (1 - c) * inv_logit(a[kk[n]] * (theta[jj[n]] - b[kk[n]]));
  }
  y ~ bernoulli(eta);
}
&amp;quot;
stan_dat &amp;lt;- list(J = max(df$student), 
                 K = max(df$question), 
                 N = nrow(df),
                 jj = df$student,
                 kk = df$question,
                 y = df$y)

fit_stan &amp;lt;- stan(model_code = stan_code, data = stan_dat)
print(fit_stan)
samples &amp;lt;- extract(fit_stan)
theta_mean &amp;lt;- colMeans(samples$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pymc3-python-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PyMC3 + Python Code&lt;/h2&gt;
&lt;p&gt;Code to fit the 3PL model in PyMC3 is included below. You can find a Google Colab notebook with this code &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pymc3 as pm
import numpy as np
import pandas as pd

# import and reshape the data
df= pd.read_stata(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)
df[&amp;#39;student&amp;#39;] = np.arange(len(df))
df = df.melt(id_vars = &amp;#39;student&amp;#39;, value_name = &amp;#39;response&amp;#39;)
df[&amp;#39;question&amp;#39;] = df[&amp;#39;variable&amp;#39;].str[1:].astype(int) - 1

# Generate lists to use as indices
num_students = df[&amp;quot;student&amp;quot;].unique().shape[0]
num_items = df[&amp;quot;question&amp;quot;].unique().shape[0]
item_index = df[&amp;quot;question&amp;quot;].tolist()
student_index = df[&amp;quot;student&amp;quot;].tolist()

# Fit the model in python
with pm.Model() as irt_model:
  # Priors
  theta = pm.Normal(&amp;quot;theta&amp;quot;, mu = 0, sigma = 1, shape = num_students)
  a = pm.Lognormal(&amp;quot;a&amp;quot;, mu = 0.5, sigma = 1, shape = num_items)
  b = pm.Normal(&amp;quot;b&amp;quot;, mu = 0, sigma = 10, shape = num_items)
  c = pm.Beta(&amp;quot;c&amp;quot;, alpha = 3, beta = 30)

  # Likelihood
  eta = c + (1-c)*pm.invlogit(a[item_index]*(theta[student_index]-b[item_index]))
  response = pm.Bernoulli(&amp;quot;response&amp;quot;, p = eta, observed = df[&amp;#39;response&amp;#39;].values)

  # Inference
  posterior = pm.sample(draws = 1000, tune = 1000, return_inferencedata=True)

# Get EAP estimates for each theta_j by taking the mean of the posterior draws. 
theta_means = posterior.posterior[&amp;#39;theta&amp;#39;].mean(axis =1).mean(axis= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bean-machine-python-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bean Machine + Python Code&lt;/h2&gt;
&lt;p&gt;My attempt to fit the 3PL model in Bean Machine is below. You can find a Colab notebook with this code &lt;a href=&#34;https://colab.research.google.com/drive/1clOIXN5pgKtkzRIt2WvQ6s9q2FSbX6-g?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import beanmachine.ppl as bm
from beanmachine.ppl.model import RVIdentifier
import numpy as np
import pandas as pd
import torch
import torch.distributions as dist

# Download and reshape the dataset
df= pd.read_stata(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)
df[&amp;#39;student&amp;#39;] = np.arange(len(df))
df = df.melt(id_vars = &amp;#39;student&amp;#39;, value_name = &amp;#39;response&amp;#39;)
df[&amp;#39;question&amp;#39;] = df[&amp;#39;variable&amp;#39;].str[1:].astype(int) - 1

# Create lists to use as indices
num_students = df[&amp;quot;student&amp;quot;].unique().shape[0]
num_items = df[&amp;quot;question&amp;quot;].unique().shape[0]
item_index = df[&amp;quot;question&amp;quot;].tolist()
student_index = df[&amp;quot;student&amp;quot;].tolist()

# Fit the model
@bm.random_variable
def theta() -&amp;gt; RVIdentifier:
    return dist.Normal(0, 1).expand((num_students,))

@bm.random_variable
def a() -&amp;gt; RVIdentifier:
    return dist.LogNormal(0.5, 1).expand((num_items,))

@bm.random_variable
def b() -&amp;gt; RVIdentifier:
    return dist.Normal(0, 10).expand((num_items,))

@bm.random_variable
def c() -&amp;gt; RVIdentifier:
    return dist.Beta(3, 30)

@bm.functional
def p():
    return c()+ (1-c())*torch.sigmoid(a()[item_index]*(theta()[student_index]-b()[item_index]))

@bm.random_variable
def y() -&amp;gt; RVIdentifier:
    return dist.Bernoulli(p())
  
# Run the inference.
samples = bm.SingleSiteHamiltonianMonteCarlo(trajectory_length = 1).infer(
    queries=[theta()],
    # Old incorrect code
    # observations={y(): torch.tensor(df[&amp;quot;response&amp;quot;].values)},
    # New correct code
    observations={y(): torch.tensor(df[&amp;quot;response&amp;quot;].astype(&amp;#39;float&amp;#39;).values)},
    num_samples=2000,
    num_chains=4,
    num_adaptive_samples=2000
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating seroprevalence with data from an imperfect test on a convenience sample</title>
      <link>https://academic-demo.netlify.app/post/estimating-seroprevalence-with-data-from-an-imperfect-test-on-a-convenience-sample/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/estimating-seroprevalence-with-data-from-an-imperfect-test-on-a-convenience-sample/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;Update Jan, 2022: Since this post was published in May 2020, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanBayesianAnalysisTests2020&#34; role=&#34;doc-biblioref&#34;&gt;Gelman and Carpenter&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanBayesianAnalysisTests2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; have published a more comprehensive analysis on how to adjust for test imperfections using a Bayesian approach which goes beyond many of the ideas here. I recommend checking out their article.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Several recent studies have used data from antibody tests performed on a convenience sample to estimate seroprevalence of COVID-19 in a population. Estimating seroprevalence from this type of data presents two challenges. First, the analyst must take steps, through weighting or other measures, to deal with likely sample selection bias. Second, the analyst must take into account imperfections in the test itself. Addressing either of these challenges on their own is relatively straightforward to do using existing tools but addressing both at the same time is pretty tricky.&lt;/p&gt;
&lt;p&gt;In this blog post, I first describe the most commonly used tools for adjusting for test imperfections and performing inference on a convenience sample. I then describe two different ways of tackling both of these issues at once: a simple approach which sequentially applies the two simple approaches and a more complicated, but also more theoretically sound, Bayesian approach. I also report results from a quick Monte Carlo experiment I used to assess both approaches. A companion git &lt;a href=&#34;https://github.com/dougj892/sero_prevalence2&#34;&gt;repo&lt;/a&gt; includes code that (hopefully) can be relatively easily adapted to estimate seroprevalence from other studies using these approaches.&lt;/p&gt;
&lt;p&gt;The TLDR version of the results is that the naive approach seems to work fine for estimating overall population prevalence but that you should use the more sophisticated approach when generating estimates for subgroups.&lt;/p&gt;
&lt;div id=&#34;the-rogan-and-gladen-adjustment-to-account-for-test-imperfections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Rogan and Gladen Adjustment to Account for Test Imperfections&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-roganEstimatingPrevalenceResults1978&#34; role=&#34;doc-biblioref&#34;&gt;Rogan and Gladen&lt;/a&gt; (&lt;a href=&#34;#ref-roganEstimatingPrevalenceResults1978&#34; role=&#34;doc-biblioref&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt; developed a simple approach to adjust estimates of prevalence that takes into account test imperfections. If we define &lt;span class=&#34;math inline&#34;&gt;\(pt\)&lt;/span&gt; to be the true positive rate, &lt;span class=&#34;math inline&#34;&gt;\(se\)&lt;/span&gt; to be the test sensitivity (i.e. true positive rate), and &lt;span class=&#34;math inline&#34;&gt;\(sp\)&lt;/span&gt; to be the test specificity (i.e. the true negative rate), then the share of a population that will test positive is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ pa = se*pt+(1-sp)(1-pt) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Rogan and Gladen approach to adjusting for test imperfections solves for &lt;span class=&#34;math inline&#34;&gt;\(pt\)&lt;/span&gt; in this equation and replaces the true values of each parameter with their estimated values.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{pt} = \frac{\widehat{pa}+\widehat{sp}-1}{\widehat{se}+\widehat{sp}-1} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rogan and Gladen developed a few options for calculating standard errors of these estimates (and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-reiczigelExactConfidenceLimits2010&#34; role=&#34;doc-biblioref&#34;&gt;Reiczigel, Földi, and Ózsvári&lt;/a&gt; (&lt;a href=&#34;#ref-reiczigelExactConfidenceLimits2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; developed more sophisticated confidence intervals). Rogan and Gladen’s simplest approach to estimating the standard errors (which I will use later) is to calculate the variance of &lt;span class=&#34;math inline&#34;&gt;\(\widehat{pt}\)&lt;/span&gt; using the formula for the variance of a proportion.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{Var({\widehat{pt}})}= \frac{\widehat{pa}(1-\widehat{pa})}{N(se+sp-1)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The R package “epiR” allows users to apply the Rogan and Gladen adjustment and calculate confidence intervals using a variety of approaches. (See the function “epi.prev” in this package.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-a-convenience-sample-using-post-stratification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusting a Convenience Sample Using Post-Stratification&lt;/h2&gt;
&lt;p&gt;There are a lot of different ways to account for potential sample selection bias when analyzing data from a convenience sample but the simplest, and most commonly used, method is post-stratification. To apply post-stratification, we first divide up our samples into groups based on whatever demographic info we collected and calculate estimates for each group. We then weight the estimates for each of these groups according to the share of the total population that they represent. For example, if we sought to estimate the mean of some variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for the total population our estimate would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{\bar{y}} = \sum_{h=1}^H{\frac{N_h*\bar{y_h}}{N}} \]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y_h}\)&lt;/span&gt; is the estimate for group h, and &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_h}{N}\)&lt;/span&gt; is the share of group h in the total population from, for example, census data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-naive-approach-simple-poststratification-followed-by-a-rogan-gladen-adjustment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The naive approach: simple poststratification followed by a Rogan Gladen adjustment&lt;/h2&gt;
&lt;p&gt;The simplest approach to adjusting for both test imperfections and potential sample selection bias arising from convenience sampling is to first estimate the apparent prevalence rate (without accounting for test imperfections) using poststratification and then apply the Rogan and Gladen adjustment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;issues-with-the-naive-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Issues with the naive approach&lt;/h2&gt;
&lt;p&gt;In theory, the naive approach shouldn’t work too well. To see why this is the case, suppose you have two strata of equal sample size but one stratum represents a much larger portion of the population than the other strata (i.e. if you were to use weights, the weights for observations from this stratum would be much higher than observations from other strata). Suppose also that true prevalence is very low. Due to random test error, you will likely have some false positives in your sample. If you happen to get a false positive in the stratum with high weights, then the naive approach will lead you to overestimate the overall true prevalence. On average, your estimate of the true prevalence will be Ok but it (in theory) will have pretty high variance. (I caveat these claims with the phrase “in theory” since, as we will see below, for the simulated data I create it isn’t actually that much of a problem.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-bayesian-approach-using-modified-mrp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Bayesian Approach using Modified MRP&lt;/h2&gt;
&lt;p&gt;Theoretically, we should be able to improve on this approach by more carefully taking into account potential test imperfections. To use the example from above, if we saw that there was one positive test in the highly weighted stratum and 0 positive tests in the other stratum, we should adjust downward our overall estimate of the prevalence.&lt;/p&gt;
&lt;div id=&#34;quick-overview-of-mrp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quick overview of MRP&lt;/h3&gt;
&lt;p&gt;One way to do this is using a fully Bayesian approach built on multi-level regression and post-stratification (MRP). (For another Bayesian approach to this problem which doesn’t use MRP, see &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.04.15.20067066v1&#34;&gt;Larremore et al (2020)&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;MRP is an approach to small area estimation in which the analyst first estimates the mean of each strata using a multi-level model and then weights up these estimates using the poststratification weights. For example, to estimate the overall proportion &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in a population using data &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for each individual, you might use a simple model as follows to first estimate, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, the proportion in each stratum j using stratum variables &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta_j =logit^{-1}(X_j\beta); \beta\sim MVN(0,\sigma I); y_i \sim bernoulli(\theta_{j[i]}) \]&lt;/span&gt;
To derive your estimates of the total population, you just weight up. i.e. you calculate&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{\theta} = \sum_{h=1}^H{\frac{N_h*\widehat{\theta_j}}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;MRP is especially useful when you have a lot of different strata (which is often the case) since it allows you to more effectively “borrow strength” between strata compared to the approach where you simply model a different intercept for each stratum. (If you are simply modeling a separate intercept for each stratum, then there is no way for the model to know, for example, that a stratum for white men between 41 and 45 in Georgia and a stratum for white men between 46 and 50 are likely to be similar.) It is also, believe it or not, relatively straightforward compared to other approaches to small area estimate. For a more thorough overview of MRP, I highly recommend this &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/mrp.html&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modified-mrp-to-account-for-test-imperfections&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modified MRP to account for test imperfections&lt;/h3&gt;
&lt;p&gt;If implementing MRP using a Bayesian approach, it is fairly straightforward to modify the MRP model to take into account test error. As before, we use a multilevel model for the likelihood of the true prevalence. But in our likelihood of the test data, we use the apparent prevalence rate, which is the probability of a test being positive taking into account both prevalence and test imperfections, rather than the true prevalence. Lastly, we also model uncertainty in our estimates of the sensitivity and specificity using data on the number of true positives (tp), true negatives (tn), false positives (fp), and false negatives (fn) from a validation study of the antibody test.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ pt_j =logit^{-1}(X_j\beta); \beta\sim MVN(0,\sigma I)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ pa_j = se*pt_j+(1-sp)*(1-pt_j) \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ se \sim binom(tp, tp+fn); sp \sim binom(tn, tn+fp)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim bern(pa_{j[i]}) \]&lt;/span&gt;
For a complete Bayesian model, we also need to add priors for sensitivity and specificity.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results-from-a-monte-carlo-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results from a Monte Carlo Simulation&lt;/h2&gt;
&lt;p&gt;Theory is all well and good, but how do the two approaches compare when using data? To test this, I ran a simple Monte Carlo simulation using code borrowed from Kennedy and Gabry’s &lt;a href=&#34;https://cran.r-project.org/web/packages/rstanarm/vignettes/mrp.html&#34;&gt;MRP tutorial&lt;/a&gt;. (And big thanks to them for letting me copy their code!)&lt;/p&gt;
&lt;p&gt;Surprisingly, the naive approach actually did slightly better (in terms of average absolute deviation from the true seroprevalence) when it came to estimating overall seroprevalence. This is especially surprising since the data generating process used for the simulations is almost identical to my MRP model. The modified MRP process does much better when estimating subgroups (the Rogan and Gladen estimates for subgruops are often negative, which happens sometimes) but clearly, given the additional hassle of generating the code, the modified MRP approach is only worth it if you really want to estimate subgroup effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for-people-interested-in-using-this-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For people interested in using this code&lt;/h2&gt;
&lt;p&gt;All code for this analysis can be found &lt;a href=&#34;https://github.com/dougj892/sero_prevalence2&#34;&gt;here&lt;/a&gt;.
If you looking to copy and adapt the code, start with the R notebook “Estimate seroprevalence” in the above repo. In that notebook, I fit the modified MRP approach in two different ways: using raw Stan code and using the brms package (with some custom code to extend the package). If you would like to use the more complicated modified MRP approach, I strongly recommend you use the brms package. If you use the brms package, you should be able to copy and paste the code I created to define a “custom family” for the brms package and then modify the code in the main call to brm to suite your data. Since brms uses the lme4 syntax for defining multi-level models, customizing this code hopefully shouldn’t be too hard. By contrast, I find that modifying raw Stan code always takes quite a bit of time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gelmanBayesianAnalysisTests2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, Andrew, and Bob Carpenter. 2020. &lt;span&gt;“Bayesian Analysis of Tests with Unknown Specificity and Sensitivity.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 69 (5): 1269–83. &lt;a href=&#34;https://doi.org/10.1111/rssc.12435&#34;&gt;https://doi.org/10.1111/rssc.12435&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-reiczigelExactConfidenceLimits2010&#34; class=&#34;csl-entry&#34;&gt;
Reiczigel, J., J. Földi, and L. Ózsvári. 2010. &lt;span&gt;“Exact Confidence Limits for Prevalence of a Disease with an Imperfect Diagnostic Test.”&lt;/span&gt; &lt;em&gt;Epidemiology and Infection&lt;/em&gt; 138 (11): 1674–78. &lt;a href=&#34;https://doi.org/10.1017/S0950268810000385&#34;&gt;https://doi.org/10.1017/S0950268810000385&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-roganEstimatingPrevalenceResults1978&#34; class=&#34;csl-entry&#34;&gt;
Rogan, Walter, and Beth Gladen. 1978. &lt;span&gt;“Estimating &lt;span&gt;Prevalence&lt;/span&gt; from the &lt;span&gt;Results&lt;/span&gt; of a &lt;span&gt;Screening Test&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 107 (1): 71–76. &lt;a href=&#34;https://doi.org/10.1093/oxfordjournals.aje.a112510&#34;&gt;https://doi.org/10.1093/oxfordjournals.aje.a112510&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Alternative Approach to Power Calculations</title>
      <link>https://academic-demo.netlify.app/post/an-alternative-approach-to-power-calculations/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/an-alternative-approach-to-power-calculations/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The typical approach to power calculations goes something like this: first, the evaluator estimates the smallest MDE for which the intervention would be cost-effective. Second, the evaluator calculates the sample required to detect that MDE. Third, the evaluator throws out the calculations from step two after realizing the evaluation would be way too expensive and instead estimates the MDE she can detect given her budget constraints.
The problem with this approach is that it doesn’t take into account the cost of the evaluation itself. In this blog post, I’ll show one way to design evaluations which takes into account the cost of the evaluation itself through the use of a simple toy example&lt;/p&gt;
&lt;div id=&#34;simple-example-with-a-perfectly-accurate-study&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple example with a perfectly accurate study&lt;/h1&gt;
&lt;p&gt;Suppose a funder has USD 1 million to spend and two options for how it can spend the money: it can either invest in a tried and true intervention which it knows will save 1,000 lives or it can invest in a new intervention. The funder believes that there is a 10% chance that the new intervention is significantly more effective than the tried and true intervention and that USD 1 million invested in the new intervention would save 5,000 lives. Unfortunately, they also believe that there is a 90% probability that the new intervention is less effective than the tried and true intervention and would only save 500 lives.
If the funder seeks to maximize expected lives saved, they would invest the entire USD 1 million in the tried in true intervention since the expected number of lives saved by investing in the new intervention is only $ .1&lt;em&gt;5000+.9&lt;/em&gt;500=950 $&lt;/p&gt;
&lt;p&gt;Now suppose the funder has the option of first funding a study which would reveal, with perfect accuracy, whether the new intervention is a block-bluster or a dud. To calculate the value of the study, we may estimate the “value of information” for the study defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(I)=\sum_i P(i)EU(i)-EU \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where I is the new information which we may obtain and EU(i) is the expected utility when I=i, P(i) is probability I=I, and EU is the expected utility without I (Kochenderfer, 2015). In other words, the value of information for a variable I is the increase in expected utility if that variable is observed.
In our case:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(study)=.1*5000+.9*1000-1000 = 400 \]&lt;/span&gt; lives&lt;/p&gt;
&lt;p&gt;If we additionally assume that the cost per life saved is constant for the two interventions (i.e. if you spent Y on the first intervention you would save Y/1,000,000*1000 lives), we may calculate the exact amount the funder would be willing to spend on the study. Assuming constant cost per life saved, the funder would be willing to spend up to $285,714 on this study. Note that up until this point we have not used Bayes’ theorem at all – just some simple algebra.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-the-assumption-of-perfect-accuracy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Removing the Assumption of Perfect Accuracy&lt;/h1&gt;
&lt;p&gt;Unfortunately, studies are never perfectly accurate. Suppose that if the intervention is a blockbuster there is a 95% probability that the study will say that it is a block buster. But if the intervention is a dud, there is still a 5% probability that the study will say that it is a blockbuster.&lt;br /&gt;
To calculate VOI for this new noisy study, we first calculate the probability that the study result is “blockbuster”. This probability is .9&lt;em&gt;.05+.1&lt;/em&gt;.95=.14. Next, we need to calculate the expected utility if the study result is positive and the expected utility if the result is negative. Here’s where things get a little trickier and where Bayes’ rule comes in handy. If we get a “blockbuster” result, our post-facto estimate of the probability that the intervention is really a blockbuster would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(bb|bb result)=(P(bb result |bb)*P(bb))/(P(bb result |bb)P(bb)+P(bb result|dud)P(dud))=.68 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if the funder gets a “blockbuster” result from the study it would invest in the new intervention since the expected lives saved would be &lt;span class=&#34;math inline&#34;&gt;\(.68*5000+.32*500=3560\)&lt;/span&gt;.
I won’t calculate p(bb | dud result) since intuitively it seems fairly obvious that the funder would not invest in the new intervention if the study gave a “dud” result and all we care about for the VOI formula is the expected utility for each study result.
The value of information (in terms of lives saved) is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(noisy study)=P(bb result)*EU(bb result)+P(dud result)*EU(dud result)-1000=1358.4 \]&lt;/span&gt;
Thus, the noise in the study results reduces the value of the study information by 41.6 lives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-complicated-effects-and-studies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More Complicated Effects and Studies&lt;/h1&gt;
&lt;p&gt;In the real world things are rarely binary: intervention effects and study estimates are typically continuous. For example, suppose we are investigating an intervention which seeks to reduce infant mortality. The funder probably doesn’t believe that the intervention is either a blockbuster or a dud. Rather, they probably believe that there is a small chance the intervention works great, a small chance is works well but not great, etc. Similarly, any potential study we perform on the intervention will spit out an estimated effect size and standard error rather than a simple up/down result. To calculate the value of information with continuous effect sizes / study results, we still apply the same formula as above but the calculations will get complicated very quickly so we will no longer be able to calculate things by hand. More on this later.&lt;/p&gt;
&lt;p&gt;[1]Kochenderfer, Mykel J. Decision making under uncertainty: theory and application. MIT press, 2015.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
