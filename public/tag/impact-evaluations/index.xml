<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Impact evaluations | Doug Johnson</title>
    <link>https://academic-demo.netlify.app/tag/impact-evaluations/</link>
      <atom:link href="https://academic-demo.netlify.app/tag/impact-evaluations/index.xml" rel="self" type="application/rss+xml" />
    <description>Impact evaluations</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 23 Aug 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hua7e188933fe49dd53608be3028a25685_18646_512x512_fill_lanczos_center_3.png</url>
      <title>Impact evaluations</title>
      <link>https://academic-demo.netlify.app/tag/impact-evaluations/</link>
    </image>
    
    <item>
      <title>Estimating the Impact of State Education Policies in India Using ASER and Synthetic Controls</title>
      <link>https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0738059321000626&#34;&gt;recent article&lt;/a&gt; Andres Parrado and I take a look at the two main sources of learning outcomes data in India – NAS and ASER – and conclude that a) NAS data is (most likely) completely unreliable and b) ASER data, while reliable, is a bit noisy.&lt;/p&gt;
&lt;p&gt;The implications for people considering using NAS data are pretty clear – you &lt;strong&gt;shouldn’t&lt;/strong&gt;. For ASER, the implications are a bit murkier. In the paper, we say that analysts should probably avoid using ASER data at the district level (which the ASER Centre already recommended) and that they should be careful about comparing year-on-year changes between states. One topic we didn’t really touch is whether ASER data can be used to evaluate the impact of state-level policies. This is a tough question to answer because it depends not just on the level of noise in the ASER data but also on the methodology used and other aspects of the data.&lt;/p&gt;
&lt;p&gt;In this blog post, I look at whether it is possible to estimate the impact of state education policies by applying the synthetic control method (SCM) to ASER data. SCM is a popular approach to estimating aggregate policies (i.e. policies which affect an entire state or other large area) when data is only available at the aggregate level (i.e. there is no household-level data) but there is data for several time periods before the policy was implemented – exactly the situation we find ourselves with ASER data. An advantage of SCM is that it is (sometimes) pretty clear when it won’t work. As I show in the post, it is very clear that SCM, or at least all the versions of SCM that I try, won’t work with ASER data. This is not to say that there is no other method out that could use ASER data to estimate the effect of state-level policies on learning outcomes. Perhaps &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34;&gt;this method&lt;/a&gt; or &lt;a href=&#34;https://www.jmlr.org/papers/volume19/17-777/17-777.pdf&#34;&gt;this one&lt;/a&gt;, which further refine SCM, would perform better. But if a rigorous method well-suited to the data at hand doesn’t work it generally means that either you need more data or more assumptions (and thus less rigour).&lt;/p&gt;
&lt;p&gt;As with all of my posts, this entire post was written as a reproducible R notebook. Go to the about page for more information on how to access the code.&lt;/p&gt;
&lt;div id=&#34;how-not-to-use-aser-to-estimate-the-impact-of-a-policy-the-example-of-bcg-in-haryana&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How Not to Use ASER to Estimate the Impact of a Policy – the example of BCG in Haryana&lt;/h1&gt;
&lt;p&gt;Before diving into SCM, I first look at an example of how &lt;em&gt;not&lt;/em&gt; to use ASER code to estimate the effect of a state-level policy. My example comes from BCG’s work with Haryana on the Quality Improvement Programme (QIP), an ambitious project to improve learning in the states’ government schools through a variety of measures including monthly student assessments, teacher training, school consolidation, and leadership training. See &lt;a href=&#34;https://www.business-standard.com/article/pti-stories/haryana-to-start-quality-improvement-programme-in-schools-114050800695_1.html&#34;&gt;here&lt;/a&gt; for details. (I apologize for picking on BCG a bit in this post. BCG just did what we all do – paint our own efforts in the best light possible.) In &lt;a href=&#34;https://www.bcg.com/industries/education/transforming-education-on-massive-scale&#34;&gt;two&lt;/a&gt; &lt;a href=&#34;https://www.bcg.com/publications/2017/education-social-impact-breakthrough-education-reform-in-india&#34;&gt;articles&lt;/a&gt; on their website, BCG, the strategic partner on the QIP project, claim that QIP was a “breakthrough reform” that led to “stellar gains” and that “Haryana is now the only state in India—possibly globally—to improve learning outcomes at scale so quickly.” These claims are based largely on ASER data. According to one of the articles:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The impact of the reforms in Haryana has exceeded expectations. From 2012 through 2014, as the overhaul was being rolled out, the share of fifth graders who could do division increased 5% and the share who could read a standard second-grade text jumped 10%. That was quite a reversal: from 2010 through 2012, the share of fifth graders who could do division had fallen 26%, and the share of children who could read a standard second-grade text had dropped 17%. According to the National Achievements Survey report published in January 2016, Haryana was one of just two states in India that showed improvement in learning outcomes across all subjects, with 28 of the 30 Indian states posting declines or no change.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first issue with the BCG articles is that they mix data from ASER, NAS, and EI without clearly labeling the data sources. (The first article linked to above mixes data from all three sources in a single figure!) A second issue is that they compare Haryana with 5 other, seemingly arbitarily chosen states. A third more serious issue with the analysis that they claim that learning gains between 2012 and 2014 are due to QIP despite the fact that, by BCG’s own account, QIP started in 2014.&lt;/p&gt;
&lt;p&gt;If we look at ASER data (for all students) for Haryana from 2006 to 2018, it’s easy to see why they fudged the start date a bit. If we just look at the period between 2012 and 2014, the learning gains in Haryana do indeed look impressive! (Note that these figures are for all students, not just government school students. According to ASER, 42% of rural 6-14 year olds in Haryana attended private school in 2010 and 54% attended private school in 2014. Thus, restricting attention to government school students ignores a very large segment of the student population and makes it more difficult to compare changes over time (since the students exiting government schools are likely to be different from the ones staying).)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet, if we compare these gains with gains from other states, it’s unclear whether even with the fudged start date QIP clearly led to learning gains in Haryana. While ASER scores increased between 2012 and 2014 in Haryana, they increased just as much in other states too. Clearly, we need a more rigorous approach to estimating impact.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-synthetic-control-method-to-estimate-the-impact-of-qip&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the Synthetic Control Method to Estimate the Impact of QIP&lt;/h1&gt;
&lt;p&gt;The problem with eyeballing changes in state outcomes from one period to the next (as BCG does in their articles) is that it is very difficult to say when a change is likely due to a policy rather than noise or other factors. SCM offers a more rigorous approach to estimating the impact of state-level policies. SCM creates a “synthetic control” by looking for a combination of control states that, when summed up, closely matches the outcome trajectory of the treatment state prior to the intervention. (I am skimming over a lot of details here. For example, you can specify that the synthetic control matches the treatment unit on more than just pre-treatment outcomes. See &lt;a href=&#34;https://www.tandfonline.com/doi/pdf/10.1198/jasa.2009.ap08746&#34;&gt;Abadie, Diamond, and Hainmuller (2011)&lt;/a&gt; for a complete technical description of SCM. &lt;a href=&#34;https://economics.mit.edu/files/17847&#34;&gt;Abadie (2019)&lt;/a&gt; provides an excellent overview of the method and &lt;a href=&#34;https://dspace.mit.edu/handle/1721.1/71234&#34;&gt;Abadie et al (2011)&lt;/a&gt; provides clear guidance on how to implement SCM in R.)&lt;/p&gt;
&lt;p&gt;As mentioned above, an advantage of SCM is that it is often clear if it won’t work: if the synthetic control doesn’t closely match the trend of pre-treatment outcomes for the treatment unit then it clearly won’t provide a good approximation of the treatment unit post-treatment. (The converse is not necessarily true: even if the synthetic control matches the pre-intervention treatment unit trends closely it still may be a poor approximation for post-treatment outcomes.) In the code below, I attempt to estimate the effect of QIP on grade 5 reading levels using SCM, under the assumption that QIP started in 2012 and ended in 2014. (If you are viewing this from by blog, the code is hidden. See the about page for details on how to view the entire code.) The figure below shows grade 5 reading levels for the synthetic control versus Haryana for the entire time period for which we have data. (See code comments for model details such as what exact covariates I use.) Ideally, the lines for Haryana and the synthetic control should be close together until the onset of the “treatment” in 2012. Instead, we see that there are huge gaps prior to treatment – in some years, the gaps between the two lines are much large than the gap post-treatment (which is the estimated effect).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A second way to test the reliability of an SCM estimate is to calculate “placebo” effects for other states and compare the estimated effect for the treatment unit with the estimated “effect” for these other states. The figures below show the estimated “effect” for all states in 2013 and 2014. While the effect for Haryana is positive for both years the estimated effect in 2013 is smaller than the estimated placebo effect for 3 other states and smaller than the estimated placebo effect for 4 other states in 2014. (Note that this analysis assumes that there were no major education reform efforts in other states. I am not familiar with all of the reform efforts in other states, but I am not aware of any major education reform efforts in the states which have large “placebo” effects.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;applying-scm-to-other-states-and-outcomes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Applying SCM to Other States and Outcomes&lt;/h1&gt;
&lt;p&gt;The figure above shows that the synthetic control created for Haryana grade 5 reading outcomes using the model specification provided in the code gave a poor fit for the data. The figure below shows that this model gives a poor fit for nearly all states. The figure shows the difference between the synthetic control and the “treatment” state for all states for which we have complete ASER data over this time period (i.e. I ran the model just like I did for Haryana for each other state and then calculated the difference between the state’s grade 5 reading levels and the grade 5 reading levels for the synthetic control). Ideally, the lines would all be near 0 for the entire period (unless they launched a major education reform).&lt;/p&gt;
&lt;p&gt;These results are just for one outcome (grade 5 reading levels) and for one particular model specification but toying around with the model and outcome I couldn’t find any combination of model + outcome for which synthetic control seemed appropriate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Postscript&lt;/h1&gt;
&lt;p&gt;In late 2017, Haryana launched “Saksham Ghoshna,” an equally ambitious follow-up project to QIP which also included regular student assessments as well as new dashboards and several other pedagogic interventions. Ironically, one of the officials involved &lt;a href=&#34;https://yourstory.com/socialstory/2019/02/haryana-transformed-student-learning/amp&#34;&gt;claimed&lt;/a&gt; that the program was clearly successful because…learning outcomes had been going down prior to the project! In the officials words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If we look at NCERT’s various National Achievement Survey (NAS) and ASER reports, the surveys point out that the quality of school education in the state has been going down for years. Government school teachers in Haryana are well-qualified but somehow the link is missing. Classroom studies have not being meaningful. This was the initiation point.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evidence at USAID, part two</title>
      <link>https://academic-demo.netlify.app/post/increasing-evidence-use-at-usaid/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/increasing-evidence-use-at-usaid/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In my last &lt;a href=&#34;https://www.dougjohnson.in/post/evidence-based-programming-at-usaid/&#34;&gt;blog post&lt;/a&gt;, I shared some thoughts on Sarah Rose’s excellent recent &lt;a href=&#34;https://www.cgdev.org/publication/establishing-usaid-leader-evidence-based-foreign-aid&#34;&gt;working paper&lt;/a&gt; on how USAID could become more evidence based. TLDR: I argue that USAID doesn’t necessarily need to &lt;em&gt;generate&lt;/em&gt; evidence (since there are plenty of other orgs that do that) but it does need to the &lt;em&gt;use&lt;/em&gt; the latest and best evidence. Unfortunately, this is not always the case.&lt;/p&gt;
&lt;p&gt;Roses’ paper has several interesting recommendations for ensuring that USAID makes better use of evidence. She recommends that USAID a) consolidate the various entities responsible for evaluation, b) create a new cadre of “evidence brokers” responsible for digesting and communicating the latest evidence, and c) add rules to ensure greater attention to evidence in program design and procurement.&lt;/p&gt;
&lt;p&gt;These are all solid recommendations, but I think that they don’t get at the heart of the problem. In my opinion, the two biggest barriers to greater use of evidence are a) programming inertia and b) staff awareness of the latest evidence. First, there is a lot of inertia in major programming decisions. Much (most?) USAID funding is routed through large 5-year projects and, from what I can tell, when a project ends, it is typically replaced by a new 5-year project very similar in its overall design. For example, while at Abt Associates, most of my work was on the SHOPS project, a large 5-year project which focused on private sector health providers. Prior to the version of SHOPS that I worked on, there were 4 or 5 predecessors to SHOPS all of which did more or less the same thing. When I checked the Abt Associates website just now, it looks like there is yet another SHOPS project with more or less the same overall mission (though I think they have added “plus” to the name).&lt;/p&gt;
&lt;p&gt;Programming inertia like this means that there is little scope for evidence to inform many of the biggest funding decisions USAID makes. Some amount of inertia is probably a good thing: you wouldn’t want to make major changes to the overall distribution of funding every year. But USAID seems to err on the side of too much inertia. I don’t know what the best way to fix this is. Perhaps USAID should do a one-time overall review of all programming; perhaps it should change the way it decides which projects to pursue; or perhaps it should shift away from project-based funding overall. While the formal rules for how this stuff happens is well spelled out, I wasn’t involved in these decisions while at USAID and thus don’t know how these decisions are actually made in practice and thus don’t have too much insight into what should change. Nevertheless, it seems like something should change.&lt;/p&gt;
&lt;p&gt;The second barrier to greater use of evidence, in my opinion, is that staff simply don’t have the time or incentive to stay on top of the latest evidence. Even given the constraints of the projects they work on, USAID staff typically have a lot of flexibility in what gets funded or not. For example, on the SHOPS project mentioned above, there were dozens of sub-components to the project each of which our USAID officer could approve or reject at her discretion. Yet our AOR was typically so slammed that she had little time to devote to these decisions much less to do a thorough review of the evidence prior to making a decision.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is a tough problem to fix. Adding a rule or procedure is easy. Changing an organization’s culture so that staff have more time and are more inclined to stay on top of the latest evidence is far more difficult. I don’t have a solution to this, but here are a few ways USAID could move in this direction:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Slash the rules&lt;/strong&gt; &lt;a href=&#34;https://www.cgdev.org/sites/default/files/1424271_file_Natsios_Counterbureaucracy.pdf&#34;&gt;Andrew Natsios&lt;/a&gt; and &lt;a href=&#34;https://dash.harvard.edu/bitstream/handle/1/17467366/HONIG-DISSERTATION-2015.pdf?sequence=4&amp;amp;isAllowed=y&#34;&gt;Dan Honig&lt;/a&gt; have written much more eloquently than I could about how USAID staff are overloaded with rules and oversight Ironically, the desired intent of many of these rules is to ensure that programs are evidence-based and aligned with overall country strategy yet the cumulative effect, as Natsios and Honig show, is that technical officers are constantly busy complying with these rules and have little free time for field visits or to read the latest paper related to their sector. Many of these rules are outside the control of USAID. Yet there is one area where USAID could do much to reduce what Natsios calls the “obsessive measurement disorder”: it could reduce the ADS, particularly the program cycle ADS which specifies the rules staff must comply with when designing and carrying out programs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Increase the capacity of the Chief Economist’s office&lt;/strong&gt; When I worked at USAID, I found that nearly all of my colleagues were nice, congenial people. Overall, this is a very good thing. But, every once in a while, I wished that there was a curmudgeonly economist to call BS in a presentation I attended. In theory, the chief economist’s office could play this role – that is, it could help other offices digest and synthesize the latest evidence and, if necessary, call BS when a contractor’s claims are not backed up by data. In Rose’s terminology, the chief economist could serve as an “evidence broker” for the bureaus. To play that role, it would probably need a bigger budget and more staff though.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Minimize senior leaders’ pet projects&lt;/strong&gt; Senior USAID staff and politicians often have their own development pet projects which they are passionate about. Hilary Clinton was passionate about clean cook stoves. Ivanka Trump was passionate about women’s empowerment. These pet projects typically don’t take up a lot of money, but they take up a lot of staff time and foster the impression that evidence matters less than the whims of senior leaders/politicians.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Make hiring and promotion decisions more based on contributions to sector knowledge&lt;/strong&gt; Staying on top of the latest evidence takes a lot of time. Currently, there isn’t much of a professional incentive for staff to invest this time. Considering contributions to sector knowledge – such as blog posts, conference presentations, or papers – in hiring and promotion decisions would help correct this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evidence at USAID</title>
      <link>https://academic-demo.netlify.app/post/evidence-based-programming-at-usaid/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/evidence-based-programming-at-usaid/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sarah Rose at the Center for Global Development recently published an &lt;a href=&#34;https://www.cgdev.org/sites/default/files/establishing-usaid-as-an-evidence-leader.pdf&#34;&gt;excellent note&lt;/a&gt; on how to make USAID programming more evidence-based. As a former member of one of the groups mentioned in the article (the Evaluation and Impact Assessment group at the erstwhile Global Development Lab) and a long-time evaluator, this is a topic dear to my cold, data-driven heart. I realize that probably marks me as a member of very small fraternity, but people really should care more about making donors more evidence-based! As Abhijit Banerjee recently &lt;a href=&#34;https://www.npr.org/transcripts/909351607&#34;&gt;pointed out&lt;/a&gt;, funding from donors like USAID may make up a small share of overall development financing in most countries but, in contrast to domestic financing, is quite flexible. Politicians typically have little fiscal space after taking into account funding mandated by previous legislation and naturally seek to use this money for quick wins like new roads or bridges. Donors, not subject to those constraints, could use their money for higher impact projects like early child development or to figure out what programs are most cost effective over the long term. Unfortunately, for reasons we’ll get to, USAID doesn’t often do this.&lt;/p&gt;
&lt;p&gt;Back to Rose’s note. Rose argues that USAID has made significant progress in becoming more evidence-based over the past decade. She points out that USAID adopted a new &lt;a href=&#34;https://www.usaid.gov/evaluation/policy&#34;&gt;evaluation policy&lt;/a&gt; in 2011 and created several new units within the agency focused on evaluation. She goes on to cite two &lt;a href=&#34;https://www.usaid.gov/sites/default/files/documents/1870/Meta-Evaluation%20of%20Quality%20and%20Coverage%20of%20USAID%20Evaluations%202009-2012.pdf&#34;&gt;internal&lt;/a&gt; &lt;a href=&#34;https://pdf.usaid.gov/pdf_docs/pa00kxvt.pdf&#34;&gt;reviews&lt;/a&gt; and an external &lt;a href=&#34;https://www.gao.gov/assets/690/683157.pdf&#34;&gt;GAO review&lt;/a&gt; which found that, overall, USAID has done a decent job in executing the vision outlined in the evaluation policy. The first internal review, for instance, found that most evaluations are high-quality, relevant, and used to inform programming.&lt;/p&gt;
&lt;p&gt;Rose argues that despite these gains, there are still gaps in how USAID generates and uses evidence. To fill these gaps, Rose recommends that USAID:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Nominate an administrator who will champion evidence-based programming&lt;/li&gt;
&lt;li&gt;Create a new evidence and evaluation unit which consolidates the various groups within USAID responsible for evaluation work&lt;/li&gt;
&lt;li&gt;Hire or train a cadre of impact evaluation specialists to oversee impact evaluations and “evidence brokers” to digest, synthesize, and communicate results from new studies&lt;/li&gt;
&lt;li&gt;Build evidence use and generation into program design and procurement. This would mean that where evidence is weak, programs should seek to generate new evidence. Where evidence is strong, program design should take into account the latest evidence&lt;/li&gt;
&lt;li&gt;Develop new methods for faster, less expensive impact evaluation methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I agreed with much in Rose’s article, but I think she pulls way too many punches and disagree with some of her recommendations.&lt;/p&gt;
&lt;div id=&#34;the-state-of-evaluations-at-usaid&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The State of Evaluations at USAID&lt;/h1&gt;
&lt;p&gt;Rose’s assessment that USAID has become far more evidence-based in the past decade is a very charitable take. USAID may have superficially met many of the requirements of its own evaluation policy but, in my experience, these evaluations rarely serve any purpose other than to shuffle funding between DC and Maryland / Virginia. To give one concrete (though hopefully not representative) example: A friend of mine recently met with a USAID contractor to discuss potentially conducting an evaluation of one of their programs. As he was leaving the contractor’s office, the CEO pointed out that the CEO’s job, and the job of all the other employees he had met in the office, depended on my friend’s “independent” evaluation coming to the right conclusions. My friend didn’t take the consultancy but, if he did, it’s likely that his final evaluation report would have been deemed to be “high-quality, relevant, and useful” according to the first internal review cited above. To meet this bar, according to the review, the evaluation need only ensure that the executive summary “accurately reflects the most critical aspects of the report,” that the “basic characteristics of the program, project or activity are described” and a dozen other equally superficial criteria are met. None of the criteria attempt to ensure that the evaluation is truly independent or that the recommendations are not influenced by program implementers.&lt;/p&gt;
&lt;p&gt;Outright fraud of the type my friend encountered is (probably) rare but so are useful evaluations. Over the past decade or so, I have read hundreds of evaluations. (Yes, I should probably find better things to do with my time.) The best evaluations have shaped my thinking on what works and what doesn’t or provided helpful insight into what went right or wrong with a program. Excluding those evaluations commissioned by DIV, I can only recall a handle of evaluations that I found useful that were conducted by USAID or its partners. More worryingly, program managers often seem to ignore the latest evidence when designing programs. A recent &lt;a href=&#34;https://ssir.org/articles/entry/nowhere_to_grow&#34;&gt;SSIR article&lt;/a&gt; found that the big USAID contractors almost never scale-up proven solutions from smaller non-profits even in cases where rigorous evidence exists.&lt;/p&gt;
&lt;p&gt;As Rose points out, this isn’t really the fault of USAID staff who spend most of their time navigating the minutiae of federal regulations to get money out the door and then ensuring compliance with various reporting requirements once it is out. An additional challenge, not mentioned in Rose’s note, is that many in USAID senior leadership have a naïve understanding of what constitutes “evidence-based development.” Political appointees often come from business backgrounds, where measuring success (i.e. profits) is relatively easy, and have little experience in international development, where measuring impact is much, much harder. Faced with the task of prioritizing different programs, these political appointees often push staff to come up numbers on program impact (“metrics” in business-speak) that simply don’t exist. The result is a set of absurdly ambitious claims about “number of lives saved” and program staff with an understandable aversion to any further measurement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensuring-generation-of-more-high-quality-evidence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensuring Generation of More High-quality Evidence&lt;/h1&gt;
&lt;p&gt;How do we fix this situation? Many of Rose’s recommendation focus on increasing USAID’s capacity to conduct impact evaluations. Superficially, this makes sense – if USAID wants to be more evidence-based making sure that it generates more high-quality evidence seems like a good first step. Yet, to a first approximation, USAID doesn’t really &lt;em&gt;need&lt;/em&gt; to generate its own evidence. The type of programming USAID funds is very similar to the type of programming other donors fund and outside researchers already evaluate. Academics, who conduct the majority of impact evaluations, are incentivized to create interesting evidence not policy-relevant evidence but observers have probably made more of this difference it deserves and, in any case, this problem is not unique to USAID.&lt;/p&gt;
&lt;p&gt;Even if USAID has a unique evidence need, USAID staff probably aren’t the best people to lead an impact evaluation. Identifying opportunities for useful impact evaluations requires a deep understanding of the latest evidence in a sector and carrying out an impact evaluation requires a very specific set of skills. These tasks are best left to professors or outside organizations specializing in this work. (An added bonus of working with professors is that their time comes heavily subsidized.) This holds doubly true for efforts to develop new impact evaluation methodologies – these tasks are best left to much nimbler organizations.&lt;/p&gt;
&lt;p&gt;This is not to say that USAID should not be involved in impact evaluations. USAID funds a lot of interesting, cutting edge programs. Impact evaluations of these programs could potentially add a lot to the global evidence base. So while USAID doesn’t necessarily need to take the lead in evidence generation, it would be great if it could collaborate with outside researchers to ensure that the most innovative programs are evaluated for impact. Unfortunately, this is rarely the case. The decision of whether to subject a program to an impact evaluation typically falls to individual program managers. Thus, whether or not a program receives an impact evaluation is more dependent on an individual manager’s enthusiasm for impact evaluations in general than it is on the program’s suitability for an impact evaluation. In addition, since impact evaluations add administrative hassle to program implementation, only the most enthusiastic managers sign up for their programs to receive an impact evaluation.&lt;/p&gt;
&lt;p&gt;I’m not sure what the best way to fix this is, but a good start would be to modify the rule for when impact evaluations. USAID’s current evaluation policy states that all innovative programs should be subject to an impact evaluation (unless it is not possible to do so). If, instead, bureau heads would given a target of being involved in at at least 3 or so high-quality impact evaluations (where for lack of a better metric quality would be measured by where the results were published) bureaus would have an incentive to work with researchers to conduct useful impact evaluations rather than be forced to justify individual decisions on whether or not to evaluate the impact of a specific project. This is a bit similar to Rose’s recommendation to “focus impact evaluations more strategically.” Where I differ from Rose is that I don’t see a need for bureaus to plan out in advance what impact evaluations they will be involved in. For various reasons, it is often hard to plan impact evaluations too far in advance and, as mentioned above, I don’t think USAID staff are well positioned to identify gaps in the evidence base.&lt;/p&gt;
&lt;p&gt;Another promising fix would be to create a new internal fund dedicated to impact evaluations which could be mobilized rapidly and which would cover the cost of both the evaluation and any additional implementation hassle. This would allow bureaus and outside evaluators to rapidly respond to interesting impact evaluation opportunities and reduce the negative incentive for program managers to be involved in impact evaluations.&lt;/p&gt;
&lt;p&gt;Alternatively, USAID could just allocate more funding to DIV, perhaps with some slight changes to its mission. While USAID as a whole hasn’t done very well when it comes to impact evaluations, DIV has been a shining exception to this rule. DIV’s mission is not to fund impact evaluations per se but this has been a positive side effect of its work.
# Ensuring More Effective Use of Evidence&lt;/p&gt;
&lt;p&gt;Rose also makes several recommendations for how USAID could better use existing evidence. Here, I wholeheartedly agree with the overall aim. USAID doesn’t necessarily have to generate new evidence, but it absolutely does need to &lt;em&gt;use&lt;/em&gt; existing evidence. Rose’s key recommendations here are to a) consolidate the various entities responsible for evaluation, b) create a new cadre of “evidence brokers” responsible for digesting and communicating the latest evidence to other stuff, and c) add rules to ensure greater attention to evidence in program design and procurement.&lt;/p&gt;
&lt;p&gt;All of these recommendations make sense, but I have minor quibbles with each. My issue with the first recommendation is only that the timing absolutely sucks. In normal times, the first recommendation would make a lot of sense. Less than a year after an extremely painful reorganization though, yet another shuffling of official titles might just break the severely weakened will of many USAID staff. Still, this is probably something that should happen relatively soon.&lt;/p&gt;
&lt;p&gt;The second recommendation – to create a cadre of “evidence brokers” – is intriguing but would face many practical hurdles. Existing staff probably wouldn’t be too keen on the idea that they need “evidence brokers” to help them figure out which programs are most effective. Without any formal authority over programming decisions, these “evidence brokers,” no matter how talented, could easily be relegated to a cold dark room within RRB never to be seen or heard from. Still, this is an interesting approach and deserves further exploration.&lt;/p&gt;
&lt;p&gt;My lukewarm reaction to the third recommendation really comes down to the fact that it would add yet more rules to already overburdened bureaucrats.I would love to see program solicitations be more informed by evidence, yet the last thing program staff needs is yet another box to check when designing programs. The existing set of bureaucratic boxes related to program design, the ADS 201, is already over a hundred pages (of very dense type).&lt;/p&gt;
&lt;p&gt;Whew, that was much longer than I originally anticipated and I haven’t even gotten to my own recommendations for how to ensure USAID makes better use of existing evidence. That will have to wait for another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Response to Blattman&#39;s Post on Why What Works Is The Wrong Question</title>
      <link>https://academic-demo.netlify.app/post/blattman/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/blattman/</guid>
      <description>&lt;p&gt;Last week, Chris Blattman published a &lt;a href=&#34;http://chrisblattman.com/2016/07/19/14411/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;long blog post&lt;/a&gt; titled “Why ‘what works?’ is the wrong question: evaluating ideas not programs.”  In the blog post, which was adapted from a talk he gave at DFID, Blattman argues that a) impact evaluations should focus on deeper, theory-driven questions rather than just whether a program works or not and b) researchers should design impact evaluations to allow for generalizability by paying attention to context and running multiple evaluations in multiple contexts.&lt;/p&gt;
&lt;p&gt;There’s a lot to like in this post, but ultimately, it left me frustrated— not because I didn’t agree with the substance of the arguments, but because I think he squandered a great opportunity to push DFID in the direction of better evaluation.&lt;/p&gt;
&lt;p&gt;First, the bit I liked. Blattman’s argument that researchers should design impact evaluations with generalizability in mind struck a chord with me.  As Eva Vivalt and others have shown, context matters a lot for impact and extrapolating results from one context to another is really, really hard.  Thus, if your goal is the creation of general knowledge you should care just as much about external validity as you should about internal validity: an extremely rigorous result in one context is of no use if policymakers can’t figure out how the impact would like change if adapted to another context.  As Blattman points out, this implies a shift in how we go about doing impact evaluations that seek to create general knowledge.  Rather than one-off evaluations in one context, we should first think hard about context and then attempt to run multiple evaluations in multiple contexts all testing the same basic idea.&lt;/p&gt;
&lt;p&gt;Yet while I liked Blattman’s call for more attention to generalizability, I was disappointed that he didn’t more explicitly tell DFID how to make this happen.  Let me take a step back here and review how donors like USAID and DFID decide which programs to conduct an impact evaluation on (based on my, admittedly limited, experience).  First, they design a program. Then, if the person in charge of the program is a fan of impact evaluations, that person will set aside a portion of the budget to run an impact evaluation.  (There are exceptions to this rule, like DIV, but in general this is how it works.)&lt;/p&gt;
&lt;p&gt;In this context, calling for donors to conduct multiple trials in multiple places of the same idea is a bit of a pipe dream: an evaluator would have to have to simultaneously convince multiple program managers not only to participate in an impact evaluation but also to subjugate project design and scheduling considerations to the needs of the impact evaluation.  In my view, the way around this is for donors to clearly distinguish between evaluations whose main purpose is to improve the program being evaluated and evaluations whose main purpose is to create general knowledge.  (For more on that distinction, see &lt;a href=&#34;http://idinsight.org/idinsight-presents-at-3ie-evidence-week-on-the-future-of-impact-evaluation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.)  For the former, the existing method of leaving the evaluation decision up to program manager is, for the most part, fine.  In contrast, for the latter, donors should first identify the big questions that they want to answer and then identify which programs can help them answer these questions.&lt;/p&gt;
&lt;p&gt;Suggesting that an large bureaucracy should add yet another centralized, bureaucratic process is always a dangerous proposition but, in this case, I think it is necessary.  Currently, at donors like USAID and DFID  impact evaluations tend to be conducted on programs run by people who are sympathetic to impact evaluations rather than on programs for which there is little evidence or which should be a high priority for an evaluation for other reasons.  This not only means that the type of ambitious, multi-site trials that Blattman suggests are infeasible, but that most impact evaluations tend to be up-or-down assessments of large, complex programs which yield little in the way of useful results.  A more centralized system for identifying priority research questions to be address by KFEs would make these evaluations much more useful.&lt;/p&gt;
&lt;p&gt;Lastly, I was going to make one last point about Blattman’s call for more theory-driven evaluations, but this post is already getting way too long so will save that for another post!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
