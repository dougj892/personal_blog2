<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Doug Johnson</title>
    <link>https://academic-demo.netlify.app/</link>
      <atom:link href="https://academic-demo.netlify.app/index.xml" rel="self" type="application/rss+xml" />
    <description>Doug Johnson</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://academic-demo.netlify.app/media/icon_hua7e188933fe49dd53608be3028a25685_18646_512x512_fill_lanczos_center_3.png</url>
      <title>Doug Johnson</title>
      <link>https://academic-demo.netlify.app/</link>
    </image>
    
    <item>
      <title>Python basics</title>
      <link>https://academic-demo.netlify.app/courses/example/python/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/courses/example/python/</guid>
      <description>&lt;p&gt;Build a foundation in Python.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/rfscVS0vtbw&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the difference between lists and tuples?&lt;/summary&gt;
  &lt;p&gt;&lt;p&gt;Lists&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lists are mutable - they can be changed&lt;/li&gt;
&lt;li&gt;Slower than tuples&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_list = [1, 2.0, &#39;Hello world&#39;]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Tuples&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Tuples are immutable - they can&amp;rsquo;t be changed&lt;/li&gt;
&lt;li&gt;Tuples are faster than lists&lt;/li&gt;
&lt;li&gt;Syntax: &lt;code&gt;a_tuple = (1, 2.0, &#39;Hello world&#39;)&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Is Python case-sensitive?&lt;/summary&gt;
  &lt;p&gt;Yes&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Visualization</title>
      <link>https://academic-demo.netlify.app/courses/example/visualization/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/courses/example/visualization/</guid>
      <description>&lt;p&gt;Learn how to visualize data with Plotly.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hSPmj7mK6ng&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;When is a heatmap useful?&lt;/summary&gt;
  &lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit.&lt;/p&gt;
&lt;/details&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Write Plotly code to render a bar chart&lt;/summary&gt;
  &lt;p&gt;&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import plotly.express as px
data_canada = px.data.gapminder().query(&amp;quot;country == &#39;Canada&#39;&amp;quot;)
fig = px.bar(data_canada, x=&#39;year&#39;, y=&#39;pop&#39;)
fig.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Statistics</title>
      <link>https://academic-demo.netlify.app/courses/example/stats/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/courses/example/stats/</guid>
      <description>&lt;p&gt;Introduction to statistics for data science.&lt;/p&gt;
&lt;p&gt;
  &lt;i class=&#34;fas fa-clock  pr-1 fa-fw&#34;&gt;&lt;/i&gt; 1-2 hours per week, for 8 weeks&lt;/p&gt;
&lt;h2 id=&#34;learn&#34;&gt;Learn&lt;/h2&gt;
&lt;p&gt;The general form of the &lt;strong&gt;normal&lt;/strong&gt; probability density function is:&lt;/p&gt;
&lt;p&gt;$$
f(x) = \frac{1}{\sigma \sqrt{2\pi} } e^{-\frac{1}{2}\left(\frac{x-\mu}{\sigma}\right)^2}
$$&lt;/p&gt;
&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    The parameter $\mu$ is the mean or expectation of the distribution.
$\sigma$ is its standard deviation.
The variance of the distribution is $\sigma^{2}$.
  &lt;/div&gt;
&lt;/div&gt;

&lt;h2 id=&#34;quiz&#34;&gt;Quiz&lt;/h2&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;What is the parameter $\mu$?&lt;/summary&gt;
  &lt;p&gt;The parameter $\mu$ is the mean or expectation of the distribution.&lt;/p&gt;
&lt;/details&gt;</description>
    </item>
    
    <item>
      <title>Example Talk</title>
      <link>https://academic-demo.netlify.app/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Why Nothing Scales Doesn&#39;t Mean Nothing Works</title>
      <link>https://academic-demo.netlify.app/post/why-nothing-scales-doesn-t-mean-nothing-works/</link>
      <pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/why-nothing-scales-doesn-t-mean-nothing-works/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/why-nothing-scales-doesn-t-mean-nothing-works/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Jason Kerwin, a development economist at the University of Minnesota, wrote an interesting blog post some time back in which he argued that &lt;a href=&#34;https://jasonkerwin.com/nonparibus/2021/11/03/nothing-scales/&#34;&gt;nothing scales&lt;/a&gt;. Kerwin’s point was that we (i.e. the international development community) often get really excited about new, innovative interventions after some small pilot, scale them up, and then find that they don’t work nearly as well at a larger scale. Kerwin cited the examples of growth mindset training, warning young women about “sugar daddies” (see the blog post for more info), and the “Jamaican” model of early child home visitations.&lt;/p&gt;
&lt;p&gt;Lee Crawfurd, a researcher at the Center for Global Development, had a couple of great tweet threads in response to the Kerwin post. In the first thread, he added a bunch more examples of interventions which seemed really promising when implemented at a small scale but weren’t that impressive when scaled up.&lt;/p&gt;
&lt;p&gt;&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I enjoyed this blog by &lt;a href=&#34;https://twitter.com/jt_kerwin?ref_src=twsrc%5Etfw&#34;&gt;@jt_kerwin&lt;/a&gt; about how &amp;quot;nothing scales&amp;quot;. &lt;br&gt;&lt;br&gt;What other examples are there?&lt;a href=&#34;https://t.co/suD5WQPCIn&#34;&gt;https://t.co/suD5WQPCIn&lt;/a&gt;&lt;/p&gt;&amp;mdash; Lee Crawfurd (@leecrawfurd) &lt;a href=&#34;https://twitter.com/leecrawfurd/status/1461664274116452352?ref_src=twsrc%5Etfw&#34;&gt;November 19, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;
&lt;/p&gt;
&lt;p&gt;In a later, far less lengthy, thread he cited a few examples of things which do seem to scale well. I can’t seem to find the second thread but, from what I recall, the overall gist of the two threads combined was that “&lt;em&gt;almost&lt;/em&gt; nothing scales.”’&lt;/p&gt;
&lt;p&gt;This is definitely a huge bummer. At the same time, I don’t think it is as big a bummer as many other commenters seemed to think. Judging by some of the reactions I saw on twitter, it seemed like many people were quick to jump from “nothing scales” to “nothing works.” Yes, interventions which seem promising at a small scale often have disappointing effects at a larger scale but the model of development which goes from innovation to piloting + evaluation to scale is just one model of development. We’re so trained to think in terms of this innovation -&amp;gt; test -&amp;gt; scale cycle that it’s easy to forget that there are many other paths to change. Rather than coming up with some new program we can try to do a better job of implementing existing programs. Or we can lobby for better laws or regulations. Or perhaps the sum total of many scall scale efforts can lead to large scale impact. You could argue that if you increase government capacity or adopt some new policy you are, in some ways, taking &lt;em&gt;something&lt;/em&gt; to scale (e.g. a policy) but I think that is glossing over substantial differences with a bit of semantics. A lot of new policies aren’t fundamentally innovative or even tested and increases in government capacity, at least in my experience, are often the result of a bunch of small, hard to measure actions taken by inspired leaders.&lt;/p&gt;
&lt;p&gt;The problem is that all of this stuff is really, really hard to do especially if you are a foreign funder / multilateral. Most funders are reluctant to engage in policy advocacy much less activism. And increasing government capacity from the outside is kind of like building Ikea furniture – i.e. there is a correct way to do it but no one seems to know how. I think this is why many of us have been so drawn to the innovate -&amp;gt; test -&amp;gt; scale model: even as outsiders, we can usually come up with an test some new program and, if we use rigorous impact evaluation methods, we can get some certainty of whether it works at a small scale.&lt;/p&gt;
&lt;p&gt;I don’t have great ideas on how to overcome these challenges. That said, I think it is time that funders, especially smaller funders with more flexibility, paid more attention to these other potential paths to change. The development sector as a whole has spent an enormous amount of effort and resources on the innovate -&amp;gt; test -&amp;gt; scale model – e.g. conducting thousands of RCTs and refining the methodological toolbox for impact evaluations. It would be great to see more money go to local think tanks working on policy issues or even activist organizations pushing for social change.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brief Notes on Kasy and Sautmann&#39;s Exploration Sampling</title>
      <link>https://academic-demo.netlify.app/post/brief-notes-on-kasy-and-sautmann-s-exploration-sampling/</link>
      <pubDate>Fri, 14 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/brief-notes-on-kasy-and-sautmann-s-exploration-sampling/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/brief-notes-on-kasy-and-sautmann-s-exploration-sampling/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I’ve been spending a fair bit of time digging through the literature on the cognitive science of learning and one consistent result from the literature is that taking notes (ideally good notes) is really helpful for making sure that you learn and remember something. I typically take quick notes in Zotero when I read papers or books but when I go back to the notes a month or two later often find that they aren’t that useful. For the next month or so, I’m going to instead share quick notes on stuff I’ve read through blog posts to see if that is a more effective way of making sure I don’t forget everything I’ve read.&lt;/p&gt;
&lt;p&gt;In the first post in this series, I discuss Kasy and Sautmann’s paper “Adaptive Treatment Assignment in Experiments for Policy.” In their paper, Kasy and Sautmann analyze treatment assignment rules for experiments with adaptive treatment assignments and come up with a new adaptive treatment assignment rule, which they call “exploration sampling,” which performs a lot better the most commonly used rule in some contexts. That’s a lot of jargon for one sentence so let me elaborate on each of those points.&lt;/p&gt;
&lt;div id=&#34;adaptive-treatment-assignment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adaptive Treatment Assignment&lt;/h2&gt;
&lt;p&gt;“Adaptive treatment assignment” refers to the practice of adjusting the probability of treatment assignment over the course of an experiment. In a typical experiment, all study participants are assigned all at once to one of several treatment arms (or a control) at the start of the study. This practice works fine if you are just comparing one treatment to a control but isn’t really optimal if you are trying to compare several different treatments and are able to observe study outcomes relatively quickly. For example, let’s say you are running an experiment with 10 different treatment arms and notice halfway through that 5 of the arms are doing well and 5 are doing really poorly. Intuitively, it seems like it would make sense to drop the 5 that are doing really poorly and distribute the remaining sample between the 5 that are doing really well.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thompson-sampling&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thompson Sampling&lt;/h2&gt;
&lt;p&gt;The most commonly used approach to adaptive treatment assignment is Thompson sampling. With Thompson sampling, study participants are randomly assigned to each arm one at a time according to the posterior probability that that arm is the best. For example, let’s say you are in the situation described above and halfway through a study comparing 10 different treatment arms you estimate that there is a 15% probability that each of the first 5 treatment arms is the best and a 5% probability that each of the remaining arms is the best you would assign the next study participant to each of the first 5 arms with 15% probability and to each of the next 5 arms with 5% probability.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;thompson-sampling-works-well-in-sample-but-not-out-of-sample&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Thompson Sampling Works Well In Sample but Not Out of Sample&lt;/h2&gt;
&lt;p&gt;Thompson sampling works reasonably well when your goal is to maximize the welfare of the study participants. If that is your goal, you face a classic “exploration-exploitation” tradeoff – i.e. when randomly assigning a study participant you must weigh the benefit of exploring the treatment space by assigning the participant to a treatment arm that looks less effective but which &lt;em&gt;could&lt;/em&gt; be more effective than your current best bet and exploiting your current information by assigning the participant to your current best bet. There are a lot of contexts where maximizing the welfare of study participants is more of less your real goal and Thompson sampling works great. [ russo et al ] provide a lot of great examples (and also a great more detailed intro to Thompson sampling).
There are a lot of contexts where this isn’t really your goal though and, to a first approximation, you don’t really care about the welfare of the study participants. Bio-ethicists would probably disagree, but I think that the recent COVID-19 vaccine and therapeutic trials (at least two of which used adaptive treatment assignment btw) are good examples of this. These trials were conducted on relatively small samples of healthy young adults with the goal of identifying the best vaccine/treatments for COVID-19 for pretty much the entire population of the world. With those trials, you care far more about selecting the best treatments than about whether the study participants themselves were assigned the best treatment. Thompson sampling doesn’t work great in those contexts because it assigns far too large a share of study participants to the treatment arm that appears the best rather than exploring other potential arms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;exploration-sampling-for-policy-selection&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Exploration Sampling for Policy Selection&lt;/h2&gt;
&lt;p&gt;In theory, given any specific experiment and the objective of selecting the treatment arm at the end of the experiment, you could figure out an optimal treatment assignment strategy using dynamic stochastic programming. The basic idea behind this technique is that you map out each and every possible treatment assignment and outcome over time kind of like if you were trying to find the optimal strategy for chess by mapping out each potential move and response. This quickly becomes computationally infeasible though.&lt;/p&gt;
&lt;p&gt;Kasy and Sautmann’s contribution is to define a general rule for treatment assignment which seems to do much better than Thompson sampling when the goal of an experiment is to select the best treatment arm. They call their rule “exploration sampling” (to emphasize the lack of a exploitation motive) and it is really simple. Rather than assigning units to treatment arm d according to the posterior probability at time t that arm d is the best arm (&lt;span class=&#34;math inline&#34;&gt;\(p^d_t\)&lt;/span&gt;) as in Thompson sampling you instead assign participants with probability:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ q^d_t = \frac{p^d_t*(1-p^d_t)}{\sum_d{p^d_t*(1-p^d_t)}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The graph below shows how q varies with p. Unlike with Thompson sampling, once a treatment arm appears to be the best (i.e. p &amp;gt; .5), treatment assignment probability gradually goes does to give space to learn about other treatment arms.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(ggplot2)
ggplot() +  xlim(0, 1) + 
  geom_function(fun = function(x) x*(1-x)) +
  labs(x = &amp;quot;p&amp;quot;, y = &amp;quot;q&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/brief-notes-on-kasy-and-sautmann-s-exploration-sampling/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;
The other slight difference between exploration sampling and Thompson sampling is that they allow for treatment assignment in batches. Their motivation here is logistical rather than statistical – in most RCTs it would be extremely challenging to randomly assign individual units to arms.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-from-a-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results from a Simulation&lt;/h2&gt;
&lt;p&gt;In addition to showing that exploration sampling has nice theoretical properties, they show that it seems to work well in practice by comparing the performance of regular one-off treatment asssignment, Thompson sampling, and exploration sampling using a Monte Carlo simulation in which the data generating process is based on the estimated parameters for a few well known multi-arm RCTs.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Stan vs PyMC3 vs Bean Machine</title>
      <link>https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/stan-vs-pymc3-vs-bean-machine/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I have been a light user of Stan and RStan for some time and while there are a lot of things I really like about the language (such as the awesome community you can turn to for support and ShinyStan for inspecting Stan output) there are also a few things that I find frustrating. My biggest gripes with Stan are…&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;strong&gt;Installation can be buggy&lt;/strong&gt; – I think that every time I have installed RStan I have encountered some weird installation error. In some cases, the fix was an easy google search away but in other cases it took a lot more time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model compilation is frustratingly slow&lt;/strong&gt; – It generally takes quite a while (a few minutes) to compile a model in Stan. I find this frustrating because I often make really stupid mistakes that only get caught when a model is compiled or run.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Debugging is tough&lt;/strong&gt; - In some ways, a Stan model is a bit of a black box – you define your model, feed it some data, and out pops samples from the posterior (or the MAP or whatever). This makes it a bit tough to debug models, especially complicated ones.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;The documentation is comprehensive but dense&lt;/strong&gt; – The Stan documentation (the user manual, reference guide, and tutorials) are very comprehensive but not great for learning the language. Now that I have mastered the basics, this is less of a concern, but it still takes me quite a bit of time to find answers to my Stan questions.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With the release of Bean Machine a couple of weeks ago I figured it is high time I checked out other probabilistic programming languages (PPLs) so I attempted to fit a simple item response theory model in all 3 languages. I have included the raw code for all three languages at the end (you can find a Google colab notebook for the PyMC3 model &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt; and a Google colab notebook for the Bean Machine model &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt; and some quick thoughts about what I liked and disliked about PyMC3 and Bean Machine below. Despite the click-baity title, this isn’t intended to be a comprehensive comparison of the strengths and weaknesses of each language – that is a job for someone with a far better understanding of each language’s capabilities than me – but rather a quick summary of my impressions from trying to fit a simple model in each language.&lt;/p&gt;
&lt;div id=&#34;what-i-liked-and-disliked-about-pymc3&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I Liked and Disliked about PyMC3&lt;/h2&gt;
&lt;p&gt;Overall, I really liked PyMC3. In fact, I liked it so much that in the future I think I will likely use PyMC3 rather than Stan for my modelling needs. Some of the things I really liked about the language include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Installation was extremely easy and no installation is required on Google Colab.&lt;/strong&gt; Installation of PyMC3 was very easy and I didn’t encounter any weird errors. Even better, it comes preinstalled on Google Colab!&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sampling from the prior predictive is quick and easy which is really helpful for model checking.&lt;/strong&gt; In PyMC3, to sample from the prior predictive (i.e. the distribution you get if you sample from the priors and likelihood without considering the data) you simply add one line of code at the end of your model. I found this really useful for checking that there are no basic errors in my model code and that my priors pass a basic sniff test. By contrast, in Stan, sampling from the prior predictive requires more or less duplicating your model in the generated quantities block and, for reasons I don’t understand, is super slow.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The syntax is concise and intuitive.&lt;/strong&gt; PyMC3’s syntax is very similar to Stan except that you don’t have to declare your variables (variable types are inferred from their distributions) or your data (since the data comes from the python environment). I really like this since I often make stupid mistakes in the data and parameters blocks of Stan (e.g. confusing indices).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I haven’t spent enough time with PyMC3 to say much about the documentation but based on a quick glance it seemed kind of similar to that for Stan – i.e. very comprehensive but not great for those new to the language. Sampling speed was also very similar to Stan’s (though the fact that this can be done easily on Google Colab effectively saves me some time since I can’t do much else on my laptop while Stan is running and running Stan in the cloud is more of a pain that it is worth.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;what-i-liked-and-disliked-about-bean-machine&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;What I Liked and Disliked about Bean Machine&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Update: After posting an issue on the Bean Machine Github repo, the Bean Machine team graciously took the time to figure out what was wrong with my implementation of the 3PL model in Bean Machine. The issue was that the dtype of the data that I was passing to the Bean Machine sampler was incorrect. I was passing a torch tensor of type int when Bean Machine was expecting a torch tensor of type float (since that is the output generated from the torch Bernoulli distribution). In the code below, I have commented out the incorrect line and replaced it with the new correct line.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;I started this exercise thinking that I would probably like Bean Machine more than the other two languages. While it seems like Bean Machine has a lot of potential, I found it very hard to use despite spending far more time trying to learn Bean Machine than PyMC3. Some of the things I found challenging/disliked when using Bean Machine include:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Sampling each of the random variables separately is not possible (or, at least, I couldn’t figure out how to do it).&lt;/strong&gt; One of the reasons I was excited about Bean Machine at first is that, in Bean Machine, models are not monolithic black boxes but rather a set of collection of separately defined variables. Thus, I thought it would be really easy to first sample from the priors one by one and then sample from the likelihood – something I would find really useful for debugging. Unfortunately, I couldn’t figure out how to do this easily.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;The syntax is a bit verbose.&lt;/strong&gt; In Bean Machine, each model variable requires a full function definition which means that there is a lot of cruft to sift through when looking at code. You also have to know a bit of Pytorch though with pytorch’s increasing popularity that is probably not a barrier for most people.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;del&gt;3. &lt;strong&gt;Inference seems to be a bit buggy.&lt;/strong&gt; I couldn’t get the NUTS or HMC samplers to work for my simple reference model despite the fact that, in theory, these should be well suited to the model and thus used plain old MH sampling instead.&lt;/del&gt; (See update above.)&lt;/p&gt;
&lt;p&gt;On the plus side, installation was super easy and I really liked the basic tutorials.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-reference-model-the-3-paramater-logistic-item-response-theory-model&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Reference Model – The 3 Paramater Logistic Item Response Theory Model&lt;/h2&gt;
&lt;p&gt;Since I have been playing around a lot with learning assessment data, I used a 3 parameter logistic item response theory model (3PL) as . The 3PL model is often used when you have dichotomous response data for a set of students – i.e. for each student and question combination you have an indicator for whether the student got the question right. I use a sample dataset provided by Stata. The 3PL model assumes that the probability that student j gets answer k correct is&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Pr(y_{kj}=1 |\theta_j)= c+(1-c)\frac{1}{1+e^{-(a_k(\theta_j-b_k))}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where c is a parameter which accounts for the fact that even if a student guesses there is some probability that they will get the answer right, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is the ability level for student j, &lt;span class=&#34;math inline&#34;&gt;\(b_k\)&lt;/span&gt; is the difficulty of question k, and &lt;span class=&#34;math inline&#34;&gt;\(a_k\)&lt;/span&gt; is a parameter for how well question k discriminates between low and high ability students.&lt;/p&gt;
&lt;p&gt;I first fit this model in Stata using maximum likelihood (which I think uses some sort of EM algorithm under the hood) and generate predicted abilities for each student in the dataset. I use the estimates from Stata to inform the priors for the full Bayesian model and as a sanity check on the model output.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;use http://www.stata-press.com/data/r14/masc1.dta
irt 3pl q1-q9
predict theta_hat, latent&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;As an aside, you might be wondering why you would want to bother with a PPL given that using Stata and an ML approach is so easy. IMO, you probably don’t. There are a few situations where using a PPL for IRT might be useful though. First, you might want to fit a model for which there is no existing software package (e.g. some fancy new model which incorporates student response time). Second, there are some instances where you might want to use a Bayesian rather than frequentist approach. For example, Zajonc and Das use a Bayesian approach to IRT to compare the full distribution of student learning outcomes in India versus other countries. The authors couldn’t have done this using a maximum likelihood approach.&lt;/p&gt;
&lt;p&gt;When fitting the model using the PPLs, I use the following priors:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta_j \sim N(0,1);  a_k \sim Lognormal(.5,1);  b_k \sim N(0,10); c \sim Beta(3,30) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The prior on &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt; is standard for most IRT models. The prior on a is borrowed from the EdStan R package The prior on b is very diffuse and may be considered more or less an uninformative prior. The prior on c is tightly centered around the ML estimate of c from the Stata output (which helps ensure that the models fit quickly).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;stan-r-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Stan + R Code&lt;/h2&gt;
&lt;p&gt;Stan code for the 3PL model is included below.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse); library(rstan)
df &amp;lt;- haven::read_dta(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)

# add column for student number
df[&amp;quot;student&amp;quot;] &amp;lt;- 1:nrow(df)

# reshape to long format
df &amp;lt;- df %&amp;gt;% pivot_longer(cols = -student, names_to = &amp;quot;question&amp;quot;, values_to = &amp;quot;y&amp;quot;) %&amp;gt;% 
  mutate(question = as.numeric(str_remove(question, &amp;quot;q&amp;quot;)))

stan_code &amp;lt;- &amp;quot;
data {
  int&amp;lt;lower=1&amp;gt; J;             // number of respondents
  int&amp;lt;lower=1&amp;gt; K;             // number of items
  int&amp;lt;lower=1&amp;gt; N;             // number of observations
  int&amp;lt;lower=1,upper=J&amp;gt; jj[N]; // respondent for observation n
  int&amp;lt;lower=1,upper=K&amp;gt; kk[N]; // item for observation n
  int&amp;lt;lower=0,upper=1&amp;gt; y[N];  // score for observation n
}
parameters {
  vector[J] theta;             // ability for student j
  vector[K] b;                 // difficulty of question k
  vector&amp;lt;lower=0&amp;gt;[K] a;        // discriminating param for question k
  real&amp;lt;lower=0,upper=1&amp;gt; c; // guessing param
}
model {
  vector[N] eta;
  
  // priors
  theta ~ normal(0, 1); // Typical assumption for theta.
  b ~ normal(0, 10); // Diffuse prior for b. Same as that used by edstan
  a ~ lognormal(0.5, 1); // Diffuse prior. Same as that used by edstan
  c ~ beta(3, 30); // Tight prior around the estimated value from Stata output
  
  // model
  for (n in 1:N) {
    eta[n] = c + (1 - c) * inv_logit(a[kk[n]] * (theta[jj[n]] - b[kk[n]]));
  }
  y ~ bernoulli(eta);
}
&amp;quot;
stan_dat &amp;lt;- list(J = max(df$student), 
                 K = max(df$question), 
                 N = nrow(df),
                 jj = df$student,
                 kk = df$question,
                 y = df$y)

fit_stan &amp;lt;- stan(model_code = stan_code, data = stan_dat)
print(fit_stan)
samples &amp;lt;- extract(fit_stan)
theta_mean &amp;lt;- colMeans(samples$theta)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;pymc3-python-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;PyMC3 + Python Code&lt;/h2&gt;
&lt;p&gt;Code to fit the 3PL model in PyMC3 is included below. You can find a Google Colab notebook with this code &lt;a href=&#34;https://colab.research.google.com/drive/15SvGVsplzp_mGub84J8ttS2k8HF0Q94a?usp=sharing&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import pymc3 as pm
import numpy as np
import pandas as pd

# import and reshape the data
df= pd.read_stata(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)
df[&amp;#39;student&amp;#39;] = np.arange(len(df))
df = df.melt(id_vars = &amp;#39;student&amp;#39;, value_name = &amp;#39;response&amp;#39;)
df[&amp;#39;question&amp;#39;] = df[&amp;#39;variable&amp;#39;].str[1:].astype(int) - 1

# Generate lists to use as indices
num_students = df[&amp;quot;student&amp;quot;].unique().shape[0]
num_items = df[&amp;quot;question&amp;quot;].unique().shape[0]
item_index = df[&amp;quot;question&amp;quot;].tolist()
student_index = df[&amp;quot;student&amp;quot;].tolist()

# Fit the model in python
with pm.Model() as irt_model:
  # Priors
  theta = pm.Normal(&amp;quot;theta&amp;quot;, mu = 0, sigma = 1, shape = num_students)
  a = pm.Lognormal(&amp;quot;a&amp;quot;, mu = 0.5, sigma = 1, shape = num_items)
  b = pm.Normal(&amp;quot;b&amp;quot;, mu = 0, sigma = 10, shape = num_items)
  c = pm.Beta(&amp;quot;c&amp;quot;, alpha = 3, beta = 30)

  # Likelihood
  eta = c + (1-c)*pm.invlogit(a[item_index]*(theta[student_index]-b[item_index]))
  response = pm.Bernoulli(&amp;quot;response&amp;quot;, p = eta, observed = df[&amp;#39;response&amp;#39;].values)

  # Inference
  posterior = pm.sample(draws = 1000, tune = 1000, return_inferencedata=True)

# Get EAP estimates for each theta_j by taking the mean of the posterior draws. 
theta_means = posterior.posterior[&amp;#39;theta&amp;#39;].mean(axis =1).mean(axis= 0)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;bean-machine-python-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Bean Machine + Python Code&lt;/h2&gt;
&lt;p&gt;My attempt to fit the 3PL model in Bean Machine is below. You can find a Colab notebook with this code &lt;a href=&#34;https://colab.research.google.com/drive/1clOIXN5pgKtkzRIt2WvQ6s9q2FSbX6-g?usp=sharing&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre class=&#34;python&#34;&gt;&lt;code&gt;import beanmachine.ppl as bm
from beanmachine.ppl.model import RVIdentifier
import numpy as np
import pandas as pd
import torch
import torch.distributions as dist

# Download and reshape the dataset
df= pd.read_stata(&amp;quot;http://www.stata-press.com/data/r14/masc1.dta&amp;quot;)
df[&amp;#39;student&amp;#39;] = np.arange(len(df))
df = df.melt(id_vars = &amp;#39;student&amp;#39;, value_name = &amp;#39;response&amp;#39;)
df[&amp;#39;question&amp;#39;] = df[&amp;#39;variable&amp;#39;].str[1:].astype(int) - 1

# Create lists to use as indices
num_students = df[&amp;quot;student&amp;quot;].unique().shape[0]
num_items = df[&amp;quot;question&amp;quot;].unique().shape[0]
item_index = df[&amp;quot;question&amp;quot;].tolist()
student_index = df[&amp;quot;student&amp;quot;].tolist()

# Fit the model
@bm.random_variable
def theta() -&amp;gt; RVIdentifier:
    return dist.Normal(0, 1).expand((num_students,))

@bm.random_variable
def a() -&amp;gt; RVIdentifier:
    return dist.LogNormal(0.5, 1).expand((num_items,))

@bm.random_variable
def b() -&amp;gt; RVIdentifier:
    return dist.Normal(0, 10).expand((num_items,))

@bm.random_variable
def c() -&amp;gt; RVIdentifier:
    return dist.Beta(3, 30)

@bm.functional
def p():
    return c()+ (1-c())*torch.sigmoid(a()[item_index]*(theta()[student_index]-b()[item_index]))

@bm.random_variable
def y() -&amp;gt; RVIdentifier:
    return dist.Bernoulli(p())
  
# Run the inference.
samples = bm.SingleSiteHamiltonianMonteCarlo(trajectory_length = 1).infer(
    queries=[theta()],
    # Old incorrect code
    # observations={y(): torch.tensor(df[&amp;quot;response&amp;quot;].values)},
    # New correct code
    observations={y(): torch.tensor(df[&amp;quot;response&amp;quot;].astype(&amp;#39;float&amp;#39;).values)},
    num_samples=2000,
    num_chains=4,
    num_adaptive_samples=2000
)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Conventional Attrition Tests Don&#39;t Make Much Sense - Here&#39;s a Better Way</title>
      <link>https://academic-demo.netlify.app/post/2021-10-25-conventional-attrition-tests-don-t-make-much-sense-here-s-a-better-way/</link>
      <pubDate>Mon, 25 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-10-25-conventional-attrition-tests-don-t-make-much-sense-here-s-a-better-way/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-10-25-conventional-attrition-tests-don-t-make-much-sense-here-s-a-better-way/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;A while back, I was involved in an education RCT that had pretty high (40% or so) attrition. What’s worse, the remaining treatment and control students appeared to be quite different from each other. We ended taking a series of steps which seems quite common for researchers in this situation:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;First, we explored whether it would be feasible to track down a random sample of attritors. In our case (as in most cases I think), this would have been really hard and expensive to do so we gave up on that.&lt;/li&gt;
&lt;li&gt;Next, we tried Lee bounds. Unfortunately, these were really wide so we gave up on this idea as well.&lt;/li&gt;
&lt;li&gt;Without any better option, we argued that attrition was no big deal since we performed three standard tests attrition tests and the results were more or less Ok.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I don’t think any of us really believed in the attrition tests we performed but, without any clear way to adjust our estimates to account for attrition, we were kind of backed into a corner. In this blog post, I explain why we didn’t believe in those tests and what (I think) would be a more reasonable way to test for attrition. In a future blog post, I will describe what I think is a more reasonable way to adjust for attrition if your Lee bounds are too wide.&lt;/p&gt;
&lt;div id=&#34;typical-tests-for-attrition-dont-make-sense&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Typical Tests for Attrition Don’t Make Sense&lt;/h1&gt;
&lt;p&gt;We performed three different standard tests for attrition. First, we compared the overall attrition rates (i.e. the share of the sample which had dropped out between baseline and end line) between treatment and control. Second, we tested the covariate balance between treatment and control for the remaining students. Third, we tested the covariate balance between the attritors and remaining students. These seemed to be the standard tests at the time. (I confess that I haven’t paid enough attention to know if the situation has changed.)&lt;/p&gt;
&lt;p&gt;There are a few issues with these tests. First, comparing attritors and those who remain in the sample usually serves little purpose. If attritors are different from those who remain in the sample this limits the extent to which we can generalize results from the RCT but the initial sample of schools was already somewhat unrepresentative. Second, it is inefficient to test for differences in covariates by just looking at the remaining students.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-better-test-for-attrition&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;A Better Test for Attrition&lt;/h1&gt;
&lt;p&gt;It would make more sense to test whether attrition caused an imbalance in your sample by also looking at attritors, i.e for a individual RCT with a single covariate you would regress&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ D_i=α+X_i\beta_1+X_iT_i\beta_2+\varepsilon_i \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where D is inclusion in sample, X is the baseline variable, and T is your treatment variable. You would then look at parameter &lt;span class=&#34;math inline&#34;&gt;\(\beta_2\)&lt;/span&gt;. (With a grouped RCT, you should cluster your standard errors and with more than one variable you would look at an overall F test of all the interaction terms or use something like &lt;a href=&#34;https://www.jstor.org/stable/27645895&#34;&gt;this&lt;/a&gt;.) You might argue that it doesn’t make sense to look at the attritors because all you care about is whether there is balance between the remaining observations but I think this argument misses the point of why we do balance tests. The reason we perform balance test is not because we particularly care about balance along the particular covariates included in the test (we can always adjust for these) but rather because we want to know if there was anything fishy in the randomization (or the attrition process).&lt;/p&gt;
&lt;p&gt;Lastly, it doesn’t really make sense to say that everything is Ok as long as your p-value is greater than .05 or so. This means that you are saying that unless there is really strong proof that attrition is affecting your results you are going to ignore it. Instead, it would make a lot more sense to use some sort of equivalence test (e.g. a TOST) or at least set a very, very high threshold for your p-value.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is GiveWell Right that Health Interventions Should Prioritized over Education Interventions?</title>
      <link>https://academic-demo.netlify.app/post/2021-10-16-the-effective-altruism-case-for-education/</link>
      <pubDate>Sat, 16 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-10-16-the-effective-altruism-case-for-education/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-10-16-the-effective-altruism-case-for-education/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;Most articles on international education start out with a well-worn two-part lede: first, developing countries are facing a “learning crisis;” second, there is a mountain of evidence demonstrating the long-term effects of education on everything from personal (and country-level) incomes to health. Yet, in their &lt;a href=&#34;https://www.givewell.org/international/technical/programs/education&#34;&gt;analysis&lt;/a&gt; of the philanthropic potential of international education interventions, GiveWell states that “there are a limited number of experimental studies providing direct evidence that education interventions improve the outcomes that we consider most important, such as earnings, health, and rates of marriage and fertility among teenage girls” and concludes that “the existing evidence for positive effects on earnings is too thin to draw general conclusions.”&lt;/p&gt;
&lt;p&gt;So which is it? Is there a mountain of evidence (presumably some of which is experimental) for the primary of education or is the evidence limited? I was curious about this and don’t entirely trust either GiveWell’s analysis (since their threshold for rigor is too high for my tastes) or reports like the World Bank’s World Development report on education (since one of the goals of these reports is to advocate for a cause) so I dug into the evidence a bit. From what I can tell, GiveWell is wrong about the link between education and income: there is a lot of research showing that more education leads to more income. Yet GiveWell’s larger point holds. The effect of education on income may be due to signalling in which case the social returns to education are low and there is very little evidence of the long-term effects of the type of early grade education interventions that the ed sector is most excited about. This is not to say that governments and donors should stop funding education programs, but it does suggest that additional evidence of the long-term effect educational interventions would be really, really helpful.&lt;/p&gt;
&lt;div id=&#34;the-card-consensus-more-education-does-lead-to-more-income&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Card Consensus – More Education Does Lead to More Income&lt;/h1&gt;
&lt;p&gt;When it comes to education and income, the GiveWell analysis is pretty far off the mark. There is quite a bit of evidence, some of it reasonably rigorous, that more years of school increase personal income. First, there is a high and consistent (across both study and context) correlation between years of school and income. For an exhaustive review of the literature check out &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-patrinosReturnsEducationDeveloping2020&#34; role=&#34;doc-biblioref&#34;&gt;Patrinos and Psacharopoulos&lt;/a&gt; (&lt;a href=&#34;#ref-patrinosReturnsEducationDeveloping2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;. Of course, if harder working, smarter people tend to get more education this correlation could just be due to an unobserved ability bias. (In fact, unobserved ability bias in a regression of wages on schooling is just about the textbook example of a potential unobserved variable bias.) Yet estimates of the causal effect of years of schooling on income from a variety of other more sophisticated methods all point in a similar direction. There are studies which use distance to college, quarter of birth, a school construction program, and differences in education between identical twins as sources of exogenous variation in years of schooling. There are studies which take great pains to measure intelligence and ability somewhat reliably. (See &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-caplanCaseEducationWhy2019&#34; role=&#34;doc-biblioref&#34;&gt;Caplan&lt;/a&gt; (&lt;a href=&#34;#ref-caplanCaseEducationWhy2019&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; for a review of these.) And, finally, there are the RCTs of secondary school scholarship programs in Ghana and Colombia that the GiveWell review looks at. Nearly all these studies yield more or less similar results.&lt;/p&gt;
&lt;p&gt;Estimates of the returns to schooling are so consistent across estimation approaches that, after years of endless back and forth over how best to account for ability bias, measurement error, and other technical issues, many (perhaps most) labor economists have come full circle and believe that a simple regression of log wages on years of schooling and a few other variables provides a pretty good estimate of the returns to education, a phenomenon Bryan Caplan labels the “Card consensus” after a famous economist who wrote a much-cited review article on the subject &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-cardEstimatingReturnSchooling2001&#34; role=&#34;doc-biblioref&#34;&gt;Card 2001&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;but-this-could-be-signaling&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;But This Could be Signaling&lt;/h1&gt;
&lt;p&gt;While more years of schooling does appear to lead to personal increases in income it may not be because school makes people more productive. It could be that school serves as a signaling device – i.e. more years of school demonstrate conscientiousness, intelligence, and conformity, attributes that employers highly value.&lt;/p&gt;
&lt;p&gt;In his book &lt;em&gt;The Case against Education: Why the Education System is a Waste of Time and Money&lt;/em&gt;, Bryan Caplan makes a strong case for the role of signaling in the wage returns to years of schooling. Caplan puts forth many arguments in favor of the signaling hypothesis but I found arguments particularly strong, at least when it comes to education in low- and middle-income countries. First, there is a large divergence between the personal return and the national returns to schooling. There has been a massive increase in educational attainment throughout the world over the past 50 years yet many countries which saw large increases in educational attainment saw only modest increases in GDP. (See, for example, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-pritchettWhereHasAll2001&#34; role=&#34;doc-biblioref&#34;&gt;Pritchett&lt;/a&gt; (&lt;a href=&#34;#ref-pritchettWhereHasAll2001&#34; role=&#34;doc-biblioref&#34;&gt;2001&lt;/a&gt;)&lt;/span&gt; though there are many other articles with similar findings.) If education caused large increases in productivity, we would expect that national incomes would rise as countries increased educational attainment. Second, anyone who has spent any amount of time in the workforce can attest that the typical curriculum does not prepare one well for a job. This is true in the US but doubly true in many low- and middle-income countries.&lt;/p&gt;
&lt;p&gt;The recent WDR makes a half-hearted attempt to counter the signaling argument but I wasn’t at all convinced &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-worldbankWorldDevelopmentReport2018&#34; role=&#34;doc-biblioref&#34;&gt;Bank 2018&lt;/a&gt;)&lt;/span&gt;. (See box 1.1 for their argument.) They claim that “the returns to an additional year of schooling for those who drop out without a high school or university diploma are as large as for those who complete the degree” yet, from what I can tell, this is outright false – there is plenty of evidence documenting a so-called “sheepskin effect” in low- and middle-income countries.&lt;/p&gt;
&lt;p&gt;They also claim that “if education worked only as a screening device, individuals with the same years of schooling should have similar outcomes regardless of the skills they acquired, which is not the case.” The best evidence I could find on this comes from a study by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-perez-alvarezReturnsCognitiveSkills&#34; role=&#34;doc-biblioref&#34;&gt;Perez-Alvarez&lt;/a&gt; (&lt;a href=&#34;#ref-perez-alvarezReturnsCognitiveSkills&#34; role=&#34;doc-biblioref&#34;&gt;n.d.&lt;/a&gt;)&lt;/span&gt; who shows that, conditional on education, people with higher learning levels do earn more but the premium is very small. She finds that “on average, an increase in one standard deviation of literacy scores increases net hourly wages by 8.5%, conditioning on years of schooling.” To put that in perspective, that is equivalent to the return from about one additional year of school in most countries.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-education-also-likely-improves-health-somewhat&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More Education Also Likely Improves Health Somewhat&lt;/h1&gt;
&lt;p&gt;There is an incredibly strong correlation between one’s educational attainment and one’s health and, for women, the health of one’s children. This correlation is particularly well documented in the US and Europe but there is ample evidence supporting this correlation in many other countries as well. (See &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-cutlerEducationHealthInsights2012&#34; role=&#34;doc-biblioref&#34;&gt;Cutler and Lleras-Muney&lt;/a&gt; (&lt;a href=&#34;#ref-cutlerEducationHealthInsights2012&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-voglEducationHealthDeveloping2014&#34; role=&#34;doc-biblioref&#34;&gt;Vogl&lt;/a&gt; (&lt;a href=&#34;#ref-voglEducationHealthDeveloping2014&#34; role=&#34;doc-biblioref&#34;&gt;2014&lt;/a&gt;)&lt;/span&gt; for reviews. See &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-oyeGirlsSchoolingGood2016&#34; role=&#34;doc-biblioref&#34;&gt;Oye, Pritchett, and Sandefur&lt;/a&gt; (&lt;a href=&#34;#ref-oyeGirlsSchoolingGood2016&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-caseGreatDivideEducation2021&#34; role=&#34;doc-biblioref&#34;&gt;Case and Deaton&lt;/a&gt; (&lt;a href=&#34;#ref-caseGreatDivideEducation2021&#34; role=&#34;doc-biblioref&#34;&gt;2021&lt;/a&gt;)&lt;/span&gt; for more recent evidence from LICs/LMICs and the US respectively.)&lt;/p&gt;
&lt;p&gt;To what extent this reflects a causal relationship between educational attainment and health is less clear. Indeed, we know that increased health can cause increased educational attainment. (For example, the famous worms study by Kremer and Miguel showed that deworming increased schooling.) There is some evidence from RCTs and a few natural experiments (e.g. changes in compulsory schooling laws and a school construction program) which suggests that increases in education cause increases in health but the estimated effects vary.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;improvements-in-education-quality-are-much-more-promising-but-lack-long-term-evidence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Improvements in Education Quality Are Much More Promising but Lack Long-Term Evidence&lt;/h1&gt;
&lt;p&gt;Asking whether more education leads to more income, health, or whatever kind of misses the point. The goal of most education interventions is not to increase access to education but rather to increase the quality of education. In most countries around the world, nearly all children receive at least a primary school education. Put in historical context, the expansion of access to education is staggering: in 2010, the average Liberian had more years of school than the average Spaniard in 1950 (and only a tiny bit less than the average Frenchman or Italian in 1950). Yet, in many developing countries, children exit school without basic literacy or numeracy schools. For example, &lt;a href=&#34;http://img.asercentre.org/docs/Impact/ASER%20Abroad/PalNetworkMay2017.pdf&#34;&gt;in Mali in 2016&lt;/a&gt;, 10.3% of grade 5 students were able to read a grade 2 text and 12.3% were able to perform grade 2 arithmetic.&lt;/p&gt;
&lt;p&gt;With reasonably high access and very low quality the focus of most education interventions is to increase quality rather than further increase quantity. And there have some spectacular successes in improving the quality of education. For example, several RCTs of Pratham’s Teaching at the Right Level methodology (subsequently renamed the CAMaL approach) across many different contexts shows that the approach causes large increases in learning levels for very low cost &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-banerjeeProofConceptScalable2017&#34; role=&#34;doc-biblioref&#34;&gt;Banerjee et al. 2017&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Yet very, very few of these studies look at the long-term impacts of increases in the quality of early grade education on income or health. This is unsurprising given the time required to measure these impacts: a prospective study of the impact an early grade learning intervention on income would require decades of follow-up. To my knowledge, the only two reasonably rigorous studies which look at the impact of increases in the quality of early grade (but not preschool) education are those by Chetty and co-authors in the US. In one study, Chetty et al looked at the impact of having a good early grade (kindergarten to 3rd grade) classroom on adult income and other outcomes where the quality of the classroom was defined by peer scores &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-chettyHowDoesYour2011&#34; role=&#34;doc-biblioref&#34;&gt;R. Chetty et al. 2011&lt;/a&gt;)&lt;/span&gt;. (Students were randomly assigned to different classrooms so variation in peer scores was random.) In another study, Chetty et al looked at the effect of having a good primary school (grades 3-8) teacher on adult outcomes where the authors use fancy statistics to estimate teacher quality and to show that their estimates are likely exogenous to student characteristics &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-chettyMeasuringImpactsTeachers2014a&#34; role=&#34;doc-biblioref&#34;&gt;Raj Chetty, Friedman, and Rockoff 2014&lt;/a&gt;)&lt;/span&gt;. In both cases, Chetty et al find surprisingly large impact. While great it is unclear whether these results extrapolate to other interventions and contexts.&lt;/p&gt;
&lt;p&gt;At the macro level, there is a very strong correlation between learning levels and subsequent growth &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hanushekEducationEconomicGrowth2010&#34; role=&#34;doc-biblioref&#34;&gt;Hanushek and Woessmann 2010&lt;/a&gt;)&lt;/span&gt;. In fact, learning levels are far more predictive of later national economic growth than any other variable that has been tested. Personally, I find this evidence somewhat convincing but as with all cross-country regressions, there is the high risk of bias due to reverse causality and/or unobserved variables.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-need-for-long-term-follow-up-of-effective-early-grade-interventions&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The Need for Long-Term Follow-Up of Effective Early Grade Interventions&lt;/h1&gt;
&lt;p&gt;TBH, I’m perfectly happy taking the effect of increases in learning on reasoning and faith alone. Basic literacy and numeracy are so useful in so many contexts that it’s hard to imagine that improvements in basic literacy and numeracy &lt;strong&gt;wouldn’t&lt;/strong&gt; increase adult income and other outcomes even for people who don’t rely on these skills for their jobs (e.g. farmers and housewives). I don’t think I’m alone in accepting this on faith. In fact, I would venture to say that the vast majority of people, including those in development, feel this way. Having said that, more precise estimates of the impact of education interventions, particularly promising early grade interventions which increase foundational literacy and numeracy, on long-term outcomes like income and health would be really helpful. At a minimum, such evidence could potentially (if it showed big positive effects) convince GiveWell and other similar funders. This is not a trivial amount of money – GiveWell allocates something like 100 million USD and the size of that bucket is growing rapidly. More broadly, this evidence could increase (again, assuming positive effects) education budgets or lead to shifts in ed money from secondary to primary (or vice versa). And, of course, I could be wrong. Perhaps early grade learning doesn’t matter that much.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-banerjeeProofConceptScalable2017&#34; class=&#34;csl-entry&#34;&gt;
Banerjee, Abhijit, Rukmini Banerji, James Berry, Esther Duflo, Harini Kannan, Shobhini Mukerji, Marc Shotland, and Michael Walton. 2017. &lt;span&gt;“From &lt;span&gt;Proof&lt;/span&gt; of &lt;span&gt;Concept&lt;/span&gt; to &lt;span&gt;Scalable Policies&lt;/span&gt;: Challenges and &lt;span&gt;Solutions&lt;/span&gt;, with an &lt;span&gt;Application&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Journal of Economic Perspectives&lt;/em&gt; 31 (4): 73–102. &lt;a href=&#34;https://doi.org/10.1257/jep.31.4.73&#34;&gt;https://doi.org/10.1257/jep.31.4.73&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-worldbankWorldDevelopmentReport2018&#34; class=&#34;csl-entry&#34;&gt;
Bank, World. 2018. &lt;span&gt;“World &lt;span&gt;Development Report&lt;/span&gt; 2018: Learning to &lt;span&gt;Realize Education&lt;/span&gt;’s &lt;span&gt;Promise&lt;/span&gt;.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-caplanCaseEducationWhy2019&#34; class=&#34;csl-entry&#34;&gt;
Caplan, Bryan Douglas. 2019. &lt;em&gt;The Case Against Education: Why the Education System Is a Waste of Time and Money&lt;/em&gt;. First Paperback Edition. &lt;span&gt;Princeton ; Oxford&lt;/span&gt;: &lt;span&gt;Princeton University Press&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-cardEstimatingReturnSchooling2001&#34; class=&#34;csl-entry&#34;&gt;
Card, David. 2001. &lt;span&gt;“Estimating the &lt;span&gt;Return&lt;/span&gt; to &lt;span&gt;Schooling&lt;/span&gt;: Progress on &lt;span&gt;Some Persistent Econometric Problems&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Econometrica&lt;/em&gt; 69 (5): 1127–60. &lt;a href=&#34;https://doi.org/10.1111/1468-0262.00237&#34;&gt;https://doi.org/10.1111/1468-0262.00237&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-caseGreatDivideEducation2021&#34; class=&#34;csl-entry&#34;&gt;
Case, Anne, and Angus Deaton. 2021. &lt;span&gt;“The &lt;span&gt;Great Divide&lt;/span&gt;: Education, &lt;span&gt;Despair&lt;/span&gt; and &lt;span&gt;Death&lt;/span&gt;.”&lt;/span&gt; w29241. &lt;span&gt;Cambridge, MA&lt;/span&gt;: &lt;span&gt;National Bureau of Economic Research&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.3386/w29241&#34;&gt;https://doi.org/10.3386/w29241&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-chettyHowDoesYour2011&#34; class=&#34;csl-entry&#34;&gt;
Chetty, R., J. N. Friedman, N. Hilger, E. Saez, D. W. Schanzenbach, and D. Yagan. 2011. &lt;span&gt;“How &lt;span&gt;Does Your Kindergarten Classroom Affect Your Earnings&lt;/span&gt;? Evidence from &lt;span&gt;Project Star&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;The Quarterly Journal of Economics&lt;/em&gt; 126 (4): 1593–1660. &lt;a href=&#34;https://doi.org/10.1093/qje/qjr041&#34;&gt;https://doi.org/10.1093/qje/qjr041&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-chettyMeasuringImpactsTeachers2014a&#34; class=&#34;csl-entry&#34;&gt;
Chetty, Raj, John N. Friedman, and Jonah E. Rockoff. 2014. &lt;span&gt;“Measuring the &lt;span&gt;Impacts&lt;/span&gt; of &lt;span&gt;Teachers II&lt;/span&gt;: Teacher &lt;span&gt;Value&lt;/span&gt;-&lt;span&gt;Added&lt;/span&gt; and &lt;span&gt;Student Outcomes&lt;/span&gt; in &lt;span&gt;Adulthood&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Economic Review&lt;/em&gt; 104 (9): 2633–79. &lt;a href=&#34;https://doi.org/10.1257/aer.104.9.2633&#34;&gt;https://doi.org/10.1257/aer.104.9.2633&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-cutlerEducationHealthInsights2012&#34; class=&#34;csl-entry&#34;&gt;
Cutler, David, and Adriana Lleras-Muney. 2012. &lt;span&gt;“Education and &lt;span&gt;Health&lt;/span&gt;: Insights from &lt;span&gt;International Comparisons&lt;/span&gt;.”&lt;/span&gt; w17738. &lt;span&gt;Cambridge, MA&lt;/span&gt;: &lt;span&gt;National Bureau of Economic Research&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.3386/w17738&#34;&gt;https://doi.org/10.3386/w17738&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-hanushekEducationEconomicGrowth2010&#34; class=&#34;csl-entry&#34;&gt;
Hanushek, Eric A., and Ludger Woessmann. 2010. &lt;span&gt;“Education and Economic Growth.”&lt;/span&gt; &lt;em&gt;Economics of Education&lt;/em&gt;, 60–67.
&lt;/div&gt;
&lt;div id=&#34;ref-oyeGirlsSchoolingGood2016&#34; class=&#34;csl-entry&#34;&gt;
Oye, Mari, Lant Pritchett, and Justin Sandefur. 2016. &lt;span&gt;“Girls’ &lt;span&gt;Schooling Is Good&lt;/span&gt;, &lt;span&gt;Girls&lt;/span&gt;’ &lt;span&gt;Schooling&lt;/span&gt; with &lt;span&gt;Learning Is Better&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;Education Commission, Center for Global Development, Washington, DC&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-patrinosReturnsEducationDeveloping2020&#34; class=&#34;csl-entry&#34;&gt;
Patrinos, Harry Anthony, and George Psacharopoulos. 2020. &lt;span&gt;“Returns to Education in Developing Countries.”&lt;/span&gt; In &lt;em&gt;The &lt;span&gt;Economics&lt;/span&gt; of &lt;span&gt;Education&lt;/span&gt;&lt;/em&gt;, 53–64. &lt;span&gt;Elsevier&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1016/B978-0-12-815391-8.00004-5&#34;&gt;https://doi.org/10.1016/B978-0-12-815391-8.00004-5&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-perez-alvarezReturnsCognitiveSkills&#34; class=&#34;csl-entry&#34;&gt;
Perez-Alvarez, Marcello. n.d. &lt;span&gt;“Returns to Cognitive Skills in 7 Developing Countries,”&lt;/span&gt; 55.
&lt;/div&gt;
&lt;div id=&#34;ref-pritchettWhereHasAll2001&#34; class=&#34;csl-entry&#34;&gt;
Pritchett, Lant. 2001. &lt;span&gt;“Where &lt;span&gt;Has All&lt;/span&gt; the &lt;span&gt;Education Gone&lt;/span&gt;?”&lt;/span&gt; &lt;a href=&#34;https://doi.org/10.4337/9781847202888&#34;&gt;https://doi.org/10.4337/9781847202888&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-voglEducationHealthDeveloping2014&#34; class=&#34;csl-entry&#34;&gt;
Vogl, T. S. 2014. &lt;span&gt;“Education and &lt;span&gt;Health&lt;/span&gt; in &lt;span&gt;Developing Economies&lt;/span&gt;.”&lt;/span&gt; In &lt;em&gt;Encyclopedia of &lt;span&gt;Health Economics&lt;/span&gt;&lt;/em&gt;, 246–49. &lt;span&gt;Elsevier&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.1016/B978-0-12-375678-7.00109-7&#34;&gt;https://doi.org/10.1016/B978-0-12-375678-7.00109-7&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Need for More Research on Teacher Management</title>
      <link>https://academic-demo.netlify.app/post/2021-09-21-the-need-for-more-research-on-teacher-management/</link>
      <pubDate>Tue, 21 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-21-the-need-for-more-research-on-teacher-management/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-21-the-need-for-more-research-on-teacher-management/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;Educationists and economists agree that teachers are extremely important. Studies from around the world show that there is huge variance in teacher effectiveness &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-hanushekTeacherQuality2006&#34; role=&#34;doc-biblioref&#34;&gt;Hanushek and Rivkin 2006&lt;/a&gt;)&lt;/span&gt;. (Unfortunately, that same research shows that there are few observable differences between effective and ineffective teachers which means that improving the teacher workforce is not just a matter of screening for the right candidates.) Educationists seem to interpret this evidence to mean that we should spend more time conducting (and improving) teacher training. Economists on the other hand typically interpret the evidence to mean that we should do a better job measuring performance and rewarding good teachers while punishing (or at least not rewarding) the bad ones.&lt;/p&gt;
&lt;p&gt;There’s some truth (and some wishful thinking) in both of these positions but what they both ignore is that there is a crucial first step of getting the teacher into the school in the first place. As Ramachandran and her co-authors show in their report “Getting the Right Teachers into the Right Schools” there is a &lt;em&gt;lot&lt;/em&gt; of room for improvement in this area: in many states, initial assignments for new teachers are often opaque and involve politics; similarly, in many states, transfers are paper-based, opaque, and political &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ramachandranGettingRightTeachers2018&#34; role=&#34;doc-biblioref&#34;&gt;Ramachandran et al. 2018&lt;/a&gt;)&lt;/span&gt;. As the authors show, these arbitrary and opaque rules waste a huge amount of everyone’s time. Teachers spend an inordinate amount of time working their networks for transfers and ed officials spend a ton of time in court fighting transfer disputes.
In addition, they also have a corrosive effect as teachers perceive the system as unfair.
“Getting the Right Teachers into the Right Schools” is a fantastic report but only scratches the surface of these issues. For example, it doesn’t look at the quality of the Teacher Eligibility Test, a test which all candidates for teaching positions must pass and which, by most accounts, is about as bad as the boards at testing for relevant knowledge. Similarly, it doesn’t look at recent teacher transfer reforms in Haryana and HP which, at least from what I’ve heard, seem to be on track to be successful.&lt;/p&gt;
&lt;p&gt;There are a lot of potential policy reforms in this space that probably wouldn’t be all that contentious. For example, changes to the TET probably wouldn’t encounter significant resistance from unions or Teacher Education Institutions, the institutions which train would be teachers. Unions don’t seem to care much about the hurdles candidates have to jump through to become teachers and TEIs wouldn’t lose any revenue from a change in the TET. (By contrast, policymakers have been trying to clean up the mess that is the TEIs for a long time. For a great inside account of Santosh Matthew’s failed crusade against the TEIs check out Anil Swarup’s book “Not Just a Civil Servant” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-NotJustCivil2019&#34; role=&#34;doc-biblioref&#34;&gt;&lt;em&gt;Not Just a Civil Servant&lt;/em&gt; 2019&lt;/a&gt;)&lt;/span&gt;) Changes to teacher assignment and transfer rules would, on the other hand, be contentious but recent reforms in Haryana and HP show that everyone is so sick of the current system that there is room for a carefully negotiated reform.&lt;/p&gt;
&lt;p&gt;These reforms probably wouldn’t have large direct effects on learning outcomes. For example,&lt;/p&gt;
&lt;p&gt;but could serve as a base for other reforms by freeing up ed officials’ time and improving teacher morale and trust in the system.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-hanushekTeacherQuality2006&#34; class=&#34;csl-entry&#34;&gt;
Hanushek, Eric A., and Steven G. Rivkin. 2006. &lt;span&gt;“Teacher Quality.”&lt;/span&gt; &lt;em&gt;Handbook of the Economics of Education&lt;/em&gt; 2: 1051–78.
&lt;/div&gt;
&lt;div id=&#34;ref-NotJustCivil2019&#34; class=&#34;csl-entry&#34;&gt;
&lt;em&gt;Not Just a Civil Servant&lt;/em&gt;. 2019.
&lt;/div&gt;
&lt;div id=&#34;ref-ramachandranGettingRightTeachers2018&#34; class=&#34;csl-entry&#34;&gt;
Ramachandran, Vimala, Tara Beteille, Toby Linden, Sangeeta Dey, Sangeeta Goyal, and Prerna Goel Chatterjee. 2018. &lt;span&gt;“Getting the &lt;span&gt;Right Teachers&lt;/span&gt; into the &lt;span&gt;Right Schools&lt;/span&gt;.”&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Need for More Research on Board Reform</title>
      <link>https://academic-demo.netlify.app/post/2021-09-20-the-need-for-more-research-on-board-reform/</link>
      <pubDate>Mon, 20 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-20-the-need-for-more-research-on-board-reform/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-20-the-need-for-more-research-on-board-reform/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;For as long as high stakes exams have existed, people have hated them. The system of imperial examinations used to select the mandarins of ancient Chinese dynasties (and the first high-stakes exams) is credited with building an empire. It was also nearly universally hated in their specifics which, critics claimed, caused rote learning, teaching to the test, and severe anxiety &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-kellaghanPublicExaminationsExamined2020&#34; role=&#34;doc-biblioref&#34;&gt;Kellaghan and Greaney 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given the near universal hate of high-stakes exams, it’s no surprise that people don’t like the Indian boards. Still, by any standard, Indian boards are particularly bad. Burdett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-burdettReviewHighStakes2017&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; compares several high-stakes exams including the CBSE and boards in Uganda, Nigeria, Pakistan, and Canada. The high-stakes exams in Uganda, Nigeria, and Canada, while not perfect, at least test some conceptual understanding. Burdett reserves harshest criticism for the CBSE and boards in Pakistan. According to Burdett, “overall the CBSE examination papers are heavily biased to rote-learning, do not test higher-order skills, and actively discourage students who try and display them.” India’s low quality boards cast a massive shadow on its education system, forcing students to memorize entire textbooks rather than spending time on more rewarding, useful, and fun intellectual pursuits.&lt;/p&gt;
&lt;p&gt;There have been no shortage of official recommendations to reform the boards. NCERT’s National Focus Group on Examination Reforms was no less harsh than Burdett in its assessment of Indian boards and included a bunch of reasonable suggestions for what types of questions should be included in the boards. Similarly, the NEP calls for the boards to be “redesigned to encourage holistic development” and that they should “test primarily core capacities/competencies rather than months of coaching and memorization” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mhrdNationalEducationPolicy2020&#34; role=&#34;doc-biblioref&#34;&gt;MHRD 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;If there is such widespread and long held consensus on the need for change, why hasn’t it already happened? A report by EI report on the Gujarat boards provides a clue. (Unfortunately, the link I used to download the report is broken and I can’t find a live link but will update this post once I find one.) The report contains a multitude of rich findings but I found one in particular interesting. EI found that a major impediment to board reform has to do with security. Paper setters (i.e. the people who write the actual exams) are extremely concerned about question errors since any such errors cause massive confusion, institutional embarrassment, and a lot of extra work. Paper setters are also extremely concerned about question leakage since this too can create a lot of confusion and extra work. As a result, paper setters opt for a low-risk strategy of creating questions directly from textbook passages as this almost guarantees that the question won’t contain any errors without the need for additional review (which raises the risk that questions will be leaked).&lt;/p&gt;
&lt;p&gt;I don’t know enough about the internal processes and politics of boards to say how great an impediment security is to improving the boards is but it seems plausible that it could be a major one. If so, more research on how to solve these security challenges would be vastly more useful than repeating the mantra that the boards must improve their questions. This is also a tractable question. Researchers could look to the paper-setting security practices from other boards around the world or even to security practices more generally.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-burdettReviewHighStakes2017&#34; class=&#34;csl-entry&#34;&gt;
Burdett, Newman. 2017. &lt;span&gt;“Review of &lt;span&gt;High Stakes Examination Instruments&lt;/span&gt; in &lt;span&gt;Primary&lt;/span&gt; and &lt;span&gt;Secondary School&lt;/span&gt; in &lt;span&gt;Developing Countries&lt;/span&gt;.”&lt;/span&gt; &lt;span&gt;Research on Improving Systems of Education (RISE)&lt;/span&gt;. &lt;a href=&#34;https://doi.org/10.35489/BSG-RISE-WP_2017/018&#34;&gt;https://doi.org/10.35489/BSG-RISE-WP_2017/018&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-kellaghanPublicExaminationsExamined2020&#34; class=&#34;csl-entry&#34;&gt;
Kellaghan, Thomas, and Vincent Greaney. 2020. &lt;span&gt;“Public &lt;span&gt;Examinations Examined&lt;/span&gt;.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-mhrdNationalEducationPolicy2020&#34; class=&#34;csl-entry&#34;&gt;
MHRD. 2020. &lt;span&gt;“National &lt;span&gt;Education Policy&lt;/span&gt; 2020.”&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>India Ed Stuff I Wish Their Was More Research On</title>
      <link>https://academic-demo.netlify.app/post/2021-09-19-india-ed-stuff-i-wish-their-was-more-research-on/</link>
      <pubDate>Sun, 19 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-19-india-ed-stuff-i-wish-their-was-more-research-on/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-19-india-ed-stuff-i-wish-their-was-more-research-on/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;I’ve spent the past year or so reading a &lt;em&gt;lot&lt;/em&gt; of research on education in India. There is obviously a lot of amazing research out there, but there are also a lot of areas where there is far less evidence than I originally expected. For example, there seems to be a long-held consensus that the Indian curriculum is overloaded. The PROBE &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-probeteamPublicReportBasic1998&#34; role=&#34;doc-biblioref&#34;&gt;1998&lt;/a&gt;)&lt;/span&gt; reports states that “children are burdened by an overloaded curriculum,” economists like Lant Pritchett &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-pritchettSlowYouRe2015&#34; role=&#34;doc-biblioref&#34;&gt;2015&lt;/a&gt;)&lt;/span&gt; talk about the problem of “over-ambitious curricula” and the official Yashpal report addresses “the problem of curriculum load” &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-mhrdYashpalCommitteeReport1993&#34; role=&#34;doc-biblioref&#34;&gt;MHRD 1993&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Given that, I expected to find a lot of evidence, or at least some arm-chair theorizing, on how best to lighten the curriculum load. Yet, apart from the few recommendations in the Yashpal report (which were pretty limited and obviously didn’t fix the problem), there is almost nothing. Ditto with board reform. Everyone hates the boards. Yet while there is consensus on what better boards would look like (more testing of conceptual understanding and less testing of rote memorization), there is little evidence on how to make these changes. Part of the reason for these gaps may be that these questions are really difficult to answer – setting a curriculum is an intensely political affair after all – but I don’t think that’s the whole story. As I argue in later posts, I think that with some well targeted research we likely could make a lot of progress in answering how to slow down the curriculum and reform the boards. Rather, I think that a big part of the reason for these research gaps is that current education researchers don’t have the tools, professional incentives, or connections to answer these questions.&lt;/p&gt;
&lt;p&gt;From what I can tell, there seem to be two research “tribes” in international education: the economists and the educationists. Economists are agnostic with regard the type of question they answer but prefer to use quantitative methods. Educationists are agnostic between quantitative and qualitative methods but tend to focus on questions related to interactions between teachers and students (e.g. pedagogy) or, less frequently, between teachers and headmasters, other officials, and parents. Both tribes describe stuff outside of their typical domain as politics or culture. For example, Michael Kremer, Nobel winning economist who has argued forcefully that curricula in many countries are overloaded, states that “there are institutional reasons for [curriculum overload]. There are political economy reasons for [curriculum overload]. &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ogdenExperimentalConversationsPerspectives2016a&#34; role=&#34;doc-biblioref&#34;&gt;Ogden 2016&lt;/a&gt;)&lt;/span&gt;” (When an economist describes something as “political economy” she typically means you might as well give up.) Similarly, the Yashpal report states that the root of the problem is not curriculum designers, teachers, or administrators, but “a deeper malaise in our society…we continue to value a few elite qualifications far more than real competence for doing useful things in life” and recommends a change to the “culture of writing textbooks.”&lt;/p&gt;
&lt;p&gt;This is a shame because there are tons of really important questions that don’t involve teacher interactions and can’t be answered quantitatively but can be answered (or least some progress can be made) using qual methods. Over the next few blog posts, I attempt to list of few of these questions and also highlight a few of my favorite papers or reports which look at these topics. In a final blog post, I’ll present some ideas on what can be done to encourage more research on these topics.&lt;/p&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-mhrdYashpalCommitteeReport1993&#34; class=&#34;csl-entry&#34;&gt;
MHRD. 1993. &lt;span&gt;“Yashpal &lt;span&gt;Committee Report&lt;/span&gt;: Learning Without &lt;span&gt;Burden&lt;/span&gt;.”&lt;/span&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-ogdenExperimentalConversationsPerspectives2016a&#34; class=&#34;csl-entry&#34;&gt;
Ogden, Timothy N. 2016. &lt;em&gt;Experimental Conversations: Perspectives on Randomized Trials in Development Economics&lt;/em&gt;. &lt;span&gt;MIT Press&lt;/span&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-pritchettSlowYouRe2015&#34; class=&#34;csl-entry&#34;&gt;
Pritchett, Lant, and Amanda Beatty. 2015. &lt;span&gt;“Slow down, You’re Going Too Fast: Matching Curricula to Student Skill Levels.”&lt;/span&gt; &lt;em&gt;International Journal of Educational Development&lt;/em&gt; 40 (January): 276–88. &lt;a href=&#34;https://doi.org/10.1016/j.ijedudev.2014.11.013&#34;&gt;https://doi.org/10.1016/j.ijedudev.2014.11.013&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-probeteamPublicReportBasic1998&#34; class=&#34;csl-entry&#34;&gt;
Team, PROBE. 1998. &lt;span&gt;“Public &lt;span&gt;Report&lt;/span&gt; on &lt;span&gt;Basic Education&lt;/span&gt; in &lt;span&gt;India&lt;/span&gt;.”&lt;/span&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>The Proliferation of State Learning Outcomes Assessments in India</title>
      <link>https://academic-demo.netlify.app/post/2021-09-17-the-explosion-of-state-level-assessments-in-india/</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-17-the-explosion-of-state-level-assessments-in-india/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-17-the-explosion-of-state-level-assessments-in-india/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;There’s a really bad dad joke about European technical standards which goes something like “We love standards; that’s why we create so many of them.” (I told you it was bad.) I feel like that is the way that ed assessments in India are heading. Ideally, there would be a single trusted national sample assessment which provided accurate data on how well states and districts are doing compared to each other and over time. Instead, I fear that we are going to get a patchwork of different state and central assessments.&lt;/p&gt;
&lt;p&gt;According to a recent &lt;a href=&#34;https://www.dell.org/wp-content/uploads/2021/07/Large-Scale-Assessments-Report.pdf?utm_source=dellfoundation&amp;amp;utm_medium=referral&#34;&gt;report&lt;/a&gt; by MSDF, CSF, EI, CGI, and CSSL, in 2016 27 states and UTs conducted state-level assessments, some sample and some census based. The &lt;a href=&#34;https://niepid.nic.in/nep_2020.pdf&#34;&gt;NEP&lt;/a&gt; recommended that all states conduct a census-based state assessment and that a national exam be introduced in grades 3, 5, and 8. The World Bank &lt;a href=&#34;https://www.worldbank.org/en/news/press-release/2021/01/28/world-bank-signs-project-to-improve-quality-of-india-s-education-system&#34;&gt;STARs&lt;/a&gt; project has allocated quite a bit of money to increase state assessment capacity in the six states the project will be implemented. And India is still &lt;a href=&#34;https://www.ndtv.com/education/india-to-participate-in-pisa-2020-know-what-is-pisa-2177883&#34;&gt;scheduled&lt;/a&gt; to participate in the 2021 PISA (though I have no idea if that is even happening). This is going to take up a mind-boggling amount of time and resources. Even the sample assessments tend to have massive sample sizes and any assessment requires dedicated technical resources to come up with items, determine the sampling strategy, and analyze the data.&lt;/p&gt;
&lt;p&gt;This would be Ok if at least the data were high quality and usable. Unfortunately, I am skeptical that this will be the case. The first issue with state-level assessments is that it makes it really difficult to compare states. While comparing state performance in a single year isn’t all that helpful (we don’t need an assessment to tell us that HP does better than UP), comparing &lt;em&gt;changes&lt;/em&gt; in state performance, especially if the states implement very different policies, can be really helpful. Second, state policymakers often try to use assessment data both to rank schools / blocks / districts which all but guarantees that the data will be unreliable. The MSDF et al report cites the example of Saksham Ghoshna in Haryana which rewarded blocks which achieved 80% student competency on a state-level assessment. Unsurprisingly, in the four years of the project the share of students who had achieved competency level went from 40% to 80%. (Ironically, BCG made similar bold &lt;a href=&#34;https://www.dougjohnson.in/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/&#34;&gt;claims&lt;/a&gt; about the effect of an ed reform project in Haryana which happened just prior to this one.)&lt;/p&gt;
&lt;p&gt;A rigorous, well executed national sample survey on learning outcomes would be far superior to this mishmash of state-level assessments. It would also be far cheaper and take up far less student, teacher, and ed official time. In addition to the reduced duplication of effort, there would likely also be big efficiency gains from a more precise and well thought-out design. For example, you could likely significantly reduce the sample size required just by judicious stratification of schools prior to initial random selection. Unfortunately, the National Achievement Survey (NAS), is &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0738059321000626&#34;&gt;not&lt;/a&gt; that assessment. I really don’t know what it would take from an institutional perspective for the NAS to improve in quality, but the stakes are high.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What Share of Students Don&#39;t Show up In Learning Outcomes Surveys in India</title>
      <link>https://academic-demo.netlify.app/post/2021-09-17-the-share-of-indian-primary-students-attending-unrecognised-private-schools/</link>
      <pubDate>Fri, 17 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-17-the-share-of-indian-primary-students-attending-unrecognised-private-schools/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-17-the-share-of-indian-primary-students-attending-unrecognised-private-schools/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;Learning outcomes surveys typically use a school-based sampling strategy – i.e. they randomly select schools and then conduct assessments of all or some portion of students at each selected school. The advantage of school-based surveys is that they are far cheaper to conduct than household surveys. The disadvantage of school-based survey is that they potentially exclude students who aren’t enrolled, don’t show up on the day of the exam, or attend “unrecognised” private schools (i.e. private schools which don’t have official recognition and thus are not in UDISE, the official government list of schools),&lt;/p&gt;
&lt;p&gt;I took a stab at estimating the total share of primary school-age children excluded from school-based sampling surveys. I used data on enrollment for rural 7-10 year olds from ASER, data on enrollment of primary and upper primary children at an unrecognised school from NSSO’s 71st round, and data on attendance of enrolled 7-10 year olds from NFHS 2015-16. The NFHS question asks whether the child &lt;em&gt;ever&lt;/em&gt; attended school in the 2015-16 so these figures estimate the share of children that almost certainly wouldn’t be included in a school-based survey. In addition, a child may be excluded because they don’t show up on the day of the exam. Kremer and Muralidharan, in a national survey of rural schools conducted in 2004, found that student attendance was 75% in private schools and 64% in government schools.&lt;/p&gt;
&lt;p&gt;The table below gives these figures for the large states in India (excluding Telangana) and for the entire country. Overall, the share of students who would completely excluded from a school-based survey is relatively low at 10%. This surprised me somewhat. In fact, my original motivation for gathering these stats was that I thought they would show that a school-based survey would exclude far more children and thus a household survey would be a better bet. Previous estimates of the share of students attending unrecognised schools have varied between around 46% and 80%. (See footnote 6 in this &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/00220388.2020.1715943&#34;&gt;paper&lt;/a&gt; by Kingdon for a review.) And estimates of the share of enrolled students attending school from previous versions of the NFHS were much lower. While the overall share of excluded students is relatively low, the share varies quite a bit state to state from just over 1% in HP and Sikkim to nearly 20% in Bihar.&lt;/p&gt;
&lt;p&gt;The upshot for me is that school-based surveys can do a decent job generating representative estimates of school-age children but, for states like Bihar and UP, it will be important to generate estimates of learning levels for children left out of the sampling frame. And, of course, it is also important to make sure that children actually show up on the day of the assessment.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-09-17-the-share-of-indian-primary-students-attending-unrecognised-private-schools/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>New ASER Data on COVID Learning Loss</title>
      <link>https://academic-demo.netlify.app/post/2021-09-07-new-aser-data-on-covid-learning-loss/</link>
      <pubDate>Tue, 07 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-09-07-new-aser-data-on-covid-learning-loss/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-09-07-new-aser-data-on-covid-learning-loss/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;ASER Centre recently released &lt;a href=&#34;http://img.asercentre.org/docs/aserkn3-pager_06.09.211.pdf&#34;&gt;results&lt;/a&gt; from a survey of learning levels in Karnataka conducted earlier this year. To my knowledge this is the first rigorous, sample-based survey of learning levels in India during COVID which is representative of a larger population. (Earlier surveys like this &lt;a href=&#34;https://archive.azimpremjiuniversity.edu.in/SitePages/pdf/Field_Studies_Loss_of_Learning_during_the_Pandemic.pdf&#34;&gt;one&lt;/a&gt; and this &lt;a href=&#34;https://roadscholarz.net/wp-content/uploads/2021/09/English.pdf&#34;&gt;one&lt;/a&gt; were not representative of larger populations and used teacher or parent recall to assess changes in learning levels.) The results are shocking, especially for the earlier grades. The figure below shows the share of standard 3, 5, and 8 students in Karnataka able to read a standard 2 level text for several previous ASER rounds and the current survey. The results for the math assessment show similar declines.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-09-07-new-aser-data-on-covid-learning-loss/index.en_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;To put these declines in perspective, prior to COVID Karnataka ranked right in the middle of Indian states in terms of the share of std 5 students who could read a std 2 text. By contrast, Karnataka std 5 students now do about as poorly as std 5 students in MP, one of the worst performing states, prior to COVID.&lt;/p&gt;
&lt;p&gt;Unfortunately, the situation is likely even worse in other states. &lt;a href=&#34;http://www.asercentre.org/Keywords/p/371.html&#34;&gt;ASER 2020 wave 1&lt;/a&gt; assessed access to remote learning devices and remote learning study habits in each Indian state. According to that survey, Karnataka students had higher access to, and use of, remote learning across the board but were particularly more likely to have received learning materials in the previous week. (See table below with selected comparison metrics.)&lt;/p&gt;
&lt;div id=&#34;coatdbajhe&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#coatdbajhe .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#coatdbajhe .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#coatdbajhe .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#coatdbajhe .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#coatdbajhe .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#coatdbajhe .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#coatdbajhe .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#coatdbajhe .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#coatdbajhe .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#coatdbajhe .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#coatdbajhe .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#coatdbajhe .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#coatdbajhe .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#coatdbajhe .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#coatdbajhe .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#coatdbajhe .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#coatdbajhe .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#coatdbajhe .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#coatdbajhe .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#coatdbajhe .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#coatdbajhe .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#coatdbajhe .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#coatdbajhe .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#coatdbajhe .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#coatdbajhe .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#coatdbajhe .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#coatdbajhe .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#coatdbajhe .gt_left {
  text-align: left;
}

#coatdbajhe .gt_center {
  text-align: center;
}

#coatdbajhe .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#coatdbajhe .gt_font_normal {
  font-weight: normal;
}

#coatdbajhe .gt_font_bold {
  font-weight: bold;
}

#coatdbajhe .gt_font_italic {
  font-style: italic;
}

#coatdbajhe .gt_super {
  font-size: 65%;
}

#coatdbajhe .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
&lt;/style&gt;
&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Figure&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Karnataka&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;India&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Difference&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% enrolled children in hh with smartphone&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;68.6&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;61.8&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;6.8&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% enrolled children in hh with TV&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;82.8&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;60.8&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;22.0&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 students with textbooks at home&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;92.3&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;81.4&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;10.9&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 students who receive study help from family members&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;75.0&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;77.3&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;-2.3&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 who received learning materials in previous week&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;75.0&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;35.8&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;39.2&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 who watched TV classes in previous week&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;26.7&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;19.7&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;7.0&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 who watched video classes in previous week&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;29.3&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;19.7&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;9.6&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 who read textbook in previous week&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;69.2&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;60.2&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;9.0&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;% std 3-5 who used worksheet in previous week&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;48.7&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;35.5&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;13.2&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Estimating the Impact of State Education Policies in India Using ASER and Synthetic Controls</title>
      <link>https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/</link>
      <pubDate>Mon, 23 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In a &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0738059321000626&#34;&gt;recent article&lt;/a&gt; Andres Parrado and I take a look at the two main sources of learning outcomes data in India – NAS and ASER – and conclude that a) NAS data is (most likely) completely unreliable and b) ASER data, while reliable, is a bit noisy.&lt;/p&gt;
&lt;p&gt;The implications for people considering using NAS data are pretty clear – you &lt;strong&gt;shouldn’t&lt;/strong&gt;. For ASER, the implications are a bit murkier. In the paper, we say that analysts should probably avoid using ASER data at the district level (which the ASER Centre already recommended) and that they should be careful about comparing year-on-year changes between states. One topic we didn’t really touch is whether ASER data can be used to evaluate the impact of state-level policies. This is a tough question to answer because it depends not just on the level of noise in the ASER data but also on the methodology used and other aspects of the data.&lt;/p&gt;
&lt;p&gt;In this blog post, I look at whether it is possible to estimate the impact of state education policies by applying the synthetic control method (SCM) to ASER data. SCM is a popular approach to estimating aggregate policies (i.e. policies which affect an entire state or other large area) when data is only available at the aggregate level (i.e. there is no household-level data) but there is data for several time periods before the policy was implemented – exactly the situation we find ourselves with ASER data. An advantage of SCM is that it is (sometimes) pretty clear when it won’t work. As I show in the post, it is very clear that SCM, or at least all the versions of SCM that I try, won’t work with ASER data. This is not to say that there is no other method out that could use ASER data to estimate the effect of state-level policies on learning outcomes. Perhaps &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1080/01621459.2021.1891924&#34;&gt;this method&lt;/a&gt; or &lt;a href=&#34;https://www.jmlr.org/papers/volume19/17-777/17-777.pdf&#34;&gt;this one&lt;/a&gt;, which further refine SCM, would perform better. But if a rigorous method well-suited to the data at hand doesn’t work it generally means that either you need more data or more assumptions (and thus less rigour).&lt;/p&gt;
&lt;p&gt;As with all of my posts, this entire post was written as a reproducible R notebook. Go to the about page for more information on how to access the code.&lt;/p&gt;
&lt;div id=&#34;how-not-to-use-aser-to-estimate-the-impact-of-a-policy-the-example-of-bcg-in-haryana&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How Not to Use ASER to Estimate the Impact of a Policy – the example of BCG in Haryana&lt;/h1&gt;
&lt;p&gt;Before diving into SCM, I first look at an example of how &lt;em&gt;not&lt;/em&gt; to use ASER code to estimate the effect of a state-level policy. My example comes from BCG’s work with Haryana on the Quality Improvement Programme (QIP), an ambitious project to improve learning in the states’ government schools through a variety of measures including monthly student assessments, teacher training, school consolidation, and leadership training. See &lt;a href=&#34;https://www.business-standard.com/article/pti-stories/haryana-to-start-quality-improvement-programme-in-schools-114050800695_1.html&#34;&gt;here&lt;/a&gt; for details. (I apologize for picking on BCG a bit in this post. BCG just did what we all do – paint our own efforts in the best light possible.) In &lt;a href=&#34;https://www.bcg.com/industries/education/transforming-education-on-massive-scale&#34;&gt;two&lt;/a&gt; &lt;a href=&#34;https://www.bcg.com/publications/2017/education-social-impact-breakthrough-education-reform-in-india&#34;&gt;articles&lt;/a&gt; on their website, BCG, the strategic partner on the QIP project, claim that QIP was a “breakthrough reform” that led to “stellar gains” and that “Haryana is now the only state in India—possibly globally—to improve learning outcomes at scale so quickly.” These claims are based largely on ASER data. According to one of the articles:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“The impact of the reforms in Haryana has exceeded expectations. From 2012 through 2014, as the overhaul was being rolled out, the share of fifth graders who could do division increased 5% and the share who could read a standard second-grade text jumped 10%. That was quite a reversal: from 2010 through 2012, the share of fifth graders who could do division had fallen 26%, and the share of children who could read a standard second-grade text had dropped 17%. According to the National Achievements Survey report published in January 2016, Haryana was one of just two states in India that showed improvement in learning outcomes across all subjects, with 28 of the 30 Indian states posting declines or no change.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The first issue with the BCG articles is that they mix data from ASER, NAS, and EI without clearly labeling the data sources. (The first article linked to above mixes data from all three sources in a single figure!) A second issue is that they compare Haryana with 5 other, seemingly arbitarily chosen states. A third more serious issue with the analysis that they claim that learning gains between 2012 and 2014 are due to QIP despite the fact that, by BCG’s own account, QIP started in 2014.&lt;/p&gt;
&lt;p&gt;If we look at ASER data (for all students) for Haryana from 2006 to 2018, it’s easy to see why they fudged the start date a bit. If we just look at the period between 2012 and 2014, the learning gains in Haryana do indeed look impressive! (Note that these figures are for all students, not just government school students. According to ASER, 42% of rural 6-14 year olds in Haryana attended private school in 2010 and 54% attended private school in 2014. Thus, restricting attention to government school students ignores a very large segment of the student population and makes it more difficult to compare changes over time (since the students exiting government schools are likely to be different from the ones staying).)&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-2-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;Yet, if we compare these gains with gains from other states, it’s unclear whether even with the fudged start date QIP clearly led to learning gains in Haryana. While ASER scores increased between 2012 and 2014 in Haryana, they increased just as much in other states too. Clearly, we need a more rigorous approach to estimating impact.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-4-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;using-the-synthetic-control-method-to-estimate-the-impact-of-qip&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using the Synthetic Control Method to Estimate the Impact of QIP&lt;/h1&gt;
&lt;p&gt;The problem with eyeballing changes in state outcomes from one period to the next (as BCG does in their articles) is that it is very difficult to say when a change is likely due to a policy rather than noise or other factors. SCM offers a more rigorous approach to estimating the impact of state-level policies. SCM creates a “synthetic control” by looking for a combination of control states that, when summed up, closely matches the outcome trajectory of the treatment state prior to the intervention. (I am skimming over a lot of details here. For example, you can specify that the synthetic control matches the treatment unit on more than just pre-treatment outcomes. See &lt;a href=&#34;https://www.tandfonline.com/doi/pdf/10.1198/jasa.2009.ap08746&#34;&gt;Abadie, Diamond, and Hainmuller (2011)&lt;/a&gt; for a complete technical description of SCM. &lt;a href=&#34;https://economics.mit.edu/files/17847&#34;&gt;Abadie (2019)&lt;/a&gt; provides an excellent overview of the method and &lt;a href=&#34;https://dspace.mit.edu/handle/1721.1/71234&#34;&gt;Abadie et al (2011)&lt;/a&gt; provides clear guidance on how to implement SCM in R.)&lt;/p&gt;
&lt;p&gt;As mentioned above, an advantage of SCM is that it is often clear if it won’t work: if the synthetic control doesn’t closely match the trend of pre-treatment outcomes for the treatment unit then it clearly won’t provide a good approximation of the treatment unit post-treatment. (The converse is not necessarily true: even if the synthetic control matches the pre-intervention treatment unit trends closely it still may be a poor approximation for post-treatment outcomes.) In the code below, I attempt to estimate the effect of QIP on grade 5 reading levels using SCM, under the assumption that QIP started in 2012 and ended in 2014. (If you are viewing this from by blog, the code is hidden. See the about page for details on how to view the entire code.) The figure below shows grade 5 reading levels for the synthetic control versus Haryana for the entire time period for which we have data. (See code comments for model details such as what exact covariates I use.) Ideally, the lines for Haryana and the synthetic control should be close together until the onset of the “treatment” in 2012. Instead, we see that there are huge gaps prior to treatment – in some years, the gaps between the two lines are much large than the gap post-treatment (which is the estimated effect).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;A second way to test the reliability of an SCM estimate is to calculate “placebo” effects for other states and compare the estimated effect for the treatment unit with the estimated “effect” for these other states. The figures below show the estimated “effect” for all states in 2013 and 2014. While the effect for Haryana is positive for both years the estimated effect in 2013 is smaller than the estimated placebo effect for 3 other states and smaller than the estimated placebo effect for 4 other states in 2014. (Note that this analysis assumes that there were no major education reform efforts in other states. I am not familiar with all of the reform efforts in other states, but I am not aware of any major education reform efforts in the states which have large “placebo” effects.)&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-7-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;## Warning: It is deprecated to specify `guide = FALSE` to remove a guide. Please
## use `guide = &amp;quot;none&amp;quot;` instead.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-7-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;applying-scm-to-other-states-and-outcomes&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Applying SCM to Other States and Outcomes&lt;/h1&gt;
&lt;p&gt;The figure above shows that the synthetic control created for Haryana grade 5 reading outcomes using the model specification provided in the code gave a poor fit for the data. The figure below shows that this model gives a poor fit for nearly all states. The figure shows the difference between the synthetic control and the “treatment” state for all states for which we have complete ASER data over this time period (i.e. I ran the model just like I did for Haryana for each other state and then calculated the difference between the state’s grade 5 reading levels and the grade 5 reading levels for the synthetic control). Ideally, the lines would all be near 0 for the entire period (unless they launched a major education reform).&lt;/p&gt;
&lt;p&gt;These results are just for one outcome (grade 5 reading levels) and for one particular model specification but toying around with the model and outcome I couldn’t find any combination of model + outcome for which synthetic control seemed appropriate.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-08-23-estimating-the-impact-of-state-education-policies-in-india/index.en_files/figure-html/unnamed-chunk-8-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;postscript&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Postscript&lt;/h1&gt;
&lt;p&gt;In late 2017, Haryana launched “Saksham Ghoshna,” an equally ambitious follow-up project to QIP which also included regular student assessments as well as new dashboards and several other pedagogic interventions. Ironically, one of the officials involved &lt;a href=&#34;https://yourstory.com/socialstory/2019/02/haryana-transformed-student-learning/amp&#34;&gt;claimed&lt;/a&gt; that the program was clearly successful because…learning outcomes had been going down prior to the project! In the officials words:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“If we look at NCERT’s various National Achievement Survey (NAS) and ASER reports, the surveys point out that the quality of school education in the state has been going down for years. Government school teachers in Haryana are well-qualified but somehow the link is missing. Classroom studies have not being meaningful. This was the initiation point.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Index Variable Weirdness</title>
      <link>https://academic-demo.netlify.app/post/2021-07-19-index-variable-weirdness/</link>
      <pubDate>Mon, 19 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-07-19-index-variable-weirdness/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-07-19-index-variable-weirdness/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;There are many instances where you have a bunch of variables and you need to boil them down to one or just a few. For example, you may be testing the effect of an education program on students’ confidence, self-efficacy, and learning levels. You will likely have at least one measure (likely more) for each of those core outcome variables and want to combine them into one to avoid having to do any multiple hypothesis corrections.&lt;/p&gt;
&lt;p&gt;Within economics, people tend to use a few different approaches to constructing such “index” variables. The simplest method, which I’ve seen attributed to Katz, is to standardize each variable, add them all up, and then standardize again. Another common approach is to apply principal components analysis and use the first component. Lastly, people often use the approach by &lt;a href=&#34;https://www.tandfonline.com/doi/abs/10.1198/016214508000000841&#34;&gt;Anderson&lt;/a&gt; which assigns weights based on the variance-covariance of the variables.&lt;/p&gt;
&lt;p&gt;The weird thing about these methods is that the PCA and Anderson methods are, in some ways, almost complete opposites, with the Katz method somewhere in between the two. With PCA, highly correlated variables tend to be assigned higher weights. With the Anderson method, highly correlated variables tend to be assigned lower weights. Both of these approaches have a certain logic to them and seem appropriate in certain circumstances. The code below demonstrates this for the case where you have have three vars: x1 which is independent of others and z1 and z2 which are highly correlated. All have mean 0 and variance of 1. The Katz method weights each var equally. PCA assigns 0 weight to x1 and half weight to z1 and z2. And the Anderson method assigns higher weight to x1 and lower weights to z1 and z2.&lt;/p&gt;
&lt;p&gt;Each of these approaches make sense in the appropriate context. If you are trying to measure some latent, unmeasurable quantity such as intelligence then it probably makes sense to take a bunch of measures and weight those which are highly correlated more highly (i.e. use PCA). On the other hand, if you have a bunch of variables and some subset may of variables may be more or less measuring the same thing you should probably weight those variables lower than the others so that you don’t double count the thing you are measuring. If you are just aggregating a bunch of different measures, then the Katz method seems pretty reasonable.&lt;/p&gt;
&lt;p&gt;The odd thing is that people rarely justify their choice in constructing an index.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse); library(MASS)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Attaching packages --------------------------------------- tidyverse 1.3.1 --&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## v ggplot2 3.3.5     v purrr   0.3.4
## v tibble  3.1.5     v dplyr   1.0.7
## v tidyr   1.1.4     v stringr 1.4.0
## v readr   2.0.2     v forcats 0.5.1&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## -- Conflicts ------------------------------------------ tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Attaching package: &amp;#39;MASS&amp;#39;&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## The following object is masked from &amp;#39;package:dplyr&amp;#39;:
## 
##     select&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# generate random variable
n &amp;lt;- 100000
x1 &amp;lt;- rnorm(n)

# Generate two correlated random variables
rho &amp;lt;- .6
Sigma &amp;lt;- matrix(data = c(1,rho,rho, 1), nrow = 2)
z &amp;lt;- mvrnorm(n, mu = c(0,0), Sigma = Sigma)
z1 &amp;lt;- z[,1]
z2 &amp;lt;- z[,2]

# Create a dataframe from the three variables
df &amp;lt;- tibble(x1 = x1, z1 = z1, z2 = z2)

# Test that all means are 0, variances are 1, and covariance of z1 and z2 is roughly equal to rho
vars &amp;lt;- list(x1, z1, z2)
map_dbl(vars, mean)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1]  0.005211541  0.003045969 -0.001404589&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;map_dbl(vars, var)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.9981877 0.9984357 1.0017675&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;cov(z1, z2)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.5991689&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Weights from PCA
pca_weights &amp;lt;- prcomp(~ x1 + z1 + z2, data = df, rank = 1)
pca_weights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Standard deviations (1, .., p=3):
## [1] 1.2646238 0.9990934 0.6331904
## 
## Rotation (n x k) = (3 x 1):
##            PC1
## x1 0.000895669
## z1 0.706123073
## z2 0.708088556&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Weights from Anderson method
Sigma_inv &amp;lt;- solve(cov(df))
anderson_weights &amp;lt;- colSums(Sigma_inv)
anderson_weights&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        x1        z1        z2 
## 1.0013371 0.6268521 0.6232446&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Lessons from RSBY for Indian Civil Service Reform</title>
      <link>https://academic-demo.netlify.app/post/2021-07-18-lessons-from-rsby-for-indian-civil-service-reform/</link>
      <pubDate>Sun, 18 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-07-18-lessons-from-rsby-for-indian-civil-service-reform/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-07-18-lessons-from-rsby-for-indian-civil-service-reform/index.en_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;

&lt;/div&gt;

&lt;p&gt;&lt;em&gt;TLDR; The experience of RSBY, a government subsidized health insurance program, shows why India desperately needs civil service reform. RSBY showed early promise but ultimately failed because state RSBY teams were unable to perform basic monitoring tasks that even a small, reasonably competent team could have accomplished.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;About ten years ago, I spent a summer working as a World Bank consultant on RSBY, a (then) new centrally government subsidized health insurance program for in-patient care. RSBY was a radical departure from traditional Indian government health programs. Rather than seek to improve the existing government-run healthcare system, RSBY bypassed the government healthcare system entirely. Under RSBY, the government paid insurance companies a fixed annual premium for each beneficiary and beneficiaries could (in theory) access in-patient care from any hospital, government-run or private, enrolled in the scheme. The hope was that RSBY would tap into the thriving (though fraud-plagued) private health insurance and healthcare markets rather than the woefully underperforming government-run hospitals to provide quality and affordable in-patient care.&lt;/p&gt;
&lt;p&gt;I spent the summer criss-crossing Jharkhand attending enrolment drives, talking to households enrolled in RSBY, and visiting hospitals participating in RSBY. My primary responsibility that summer was to oversee an RCT looking at the effect of “health camps” on RSBY enrolment and usage but I ended up spending most of my time just talking to various people about RSBY – talking to households about what they understood about the scheme, talking to doctors and administrators at hospitals participating in the scheme (as well as those not participating in the scheme), talking to the various insurance staff responsible for implementing the scheme on the ground, and, less frequently, talking to district collectors and state-level officials responsible for overseeing the scheme.&lt;/p&gt;
&lt;p&gt;Ten years on, it’s clear that RSBY (and its successor PMJAY) failed. There is some suggestive evidence based on large datasets like the NSSO rounds that RSBY led to modest reductions in out-of-payment expenditures on in-patient care. Yet the fact that we have to rely on these analyses in the first place is damning evidence of RSBY’s inconsequence. If RSBY had been successful we wouldn’t need to rely on sophisticated statistical analysis of large datasets – it would be obvious.&lt;/p&gt;
&lt;p&gt;I have thought back on this experience a lot because I think that RSBY was very, very close, much closer than most people realize, to being successful – not in the sense of solving all of India’s health care issues, but at least in the sense of significantly increasing access to in-patient care. I believe that all it would have taken was a small team of reasonably competent well-managed people in each state overseeing the scheme. Unfortunately, RSBY was administered just like nearly all other government programs: in each state, all responsibility for managing RSBY was vested in a single senior IAS officer managing a trillion other programs at the same time and with limited time in the post; most of the day-to-day responsibilities for running the program fell to low-level staff with minimal capacity.&lt;/p&gt;
&lt;p&gt;RSBY had a lot of things going for it. India has large markets for private healthcare and private health insurance. Of course, they markets are dysfunctional in all the ways that private health markets are often dysfunctional (e.g. private provides provide a huge amount of unnecessary or even harmful care and insurance companies seem to spend more effort denying claims than other aspects of their business) but they are certainly very, very competitive. The initial prices set for in-patient procedures (i.e. the amount the insurance companies were required to reimburse hospitals for care) were reasonable. The scheme was launched within the labour ministry which meant it was somewhat insulated from intra-government attacks. And it was helmed, in the early days, by Anil Swarup, the rare IAS officer capable of rallying people to his side and tirelessly wearing down internal bureaucratic resistance to new schemes.&lt;/p&gt;
&lt;p&gt;In the very early days of RSBY it seemed to show a lot of promise. Some high-quality companies like FINO + ICICI Lombard did extremely good work under the scheme. There were certainly teething problems (e.g. with the smartcards and the backend flow of data) but it seemed like those would be ironed out eventually. At the time that I came on, which was about a year after the scheme’s launch, the mood was optimistic. In the first month or so in Jharkhand, I started noticing more serious problems with the scheme. Insurance companies blatantly disregarded the enrolment rules (they just handed out smartcards without providing any information on how to use the card). Worse, they took forever to pay the hospitals (they did this on the sly by staffing the claims department with a single fresher). As a result, many hospitals gave up on the scheme. Hospital managers wouldn’t admit to this, but it was obvious from the data and when we visited the hospitals in person. (In the insurance companies defence, they were often paid extremely late.)&lt;/p&gt;
&lt;p&gt;Staff at the World Bank and in government were receptive when I raised alarm bells about these issues. One district collector, when I pointed out the shoddy work done by the local insurance company, asked if he should throw the local insurance rep in jail. Yet little changed. State and district officials simply didn’t have the capacity to monitor the work of the insurance companies and so the scheme settled into a bad equilibrium: insurance companies would bid extremely low amounts to administer the scheme under the expectation that they would have to do very little.&lt;/p&gt;
&lt;p&gt;The frustrating aspect of this was that, in my opinion, the government didn’t need to do much to make the scheme work. Directly providing services like health and education is extremely hard. In the case of RSBY, all the government had to do was monitor the insurance companies’ work and impose reasonable and realistic penalties when the insurance companies failed to comply. The monitoring task was not arduous – really all the government had to do was make sure that the insurance companies signed up hospitals, handed out cards to beneficiaries, told the beneficiaries what the cards were for, and paid hospitals on time. If these essentials were in place the private hospitals would take care of the rest. It only took me a few days to determine whether this was happening in a district. And officials had plenty of ways in which they can penalize insurance companies. (The most straightforward being delaying payment.)&lt;/p&gt;
&lt;p&gt;Unfortunately, these basic tasks were far beyond the capacity of most state RSBY teams. In most cases, the official overseeing RSBY was an IAS officer tasked with overseeing numerous other programs. (In UP, the officer responsible for RSBY was also responsible for NREGA. It boggles the mind to think of the responsibility entrusted this one person. When we met with him, he spent the entire meeting telling us about his plan to set up a high-tech call centre for NREGA complaints.) Most of the other staff working on RSBY were low-paid junior officials with little authority or opportunities for advancement. As a result, the team did little until the IAS officer held some sort of meeting then there was a few days’ flurry of well-intentioned but ultimately fruitless activity which was followed yet more inactivity.&lt;/p&gt;
&lt;div id=&#34;lessons-for-civil-service-reform&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Lessons for Civil Service Reform&lt;/h1&gt;
&lt;p&gt;The fundamental lesson I took away from my experience on RSBY is that the Indian civil service model, in which everything revolves around and flows from a small number of IAS officers, simply doesn’t work for project implementation. This model likely worked great for colonial administration when the goal was to steal as much as possible with as few as possible. It can also work Ok for policy design. While IAS officers are typically not subject matter experts, they are often able to get up to speed quickly (though it appears that is less and less the case – more on this in a future post). It works horribly for running projects though. As anyone who has worked at a reasonably well-run organization knows, implementing projects well requires reasonably competent staff who are given clear direction and entrusted with the authority to accomplish their tasks. In India, government teams have one leader with high capacity but insufficient time to do anything more than shout orders and a large staff of subordinates with little capacity who are tasked with menial tasks.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>How India Compares to the World on Learning Outcomes</title>
      <link>https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Researchers at the World Bank recently released an excellent new dataset on learning outcomes by country.The dataset is &lt;a href=&#34;https://datacatalog.worldbank.org/dataset/harmonized-learning-outcomes-hlo-database&#34;&gt;here&lt;/a&gt; and a journal article describing the dataset is &lt;a href=&#34;https://www.nature.com/articles/s41586-021-03323-7&#34;&gt;here&lt;/a&gt;. This is the first dataset on learning outcomes (rather than educational attainment) which includes most low income countries. (Hanushek and Woesmann created a cross country dataset on learning outcomes some time back but it only included 77 countries.)&lt;/p&gt;
&lt;div id=&#34;how-well-does-india-do-according-to-the-hlo-dataset&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;How well does India do according to the HLO dataset?&lt;/h1&gt;
&lt;p&gt;I was curious to see how India does compared to other countries in the HLO dataset. The HLO dataset reports HLOs by subject and year so I first calculated the mean HLO for the most recent year data was available for each country and merged in data on GDP per capita from the World Bank. The figure below plots mean HLO for the most recent year available versus log GDP per capita for each country in the dataset.(I also label several other countries for reasons that will become clear in the next section.) According to the HLO dataset, India does about as well as would be expected given its level of income.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse); library(readxl); library(ggrepel)
hlo &amp;lt;- read_excel(&amp;quot;C:/Users/dougj/Documents/Data/Education/HLO/hlo_database.xlsx&amp;quot;, sheet = &amp;quot;HLO Database&amp;quot;)

# Take the mean by year, get the latest year
hlo_latest &amp;lt;- hlo %&amp;gt;% group_by(code, year) %&amp;gt;% 
  summarise(hlo = mean(hlo)) %&amp;gt;% 
  ungroup() %&amp;gt;% group_by(code) %&amp;gt;% 
  slice_max(order_by = year)

# Merge with GDP per capita. I grabbed this dataset from Our World in Data but the source is the World Bank. Link here https://ourworldindata.org/grapher/gdp-per-capita-worldbank
gdp &amp;lt;- read_csv(&amp;quot;C:/Users/dougj/Documents/Data/gdp-per-capita-worldbank.csv&amp;quot;) %&amp;gt;% rename(gdp = `GDP per capita, PPP (constant 2011 international $)`, code = Code, year = Year)

# generate list of countries for which we have PAL data
pal_countries &amp;lt;- c(&amp;quot;IND&amp;quot;, &amp;quot;PAK&amp;quot;, &amp;quot;KEN&amp;quot;, &amp;quot;UGA&amp;quot;, &amp;quot;TZA&amp;quot;, &amp;quot;MLI&amp;quot;, &amp;quot;SEN&amp;quot;, &amp;quot;MEX&amp;quot;, &amp;quot;NGA&amp;quot;)

# Data for 7 countries doesn&amp;#39;t merge. I just drop these
unmerged &amp;lt;- hlo_latest %&amp;gt;% anti_join(gdp, by = c(&amp;quot;code&amp;quot;, &amp;quot;year&amp;quot;))
hlo_latest &amp;lt;- hlo_latest %&amp;gt;% left_join(gdp, by = c(&amp;quot;code&amp;quot;, &amp;quot;year&amp;quot;)) %&amp;gt;% 
  mutate(sample = if_else(code %in% pal_countries, &amp;quot;PAL country&amp;quot;, &amp;quot;Others&amp;quot;)) %&amp;gt;% 
  mutate(log_gdp = log(gdp), code = if_else(code %in% pal_countries, code, &amp;quot;&amp;quot;)) 


# Graph HLO versus GDP
p &amp;lt;- ggplot(hlo_latest, aes(log_gdp, hlo, label = code)) + 
  geom_point(aes(colour = sample)) + geom_smooth(method = &amp;quot;lm&amp;quot;) +
  geom_text_repel() +
  labs(title = &amp;quot;Most recent average HLO versus GDP per capita (PPP)&amp;quot;)
print(p)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/index_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;comparing-india-to-other-countries-based-on-aser&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Comparing India to other countries based on ASER&lt;/h1&gt;
&lt;p&gt;The HLO dataset is fantastic but to generate comparable data for so many countries the creators of the dataset had to make some heroic assumptions. The HLO value for India relies entirely on one assessment: the 2009 PISA conducted in Tamil Nadu and HP. My first thought upon realizing this the 2009 PISA likely substantially over-estimates learning outcomes in India since Tamil Nadu is a relatively wealthy state and HP has a reputation for having a good education system. (HP was cited as an example of a high-performing state in the PROBE report.) Surprisingly, according to the IHDS 2012 survey, students in HP and Tamil Nadu are fairly representative of Indian students overall. The code below shows that the population weighted average of the two states is right around the average for the entire country. (HP does really well but Tamil Nadu does surprisingly poorly. I am using IHDS here rather than ASER because ASER excluded rural areas and a very large share of Tamilians live in cities. Also, I am averaging ASER scores for all children 8-11 in the sample which is not a great idea but should at least give a rough sense of HP and TN’s relative performance.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(haven); library(gt)
ihds_ind_dir &amp;lt;- &amp;quot;C:/Users/dougj/Documents/Data/IHDS/IHDS 2012/DS0001&amp;quot;
ind_file &amp;lt;- file.path(ihds_ind_dir, &amp;quot;36151-0001-Data.dta&amp;quot;)

# read in just those variables that i need
ihds &amp;lt;- read_dta(ind_file, col_select = c(STATEID, HHID, HHSPLITID, PERSONID, WT, RO3, RO7, RO5, URBAN2011, starts_with(&amp;quot;CS&amp;quot;), starts_with(&amp;quot;TA&amp;quot;), starts_with(&amp;quot;ED&amp;quot;)))

# drop the one row with missing values for weights and convert STATEID to factor
ihds &amp;lt;- ihds %&amp;gt;% filter(!is.na(WT)) %&amp;gt;% mutate(State = as_factor(STATEID))

# Look at the average of all students in TN and HP versus the rest of the country
tn_hp &amp;lt;- ihds %&amp;gt;% 
  mutate(Area = (State == &amp;quot;Tamil Nadu 33&amp;quot; | State == &amp;quot;Himachal Pradesh 02&amp;quot;)) %&amp;gt;% 
  mutate(Area = if_else(Area, &amp;quot;HP and TN&amp;quot;, &amp;quot;All other states&amp;quot;)) %&amp;gt;% 
  group_by(Area) %&amp;gt;% 
  summarize(reading = weighted.mean(TA8B, WT, na.rm = TRUE), 
            math = weighted.mean(TA9B, WT, na.rm = TRUE)) 


gt(tn_hp) %&amp;gt;% 
  fmt_number(
    columns = c(&amp;quot;reading&amp;quot;, &amp;quot;math&amp;quot;),
    decimals = 2
  )&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;umzcmpqqpk&#34; style=&#34;overflow-x:auto;overflow-y:auto;width:auto;height:auto;&#34;&gt;
&lt;style&gt;html {
  font-family: -apple-system, BlinkMacSystemFont, &#39;Segoe UI&#39;, Roboto, Oxygen, Ubuntu, Cantarell, &#39;Helvetica Neue&#39;, &#39;Fira Sans&#39;, &#39;Droid Sans&#39;, Arial, sans-serif;
}

#umzcmpqqpk .gt_table {
  display: table;
  border-collapse: collapse;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#umzcmpqqpk .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#umzcmpqqpk .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#umzcmpqqpk .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 0;
  padding-bottom: 6px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#umzcmpqqpk .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#umzcmpqqpk .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#umzcmpqqpk .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#umzcmpqqpk .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#umzcmpqqpk .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#umzcmpqqpk .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#umzcmpqqpk .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#umzcmpqqpk .gt_group_heading {
  padding: 8px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
}

#umzcmpqqpk .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#umzcmpqqpk .gt_from_md &gt; :first-child {
  margin-top: 0;
}

#umzcmpqqpk .gt_from_md &gt; :last-child {
  margin-bottom: 0;
}

#umzcmpqqpk .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#umzcmpqqpk .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 12px;
}

#umzcmpqqpk .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#umzcmpqqpk .gt_first_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
}

#umzcmpqqpk .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#umzcmpqqpk .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#umzcmpqqpk .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#umzcmpqqpk .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#umzcmpqqpk .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#umzcmpqqpk .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding: 4px;
}

#umzcmpqqpk .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#umzcmpqqpk .gt_sourcenote {
  font-size: 90%;
  padding: 4px;
}

#umzcmpqqpk .gt_left {
  text-align: left;
}

#umzcmpqqpk .gt_center {
  text-align: center;
}

#umzcmpqqpk .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#umzcmpqqpk .gt_font_normal {
  font-weight: normal;
}

#umzcmpqqpk .gt_font_bold {
  font-weight: bold;
}

#umzcmpqqpk .gt_font_italic {
  font-style: italic;
}

#umzcmpqqpk .gt_super {
  font-size: 65%;
}

#umzcmpqqpk .gt_footnote_marks {
  font-style: italic;
  font-weight: normal;
  font-size: 65%;
}
&lt;/style&gt;
&lt;table class=&#34;gt_table&#34;&gt;
  
  &lt;thead class=&#34;gt_col_headings&#34;&gt;
    &lt;tr&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_left&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;Area&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;reading&lt;/th&gt;
      &lt;th class=&#34;gt_col_heading gt_columns_bottom_border gt_right&#34; rowspan=&#34;1&#34; colspan=&#34;1&#34;&gt;math&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody class=&#34;gt_table_body&#34;&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;All other states&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;2.45&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;1.44&lt;/td&gt;&lt;/tr&gt;
    &lt;tr&gt;&lt;td class=&#34;gt_row gt_left&#34;&gt;HP and TN&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;2.45&lt;/td&gt;
&lt;td class=&#34;gt_row gt_right&#34;&gt;1.59&lt;/td&gt;&lt;/tr&gt;
  &lt;/tbody&gt;
  
  
&lt;/table&gt;
&lt;/div&gt;
&lt;p&gt;While Tamil Nadu and HP, taken together, are not obvious outliers when it comes to education generating a data point for the country as a whole from a single survey conducted more than a decade ago in only two states is still not ideal.&lt;/p&gt;
&lt;p&gt;Another, in my opinion better, approach to comparing India to other countries on learning outcomes is to use data from ASER and other assessments inspired by ASER. Several countries around the world have completed assessments very similar to ASER. This &lt;a href=&#34;http://img.asercentre.org/docs/Impact/ASER%20Abroad/PalNetworkMay2017.pdf&#34;&gt;webpage&lt;/a&gt; contains stats from ASER and other ASER-like assessments conducted in Pakistan, Kenya, Uganda, Tanzania, Mali, Senegal, Mexico, and Nigeria in 2015 or 2016 on the share of 5th grade children who can read a grade 2 text. (It also has figures for numeracy, but they aren’t really comparable across countries. For more on this very cool effort, check out the &lt;a href=&#34;https://palnetwork.org/&#34;&gt;People’s Action for Learning Network&lt;/a&gt;). The advantages of using ASER-like assessments to compare countries are that:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;&lt;p&gt;In contrast to PISA and most other learning outcomes assessments, ASER-like assessments use household sampling to select children. For countries like India with high drop-out, low attendance, and incomplete lists of private schools, children selected using household sampling are likely more representative of the total population.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ASER measures foundational literacy and numeracy which is arguably more important than the higher-order skills measured by assessments like PISA.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Because it is so simple, ASER scores are pretty easy to compare across languages and contexts without the use of any opaque statistics. (Tests like PISA vary slightly from country to country so sophisticated IRT methods are required to ensure that scores are comparable. And because countries don’t all participate in one single assessment, the maintainers of the HLO dataset have to use “linking” methods to generate comparable scores from different assessments.)&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;ASER scores, where available, are more recent.&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The downside of using these assessments is that we only have data for a few countries and, in India and Pakistan, ASER was only conducted in rural areas. To correct for the fact that the ASER was only conducted in rural areas in India and Pakistan, I add the difference between the share of rural and urban children who could read a class 2 text from the IHDS survey (which is about 10 ppt) to both the India and Pakistan ASER scores. (This is a strong assumption but in order to compare scores across countries some strong assumptions are required.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;results-from-comparison-of-aser-scores&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Results from comparison of ASER scores&lt;/h1&gt;
&lt;p&gt;The first scatterplot below compares ASER scores with HLO scores on reading. For countries with multiple HLO scores, I used the score which was nearest in time to 2015-16 since that is when the ASER scores are from. The second scatterplot plots ASER scores versus log GDP per capita.&lt;/p&gt;
&lt;p&gt;When we use ASER scores, India does considerably less well when compared to other countries. Pakistan, Kenya, Tanzania, and Uganda all have lower GDP per capita but similar or higher ASER scores. Senegal and Mali do worse on ASER but also have much lower income, Mexico does much better but is also much richer, and Nigeria does a little worse than India.&lt;/p&gt;
&lt;p&gt;If we compare the figures below with the first figure above (the one with all the HLO scores) we see that the reason for India’s comparatively lower performance is that Nigeria and Pakistan do much better, relatively, on ASER than on the HLO measure. In both figures, Kenya, Tanzania, and Uganda do much better than India (considering their relatively low income levels) but in the first figure this is balanced out by the relatively low performance of Pakistan and Nigeria.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;temp &amp;lt;- ihds %&amp;gt;% 
  mutate(grade2 = as.numeric(TA8B == 4)) %&amp;gt;% 
  group_by(URBAN2011) %&amp;gt;% 
  summarize(reading = weighted.mean(as.numeric(grade2), WT, na.rm = TRUE))

rural_urban_diff &amp;lt;- temp$reading[temp$URBAN2011 == 1]- temp$reading[temp$URBAN2011 == 0]

aser &amp;lt;- tribble(
  ~code, ~aser_reading,
  &amp;quot;IND&amp;quot;, .47+rural_urban_diff,
  &amp;quot;PAK&amp;quot;, .549+rural_urban_diff,
  &amp;quot;KEN&amp;quot;, .768,
  &amp;quot;UGA&amp;quot;, .557,
  &amp;quot;TZA&amp;quot;, .81,
  &amp;quot;MLI&amp;quot;, .103,
  &amp;quot;SEN&amp;quot;, .33,
  &amp;quot;MEX&amp;quot;, .797,
  &amp;quot;NGA&amp;quot;, .444
)

hlo_reading &amp;lt;- hlo %&amp;gt;% 
  filter(subject == &amp;quot;reading&amp;quot; &amp;amp; code %in% pal_countries) %&amp;gt;% 
  mutate(years_to_15_16 = abs(year - 2015.5)) %&amp;gt;% 
  group_by(code) %&amp;gt;% 
  slice_min(years_to_15_16, n =1, with_ties = FALSE) %&amp;gt;% 
  left_join(aser) %&amp;gt;% 
  left_join(filter(gdp, year == 2015), by = &amp;quot;code&amp;quot;) %&amp;gt;% mutate(log_gdp = log(gdp))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Joining, by = &amp;quot;code&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(hlo_reading, aes(hlo, aser_reading, label = code, colour = sourcetest)) + 
  geom_point() + geom_label_repel() +
  labs(title = &amp;quot;ASER / UWEZO vs HLO (reading)&amp;quot;, x = &amp;quot;HLO reading score&amp;quot;, y = &amp;quot;% 5th graders able to read class 2 text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/index_files/figure-html/unnamed-chunk-3-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ggplot(hlo_reading, aes(log_gdp, aser_reading, label = code)) + geom_point() +
  geom_text_repel()+ 
  labs(title = &amp;quot;ASER / UWEZO vs log GDP per capita&amp;quot;, x = &amp;quot;log GDP per capita&amp;quot;, y = &amp;quot;% 5th graders able to read class 2 text&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;https://academic-demo.netlify.app/post/2021-03-14-how-india-compares-to-the-world-on-education-performance/index_files/figure-html/unnamed-chunk-3-2.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;does-any-of-this-matter&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Does any of this matter?&lt;/h1&gt;
&lt;p&gt;I ran this analysis because I was curious how India’s education outcomes compared to other countries. That said, I’m not really sure it matters all that much. We know that many countries, India included, could do far better in educating their children. And, in India as in the US, country rankings (as long as they don’t involve cricket) don’t seem to generate much public interest. Better data which allows us to compare learning outcomes in India to other countries would be nice but I think that data which allows us to better track changes in learning outcomes &lt;strong&gt;within&lt;/strong&gt; India is far more important. Andres and I will (hopefully) be writing more on this soon.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Evidence at USAID, part two</title>
      <link>https://academic-demo.netlify.app/post/increasing-evidence-use-at-usaid/</link>
      <pubDate>Tue, 02 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/increasing-evidence-use-at-usaid/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;In my last &lt;a href=&#34;https://www.dougjohnson.in/post/evidence-based-programming-at-usaid/&#34;&gt;blog post&lt;/a&gt;, I shared some thoughts on Sarah Rose’s excellent recent &lt;a href=&#34;https://www.cgdev.org/publication/establishing-usaid-leader-evidence-based-foreign-aid&#34;&gt;working paper&lt;/a&gt; on how USAID could become more evidence based. TLDR: I argue that USAID doesn’t necessarily need to &lt;em&gt;generate&lt;/em&gt; evidence (since there are plenty of other orgs that do that) but it does need to the &lt;em&gt;use&lt;/em&gt; the latest and best evidence. Unfortunately, this is not always the case.&lt;/p&gt;
&lt;p&gt;Roses’ paper has several interesting recommendations for ensuring that USAID makes better use of evidence. She recommends that USAID a) consolidate the various entities responsible for evaluation, b) create a new cadre of “evidence brokers” responsible for digesting and communicating the latest evidence, and c) add rules to ensure greater attention to evidence in program design and procurement.&lt;/p&gt;
&lt;p&gt;These are all solid recommendations, but I think that they don’t get at the heart of the problem. In my opinion, the two biggest barriers to greater use of evidence are a) programming inertia and b) staff awareness of the latest evidence. First, there is a lot of inertia in major programming decisions. Much (most?) USAID funding is routed through large 5-year projects and, from what I can tell, when a project ends, it is typically replaced by a new 5-year project very similar in its overall design. For example, while at Abt Associates, most of my work was on the SHOPS project, a large 5-year project which focused on private sector health providers. Prior to the version of SHOPS that I worked on, there were 4 or 5 predecessors to SHOPS all of which did more or less the same thing. When I checked the Abt Associates website just now, it looks like there is yet another SHOPS project with more or less the same overall mission (though I think they have added “plus” to the name).&lt;/p&gt;
&lt;p&gt;Programming inertia like this means that there is little scope for evidence to inform many of the biggest funding decisions USAID makes. Some amount of inertia is probably a good thing: you wouldn’t want to make major changes to the overall distribution of funding every year. But USAID seems to err on the side of too much inertia. I don’t know what the best way to fix this is. Perhaps USAID should do a one-time overall review of all programming; perhaps it should change the way it decides which projects to pursue; or perhaps it should shift away from project-based funding overall. While the formal rules for how this stuff happens is well spelled out, I wasn’t involved in these decisions while at USAID and thus don’t know how these decisions are actually made in practice and thus don’t have too much insight into what should change. Nevertheless, it seems like something should change.&lt;/p&gt;
&lt;p&gt;The second barrier to greater use of evidence, in my opinion, is that staff simply don’t have the time or incentive to stay on top of the latest evidence. Even given the constraints of the projects they work on, USAID staff typically have a lot of flexibility in what gets funded or not. For example, on the SHOPS project mentioned above, there were dozens of sub-components to the project each of which our USAID officer could approve or reject at her discretion. Yet our AOR was typically so slammed that she had little time to devote to these decisions much less to do a thorough review of the evidence prior to making a decision.&lt;/p&gt;
&lt;p&gt;Unfortunately, this is a tough problem to fix. Adding a rule or procedure is easy. Changing an organization’s culture so that staff have more time and are more inclined to stay on top of the latest evidence is far more difficult. I don’t have a solution to this, but here are a few ways USAID could move in this direction:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Slash the rules&lt;/strong&gt; &lt;a href=&#34;https://www.cgdev.org/sites/default/files/1424271_file_Natsios_Counterbureaucracy.pdf&#34;&gt;Andrew Natsios&lt;/a&gt; and &lt;a href=&#34;https://dash.harvard.edu/bitstream/handle/1/17467366/HONIG-DISSERTATION-2015.pdf?sequence=4&amp;amp;isAllowed=y&#34;&gt;Dan Honig&lt;/a&gt; have written much more eloquently than I could about how USAID staff are overloaded with rules and oversight Ironically, the desired intent of many of these rules is to ensure that programs are evidence-based and aligned with overall country strategy yet the cumulative effect, as Natsios and Honig show, is that technical officers are constantly busy complying with these rules and have little free time for field visits or to read the latest paper related to their sector. Many of these rules are outside the control of USAID. Yet there is one area where USAID could do much to reduce what Natsios calls the “obsessive measurement disorder”: it could reduce the ADS, particularly the program cycle ADS which specifies the rules staff must comply with when designing and carrying out programs.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Increase the capacity of the Chief Economist’s office&lt;/strong&gt; When I worked at USAID, I found that nearly all of my colleagues were nice, congenial people. Overall, this is a very good thing. But, every once in a while, I wished that there was a curmudgeonly economist to call BS in a presentation I attended. In theory, the chief economist’s office could play this role – that is, it could help other offices digest and synthesize the latest evidence and, if necessary, call BS when a contractor’s claims are not backed up by data. In Rose’s terminology, the chief economist could serve as an “evidence broker” for the bureaus. To play that role, it would probably need a bigger budget and more staff though.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Minimize senior leaders’ pet projects&lt;/strong&gt; Senior USAID staff and politicians often have their own development pet projects which they are passionate about. Hilary Clinton was passionate about clean cook stoves. Ivanka Trump was passionate about women’s empowerment. These pet projects typically don’t take up a lot of money, but they take up a lot of staff time and foster the impression that evidence matters less than the whims of senior leaders/politicians.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Make hiring and promotion decisions more based on contributions to sector knowledge&lt;/strong&gt; Staying on top of the latest evidence takes a lot of time. Currently, there isn’t much of a professional incentive for staff to invest this time. Considering contributions to sector knowledge – such as blog posts, conference presentations, or papers – in hiring and promotion decisions would help correct this.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Evidence at USAID</title>
      <link>https://academic-demo.netlify.app/post/evidence-based-programming-at-usaid/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/evidence-based-programming-at-usaid/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Sarah Rose at the Center for Global Development recently published an &lt;a href=&#34;https://www.cgdev.org/sites/default/files/establishing-usaid-as-an-evidence-leader.pdf&#34;&gt;excellent note&lt;/a&gt; on how to make USAID programming more evidence-based. As a former member of one of the groups mentioned in the article (the Evaluation and Impact Assessment group at the erstwhile Global Development Lab) and a long-time evaluator, this is a topic dear to my cold, data-driven heart. I realize that probably marks me as a member of very small fraternity, but people really should care more about making donors more evidence-based! As Abhijit Banerjee recently &lt;a href=&#34;https://www.npr.org/transcripts/909351607&#34;&gt;pointed out&lt;/a&gt;, funding from donors like USAID may make up a small share of overall development financing in most countries but, in contrast to domestic financing, is quite flexible. Politicians typically have little fiscal space after taking into account funding mandated by previous legislation and naturally seek to use this money for quick wins like new roads or bridges. Donors, not subject to those constraints, could use their money for higher impact projects like early child development or to figure out what programs are most cost effective over the long term. Unfortunately, for reasons we’ll get to, USAID doesn’t often do this.&lt;/p&gt;
&lt;p&gt;Back to Rose’s note. Rose argues that USAID has made significant progress in becoming more evidence-based over the past decade. She points out that USAID adopted a new &lt;a href=&#34;https://www.usaid.gov/evaluation/policy&#34;&gt;evaluation policy&lt;/a&gt; in 2011 and created several new units within the agency focused on evaluation. She goes on to cite two &lt;a href=&#34;https://www.usaid.gov/sites/default/files/documents/1870/Meta-Evaluation%20of%20Quality%20and%20Coverage%20of%20USAID%20Evaluations%202009-2012.pdf&#34;&gt;internal&lt;/a&gt; &lt;a href=&#34;https://pdf.usaid.gov/pdf_docs/pa00kxvt.pdf&#34;&gt;reviews&lt;/a&gt; and an external &lt;a href=&#34;https://www.gao.gov/assets/690/683157.pdf&#34;&gt;GAO review&lt;/a&gt; which found that, overall, USAID has done a decent job in executing the vision outlined in the evaluation policy. The first internal review, for instance, found that most evaluations are high-quality, relevant, and used to inform programming.&lt;/p&gt;
&lt;p&gt;Rose argues that despite these gains, there are still gaps in how USAID generates and uses evidence. To fill these gaps, Rose recommends that USAID:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Nominate an administrator who will champion evidence-based programming&lt;/li&gt;
&lt;li&gt;Create a new evidence and evaluation unit which consolidates the various groups within USAID responsible for evaluation work&lt;/li&gt;
&lt;li&gt;Hire or train a cadre of impact evaluation specialists to oversee impact evaluations and “evidence brokers” to digest, synthesize, and communicate results from new studies&lt;/li&gt;
&lt;li&gt;Build evidence use and generation into program design and procurement. This would mean that where evidence is weak, programs should seek to generate new evidence. Where evidence is strong, program design should take into account the latest evidence&lt;/li&gt;
&lt;li&gt;Develop new methods for faster, less expensive impact evaluation methods&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I agreed with much in Rose’s article, but I think she pulls way too many punches and disagree with some of her recommendations.&lt;/p&gt;
&lt;div id=&#34;the-state-of-evaluations-at-usaid&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;The State of Evaluations at USAID&lt;/h1&gt;
&lt;p&gt;Rose’s assessment that USAID has become far more evidence-based in the past decade is a very charitable take. USAID may have superficially met many of the requirements of its own evaluation policy but, in my experience, these evaluations rarely serve any purpose other than to shuffle funding between DC and Maryland / Virginia. To give one concrete (though hopefully not representative) example: A friend of mine recently met with a USAID contractor to discuss potentially conducting an evaluation of one of their programs. As he was leaving the contractor’s office, the CEO pointed out that the CEO’s job, and the job of all the other employees he had met in the office, depended on my friend’s “independent” evaluation coming to the right conclusions. My friend didn’t take the consultancy but, if he did, it’s likely that his final evaluation report would have been deemed to be “high-quality, relevant, and useful” according to the first internal review cited above. To meet this bar, according to the review, the evaluation need only ensure that the executive summary “accurately reflects the most critical aspects of the report,” that the “basic characteristics of the program, project or activity are described” and a dozen other equally superficial criteria are met. None of the criteria attempt to ensure that the evaluation is truly independent or that the recommendations are not influenced by program implementers.&lt;/p&gt;
&lt;p&gt;Outright fraud of the type my friend encountered is (probably) rare but so are useful evaluations. Over the past decade or so, I have read hundreds of evaluations. (Yes, I should probably find better things to do with my time.) The best evaluations have shaped my thinking on what works and what doesn’t or provided helpful insight into what went right or wrong with a program. Excluding those evaluations commissioned by DIV, I can only recall a handle of evaluations that I found useful that were conducted by USAID or its partners. More worryingly, program managers often seem to ignore the latest evidence when designing programs. A recent &lt;a href=&#34;https://ssir.org/articles/entry/nowhere_to_grow&#34;&gt;SSIR article&lt;/a&gt; found that the big USAID contractors almost never scale-up proven solutions from smaller non-profits even in cases where rigorous evidence exists.&lt;/p&gt;
&lt;p&gt;As Rose points out, this isn’t really the fault of USAID staff who spend most of their time navigating the minutiae of federal regulations to get money out the door and then ensuring compliance with various reporting requirements once it is out. An additional challenge, not mentioned in Rose’s note, is that many in USAID senior leadership have a naïve understanding of what constitutes “evidence-based development.” Political appointees often come from business backgrounds, where measuring success (i.e. profits) is relatively easy, and have little experience in international development, where measuring impact is much, much harder. Faced with the task of prioritizing different programs, these political appointees often push staff to come up numbers on program impact (“metrics” in business-speak) that simply don’t exist. The result is a set of absurdly ambitious claims about “number of lives saved” and program staff with an understandable aversion to any further measurement.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ensuring-generation-of-more-high-quality-evidence&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Ensuring Generation of More High-quality Evidence&lt;/h1&gt;
&lt;p&gt;How do we fix this situation? Many of Rose’s recommendation focus on increasing USAID’s capacity to conduct impact evaluations. Superficially, this makes sense – if USAID wants to be more evidence-based making sure that it generates more high-quality evidence seems like a good first step. Yet, to a first approximation, USAID doesn’t really &lt;em&gt;need&lt;/em&gt; to generate its own evidence. The type of programming USAID funds is very similar to the type of programming other donors fund and outside researchers already evaluate. Academics, who conduct the majority of impact evaluations, are incentivized to create interesting evidence not policy-relevant evidence but observers have probably made more of this difference it deserves and, in any case, this problem is not unique to USAID.&lt;/p&gt;
&lt;p&gt;Even if USAID has a unique evidence need, USAID staff probably aren’t the best people to lead an impact evaluation. Identifying opportunities for useful impact evaluations requires a deep understanding of the latest evidence in a sector and carrying out an impact evaluation requires a very specific set of skills. These tasks are best left to professors or outside organizations specializing in this work. (An added bonus of working with professors is that their time comes heavily subsidized.) This holds doubly true for efforts to develop new impact evaluation methodologies – these tasks are best left to much nimbler organizations.&lt;/p&gt;
&lt;p&gt;This is not to say that USAID should not be involved in impact evaluations. USAID funds a lot of interesting, cutting edge programs. Impact evaluations of these programs could potentially add a lot to the global evidence base. So while USAID doesn’t necessarily need to take the lead in evidence generation, it would be great if it could collaborate with outside researchers to ensure that the most innovative programs are evaluated for impact. Unfortunately, this is rarely the case. The decision of whether to subject a program to an impact evaluation typically falls to individual program managers. Thus, whether or not a program receives an impact evaluation is more dependent on an individual manager’s enthusiasm for impact evaluations in general than it is on the program’s suitability for an impact evaluation. In addition, since impact evaluations add administrative hassle to program implementation, only the most enthusiastic managers sign up for their programs to receive an impact evaluation.&lt;/p&gt;
&lt;p&gt;I’m not sure what the best way to fix this is, but a good start would be to modify the rule for when impact evaluations. USAID’s current evaluation policy states that all innovative programs should be subject to an impact evaluation (unless it is not possible to do so). If, instead, bureau heads would given a target of being involved in at at least 3 or so high-quality impact evaluations (where for lack of a better metric quality would be measured by where the results were published) bureaus would have an incentive to work with researchers to conduct useful impact evaluations rather than be forced to justify individual decisions on whether or not to evaluate the impact of a specific project. This is a bit similar to Rose’s recommendation to “focus impact evaluations more strategically.” Where I differ from Rose is that I don’t see a need for bureaus to plan out in advance what impact evaluations they will be involved in. For various reasons, it is often hard to plan impact evaluations too far in advance and, as mentioned above, I don’t think USAID staff are well positioned to identify gaps in the evidence base.&lt;/p&gt;
&lt;p&gt;Another promising fix would be to create a new internal fund dedicated to impact evaluations which could be mobilized rapidly and which would cover the cost of both the evaluation and any additional implementation hassle. This would allow bureaus and outside evaluators to rapidly respond to interesting impact evaluation opportunities and reduce the negative incentive for program managers to be involved in impact evaluations.&lt;/p&gt;
&lt;p&gt;Alternatively, USAID could just allocate more funding to DIV, perhaps with some slight changes to its mission. While USAID as a whole hasn’t done very well when it comes to impact evaluations, DIV has been a shining exception to this rule. DIV’s mission is not to fund impact evaluations per se but this has been a positive side effect of its work.
# Ensuring More Effective Use of Evidence&lt;/p&gt;
&lt;p&gt;Rose also makes several recommendations for how USAID could better use existing evidence. Here, I wholeheartedly agree with the overall aim. USAID doesn’t necessarily have to generate new evidence, but it absolutely does need to &lt;em&gt;use&lt;/em&gt; existing evidence. Rose’s key recommendations here are to a) consolidate the various entities responsible for evaluation, b) create a new cadre of “evidence brokers” responsible for digesting and communicating the latest evidence to other stuff, and c) add rules to ensure greater attention to evidence in program design and procurement.&lt;/p&gt;
&lt;p&gt;All of these recommendations make sense, but I have minor quibbles with each. My issue with the first recommendation is only that the timing absolutely sucks. In normal times, the first recommendation would make a lot of sense. Less than a year after an extremely painful reorganization though, yet another shuffling of official titles might just break the severely weakened will of many USAID staff. Still, this is probably something that should happen relatively soon.&lt;/p&gt;
&lt;p&gt;The second recommendation – to create a cadre of “evidence brokers” – is intriguing but would face many practical hurdles. Existing staff probably wouldn’t be too keen on the idea that they need “evidence brokers” to help them figure out which programs are most effective. Without any formal authority over programming decisions, these “evidence brokers,” no matter how talented, could easily be relegated to a cold dark room within RRB never to be seen or heard from. Still, this is an interesting approach and deserves further exploration.&lt;/p&gt;
&lt;p&gt;My lukewarm reaction to the third recommendation really comes down to the fact that it would add yet more rules to already overburdened bureaucrats.I would love to see program solicitations be more informed by evidence, yet the last thing program staff needs is yet another box to check when designing programs. The existing set of bureaucratic boxes related to program design, the ADS 201, is already over a hundred pages (of very dense type).&lt;/p&gt;
&lt;p&gt;Whew, that was much longer than I originally anticipated and I haven’t even gotten to my own recommendations for how to ensure USAID makes better use of existing evidence. That will have to wait for another post.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>IHDS Quick Start Guide</title>
      <link>https://academic-demo.netlify.app/post/getting-started-with-ihds-data/</link>
      <pubDate>Fri, 07 Aug 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/getting-started-with-ihds-data/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;I recently used data from the India Human Development Survey (IHDS) for a paper on learning outcomes data in India. IHDS is a panel survey led by Sonalde Desai and others at NCAER/UofMd which collected on a wide range of topics from ~42,000 households across India. The survey includes two rounds of data collection, the first round in 2004/5 and the second round in 2011/12. (According to the website, a third round is in the works.)
For those of you working on India-related research, I would highly recommend checking out the IHDS data. Unlike data from the NSSO, IHDS is free, quick and easy to access, and requires little up-front cleaning. In addition, IHDS covers a much wider range of topics than the NFHS such as income, consumption, agriculture, education, and participation in government programmes.&lt;/p&gt;
&lt;p&gt;While the IHDS documentation is excellent, it still takes a bit of time to go through it all. I wrote this blog post to serve as a “quick start” guide to help others (and my future self) quickly get up to speed and working on the data.&lt;/p&gt;
&lt;div id=&#34;accessing-the-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Accessing the data&lt;/h1&gt;
&lt;p&gt;Accessing IHDS data is pretty straightforward. You can go to the IHDS webpage &lt;a href=&#34;https://ihds.umd.edu/&#34;&gt;here&lt;/a&gt;, click on “Download Data”, and follow the appropriate links or you can just click on the links below. Note that you will have to create a login ID. For more information on which specific files to download, see the next section below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Click &lt;a href=&#34;https://www.icpsr.umich.edu/web/ICPSR/studies/22626&#34;&gt;here&lt;/a&gt; for just the first round of data&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://www.icpsr.umich.edu/web/DSDR/studies/36151&#34;&gt;here&lt;/a&gt; for just the second round of data&lt;/li&gt;
&lt;li&gt;Click &lt;a href=&#34;https://www.icpsr.umich.edu/web/ICPSR/studies/37382/datadocumentation&#34;&gt;here&lt;/a&gt; for both rounds combined&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div id=&#34;data-files&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Data files&lt;/h1&gt;
&lt;p&gt;After arriving at the data download page, you will see a bunch of different files to download. First, there are separate files for each &lt;strong&gt;unit of observation&lt;/strong&gt;. In addition to the individual-level and household-level datasets, there are also files for medical facility-level data, school-level data, village-level data, and a few others.&lt;/p&gt;
&lt;p&gt;If you are downloading the panel data, there are also three different types of datasets corresponding to different ways of combining the two rounds of data. Datasets with &lt;strong&gt;appended&lt;/strong&gt; in the name contain all households surveyed in either round, including households in round 1 that couldn’t be found in round 2 and vice versa. Datasets with &lt;strong&gt;long&lt;/strong&gt; in the name only include households surveyed in both rounds. Similarly, datasets with &lt;strong&gt;wide&lt;/strong&gt; in the name also only include households surveyed in both rounds. The only difference between these two datasets is that long datasets contain two rows for each household (corresponding to each round of the survey) while the wide datasets contain only one row and twice the number of variables. (Note that the variables with the “X” prefix are for the first round.)&lt;/p&gt;
&lt;p&gt;You may also choose the format you would like to download the data in (SAS/SPSS/Stata/R/etc).&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;documentation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Documentation&lt;/h1&gt;
&lt;p&gt;Your download should include both a data guide and a user guide. (You can also find an online version of the data guide &lt;a href=&#34;https://www.icpsr.umich.edu/web/pages/DSDR/idhs-II-data-guide.html&#34;&gt;here&lt;/a&gt;.) The data guide provides a pretty high-level overview of the data while the user guide provides a bit more detail on the data, including a description of various constructed variables.&lt;/p&gt;
&lt;p&gt;If you are looking for more information about how IHDS was conducted, including the nitty gritty of the sampling strategy, check out &lt;a href=&#34;https://ihds.umd.edu/sites/default/files/publications/papers/technical%20paper%201.pdf&#34;&gt;this technical paper.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In addition, your download should also contain questionnaires and codebooks. (These will be in the sub-folders for each different dataset that you downloaded.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;quick-tip-on-finding-variables-in-the-datasets&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Quick tip on finding variables in the datasets&lt;/h1&gt;
&lt;p&gt;The IHDS datasets have a ton of variables and thus it can be a bit challenging to link dataset variables back to questions and vice versa.&lt;/p&gt;
&lt;p&gt;I find that a relatively quick way to find relevant variables in the dataset is to first search for keywords in the questionnaire, jot down the relevant question number, and then search for the question number in the codebook. Note that the variables in the codebooks are organized by subject rather than ordered by question number so they don’t follow the same sequence as the questionnaires.&lt;/p&gt;
&lt;p&gt;Also note that codebook section &lt;strong&gt;“ED5: HQ19 11.5 Education: Enrolled now”&lt;/strong&gt; refers to variable &lt;strong&gt;ED5&lt;/strong&gt; in the dataset which comes from &lt;strong&gt;question 11.5&lt;/strong&gt; which is on &lt;strong&gt;page 19&lt;/strong&gt; of the household questionnaire.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;sample-r-code&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Sample R code&lt;/h1&gt;
&lt;p&gt;I have included some basic R code for importing and analyzing the individual-level round 2 dataset. Note that while I use R, I actually prefer to download the data in Stata format to preserve all the variable labels.&lt;/p&gt;
&lt;div id=&#34;set-paths&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Set paths&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse); library(haven); library(survey)
ihds_ind_dir &amp;lt;- &amp;quot;C:/Users/dougj/Documents/Data/IHDS/IHDS 2012/DS0001&amp;quot;
ind_file &amp;lt;- file.path(ihds_ind_dir, &amp;quot;36151-0001-Data.dta&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;import-a-single-row-and-create-a-dataframe-of-variable-labels&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import a single row and create a dataframe of variable labels&lt;/h2&gt;
&lt;p&gt;With a large dataset like IHDS, I find it useful to first import just one row and create a separate dataframe of variable names and variable labels. This allows me to determine which variables to import later.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ihds_one_row &amp;lt;- read_dta(ind_file, n_max = 1)
labels &amp;lt;- map_chr(ihds_one_row, function(x) as.character(attributes(x)$label)[1])
ihds_labels &amp;lt;- tibble(variable = names(ihds_one_row), label = labels)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;import-the-data&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Import the data&lt;/h2&gt;
&lt;p&gt;I import just the variables I need. This saves a lot of time.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ihds &amp;lt;- read_dta(ind_file, col_select = c(STATEID, DISTID, PSUID, HHID, HHSPLITID, PERSONID, IDPSU, WT,  TA8B))

# Create a variables for PSU ID and HH ID which are unique for each PSU and household. This is required for the survey package.
ihds &amp;lt;- ihds %&amp;gt;% mutate(psu_expanded = paste(STATEID, DISTID, PSUID, sep =&amp;quot;-&amp;quot;), hh_expanded = paste(STATEID, DISTID, PSUID, HHID, HHSPLITID, sep =&amp;quot;-&amp;quot;))

# drop the one row with missing values for weights
ihds &amp;lt;- ihds %&amp;gt;% filter(!is.na(WT))

# Create variable for whether the respondent has achieved ASER at level 4. 
# This variable is only non-empty for children aged 8-11, but the functions below
# automatically drop all other respondents.
ihds &amp;lt;- ihds %&amp;gt;% mutate(ASER4 = (TA8B ==4)) %&amp;gt;% mutate(State = as_factor(STATEID))&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;use-the-survey-package-to-generate-point-estimates-and-confidence-intervals&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Use the survey package to generate point estimates and confidence intervals&lt;/h2&gt;
&lt;p&gt;With survey data you have to use a package like “survey” to generate appropriately weighted estimates and confidence intervals which take into account the sampling design.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;ihds_svy &amp;lt;- svydesign(id =~ psu_expanded + hh_expanded, weights =~ WT, data = ihds)
svyby(~ASER4, ~State, ihds_svy, svymean, na.rm=TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##                                       State ASER4FALSE  ASER4TRUE se.ASER4FALSE
## Jammu &amp;amp; Kashmir 01       Jammu &amp;amp; Kashmir 01  0.6649857 0.33501432   0.057370909
## Himachal Pradesh 02     Himachal Pradesh 02  0.4636462 0.53635381   0.040067689
## Punjab 03                         Punjab 03  0.4914748 0.50852518   0.045751591
## Chandigarh 04                 Chandigarh 04  0.3333333 0.66666667   0.103956042
## Uttarakhand 05               Uttarakhand 05  0.6122111 0.38778891   0.058721966
## Haryana 06                       Haryana 06  0.5118283 0.48817166   0.034518693
## Delhi 07                           Delhi 07  0.5037828 0.49621720   0.035436865
## Rajasthan 08                   Rajasthan 08  0.5149545 0.48504550   0.024720180
## Uttar Pradesh 09           Uttar Pradesh 09  0.6352682 0.36473182   0.023031360
## Bihar 10                           Bihar 10  0.7707418 0.22925818   0.029092679
## Sikkim 11                         Sikkim 11  0.7155111 0.28448886   0.170475931
## Arunachal Pradesh 12   Arunachal Pradesh 12  0.9455938 0.05440624   0.050433681
## Nagaland 13                     Nagaland 13  0.3227401 0.67725993   0.077900610
## Manipur 14                       Manipur 14  0.7527021 0.24729787   0.076100018
## Mizoram 15                       Mizoram 15  0.9191691 0.08083088   0.071232322
## Tripura 16                       Tripura 16  0.8808848 0.11911524   0.047025077
## Meghalaya 17                   Meghalaya 17  0.7933213 0.20667873   0.066727738
## Assam 18                           Assam 18  0.7657789 0.23422108   0.047118227
## West Bengal 19               West Bengal 19  0.5973793 0.40262072   0.029233110
## Jharkhand 20                   Jharkhand 20  0.8318841 0.16811590   0.035089495
## Orissa 21                         Orissa 21  0.5999981 0.40000187   0.038012039
## Chhattisgarh 22             Chhattisgarh 22  0.5912968 0.40870317   0.042296244
## Madhya Pradesh 23         Madhya Pradesh 23  0.5807230 0.41927698   0.023250220
## Gujarat 24                       Gujarat 24  0.6728603 0.32713971   0.028936535
## Daman &amp;amp; Diu 25               Daman &amp;amp; Diu 25  0.9282248 0.07177522   0.070587256
## Dadra+Nagar Haveli 26 Dadra+Nagar Haveli 26  0.7246527 0.27534729   0.016599128
## Maharashtra 27               Maharashtra 27  0.7771043 0.22289574   0.020304687
## Andhra Pradesh 28         Andhra Pradesh 28  0.8606345 0.13936554   0.021150722
## Karnataka 29                   Karnataka 29  0.7988545 0.20114546   0.019617888
## Goa 30                               Goa 30  0.5036842 0.49631581   0.003972885
## Kerala 32                         Kerala 32  0.5759793 0.42402074   0.031908531
## Tamil Nadu 33                 Tamil Nadu 33  0.8095212 0.19047883   0.032289616
## Pondicherry 34               Pondicherry 34  0.4237541 0.57624586   0.126186670
##                       se.ASER4TRUE
## Jammu &amp;amp; Kashmir 01     0.057370909
## Himachal Pradesh 02    0.040067689
## Punjab 03              0.045751591
## Chandigarh 04          0.103956042
## Uttarakhand 05         0.058721966
## Haryana 06             0.034518693
## Delhi 07               0.035436865
## Rajasthan 08           0.024720180
## Uttar Pradesh 09       0.023031360
## Bihar 10               0.029092679
## Sikkim 11              0.170475931
## Arunachal Pradesh 12   0.050433681
## Nagaland 13            0.077900610
## Manipur 14             0.076100018
## Mizoram 15             0.071232322
## Tripura 16             0.047025077
## Meghalaya 17           0.066727738
## Assam 18               0.047118227
## West Bengal 19         0.029233110
## Jharkhand 20           0.035089495
## Orissa 21              0.038012039
## Chhattisgarh 22        0.042296244
## Madhya Pradesh 23      0.023250220
## Gujarat 24             0.028936535
## Daman &amp;amp; Diu 25         0.070587256
## Dadra+Nagar Haveli 26  0.016599128
## Maharashtra 27         0.020304687
## Andhra Pradesh 28      0.021150722
## Karnataka 29           0.019617888
## Goa 30                 0.003972885
## Kerala 32              0.031908531
## Tamil Nadu 33          0.032289616
## Pondicherry 34         0.126186670&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Two-Phase Sampling and How it Could be Used to Collect Learning Outcomes Data</title>
      <link>https://academic-demo.netlify.app/post/school-versus-household-based-surveys-for-collecting-learning-outcomes-data/</link>
      <pubDate>Sun, 12 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/school-versus-household-based-surveys-for-collecting-learning-outcomes-data/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;Andres Parrado and I recently wrote an &lt;a href=&#34;https://www.sciencedirect.com/science/article/pii/S0738059321000626&#34;&gt;article&lt;/a&gt; in which we look at the reliability of learning outcomes data in India. The main findings of the paper are a) the government-run survey of learning outcomes (called the NAS) likely contains a lot of noise and b) the main independent survey of learning outcomes (ASER) is a tad bit noisier than the survey’s sample size would lead one to believe.&lt;/p&gt;
&lt;p&gt;In the process of working on the paper, I spent a bit of time looking at learning outcomes surveys across the world. Something that somewhat surprised me is that most learning outcomes surveys, in both rich countries (e.g. PISA) and developing countries (e.g. SACMEQ), are school-based. That is, the surveyors randomly select a bunch of schools, then randomly select a bunch of students in each selected school, and finally administer the assessment to each student.&lt;/p&gt;
&lt;p&gt;This makes sense in rich countries where you have a) high enrollment, b) accurate and up to date lists of schools, c) high attendance, d) high rates of basic literacy (and thus high capacity to take a paper and pencil test), and e) low incentives for teachers or other administrators to cheat (in the sense that their potential financial gains are a small fraction of what they stand to lose if they are caught).&lt;/p&gt;
&lt;p&gt;In developing countries, you often only have (a) – high enrollment. Typically, there is no comprehensive roster of all schools due to the proliferation of low-cost, informal private schools. Attendance on exam day cannot be guaranteed. And a large share of students don’t have sufficient reading skills to take a paper and pencil test.&lt;/p&gt;
&lt;p&gt;An alternative to the school-based approach which I think makes more sense in developing countries is to instead randomly select households. That is, you would randomly select villages, then randomly select households within each village, and finally administer the assessment to each child in each randomly selected household. This is the approach that ASER uses and seems to work really well.&lt;/p&gt;
&lt;div id=&#34;using-existing-household-surveys-to-collect-learning-outcomes-data&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Using existing household surveys to collect learning outcomes data&lt;/h1&gt;
&lt;p&gt;I think that the ideal way of collecting learning outcomes data in most developing countries would be to add a short, basic learning outcomes module to an existing large household survey like the DHS or LSMS. ASER has proved that you can get a good measure of a child’s basic literacy and numeracy in very little time so it wouldn’t add much to the total survey time or cost.&lt;/p&gt;
&lt;p&gt;Of course, getting the org running an existing large household survey to add a module to their survey is never an easy task. Out of curiosity, I decided to look at whether it would be possible to get reasonably precise estimates of learning outcomes by administering a learning outcomes assessment to just a sub-sample of children included in a larger household survey. My motivation was that, at least in theory, administering a learning module to a sub-sample of respondents rather than the full sample is lower cost and hassle and thus it might be easier to convince someone to do this than to administer the module to the full sample. (With tools like SurveyCTO it is pretty easy to randomly select a sub-sample on the spot these days.) In practice, the procedures for adding modules / questions to existing major surveys like the DHS or NSSO rounds are extremely bureaucratic so this might just be wishful thinking on my part.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;estimating-precision-from-estimating-learning-levels-using-a-sub-sample-from-a-larger-survey&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Estimating precision from estimating learning levels using a sub-sample from a larger survey&lt;/h1&gt;
&lt;p&gt;In theory, you can get reasonably high precision using only a sub-sample of a larger survey because a) you can draw a random sub-sample (i.e. without having to cluster) from the larger sample and b) a household survey like the DHS includes a lot of additional variables which are predictive of learning outcomes. To understand the first reason, suppose that sample size of the the existing household survey is so large that you can effectively consider it the whole population. If you draw a simple random sub-sample of size &lt;span class=&#34;math inline&#34;&gt;\(N_{sub}\)&lt;/span&gt; from the larger sample then your standard errors will be about as large as they would be if you drew a simple random sample from the entire population and much smaller than they would be under the typically sampling approach of two-stage clustering (e.g. selecting villages then hamlets). To understand the second reason, suppose that there is a variable in the larger sample (such as household wealth) that is extremely highly predictive of learning outcomes. You could then use this variable as an auxiliary variable when generating your estimates (or when selecting your sub-sample as a strata variable).&lt;/p&gt;
&lt;p&gt;This approach, in which you collect some information for a large sample and then collect additional information for a second sub-sample is called &lt;strong&gt;two-phase sampling&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;With a bit of math and data from the IHDS we can provide more formal estimates of the sample size requirements. The description below is a bit informal. For a more formal description, check out chapter 12 in Lohr &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-lohr2019sampling&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; or chapter 9 in Sarndal et al &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-sarndal2003model&#34; role=&#34;doc-biblioref&#34;&gt;Särndal, Swensson, and Wretman 2003&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Let &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\bar{y_f}}\)&lt;/span&gt; be the estimate of average learning outcomes that we would obtain if we administered the learning outcome assessment to all children in the sample (f stands for “full”). Let &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\bar{y_2}}\)&lt;/span&gt; be the estimate of average learning outcome if we only administer the assessment to a subsample of children (2 = “two phase”). Recall from intro stats that &lt;span class=&#34;math inline&#34;&gt;\(Var(y)=Var(E[y|x])+E[Var(y|x)]\)&lt;/span&gt;. If Z is the vector indicating household inclusion in the full sample (also called the first phase sample) then…&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ Var(\widehat{\bar{y_2}})=Var(E[\widehat{\bar{y_2}}|Z])+E[Var(\widehat{\bar{y_2}}|Z)]\approx Var(\widehat{\bar{y_f}})+E[Var(\widehat{\bar{y_2}}|Z)] \]&lt;/span&gt;
Where the second approximate equality holds as long as the &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\bar{y_2}}\)&lt;/span&gt; is approximately an unbiased estimator of &lt;span class=&#34;math inline&#34;&gt;\(\widehat{\bar{y_f}}\)&lt;/span&gt;. The second term in the equation is variance of estimator treating the full sample (i.e. the first phase sample) as the full population. Since we have a lot of auxiliary information with which to create that estimator, it’s variance is likely to be quite small.&lt;/p&gt;
&lt;p&gt;We can estimate both of these terms using IHDS data. We first estimate &lt;span class=&#34;math inline&#34;&gt;\(Var(\widehat{\bar{y_f}})\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y}\)&lt;/span&gt; is the proportion of children 8-11 who are able to read a standard 2 level text in 2011-12. (IHDS only administered the ASER tool to 8-11 year olds.)&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load required pacakges
library(tidyverse); library(survey); library(haven); library(tidymodels)

# Load the Stata version of the individual level IHDS file
# To get this file go to https://www.icpsr.umich.edu/web/DSDR/studies/36151
# Then click &amp;quot;download&amp;quot; and &amp;quot;stata&amp;quot;. You will need to create a login and you will get a bunch of other files in addition to this one.
ihds_ind_dir &amp;lt;- &amp;quot;C:/Users/dougj/Documents/Data/IHDS/IHDS 2012/DS0001&amp;quot;
ind_file &amp;lt;- file.path(ihds_ind_dir, &amp;quot;36151-0001-Data.dta&amp;quot;)
ihds &amp;lt;- read_dta(ind_file, col_select = c(STATEID, DISTID, PSUID, URBAN2011, HHID, HHSPLITID, PERSONID, IDPSU, WT, RO3, RO7, RO5, COPC, ASSETS, GROUPS, HHEDUC, HHEDUCM, HHEDUCF, INCOME, NPERSONS,starts_with(&amp;quot;CS&amp;quot;), starts_with(&amp;quot;TA&amp;quot;), starts_with(&amp;quot;ED&amp;quot;)))

# Create variables for full PSU ID, HH ID, ASER score, and state
ihds &amp;lt;- ihds %&amp;gt;% 
  mutate(psu_expanded = paste(STATEID, DISTID, PSUID, sep =&amp;quot;-&amp;quot;), 
         hh_expanded = paste(STATEID, DISTID, PSUID, HHID, HHSPLITID, sep =&amp;quot;-&amp;quot;),
         ASER4 = (TA8B ==4),
         State = as_factor(STATEID)) %&amp;gt;% 
  filter(!is.na(WT))


# Specify the survey design
# note that this and the line after can take a minute or two
ihds_svy &amp;lt;- svydesign(id =~ psu_expanded + hh_expanded, weights =~ WT, data = ihds)

# Estimate the mean of ASER4 for the full country and get the standard error
svymean(~ASER4, ihds_svy, na.rm =TRUE)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##               mean     SE
## ASER4FALSE 0.67279 0.0084
## ASER4TRUE  0.32721 0.0084&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Our standard error for our estimate is a little under one percentage point. This makes sense since our sample size is over 10,000 children so this implies a design effect of around 2.&lt;/p&gt;
&lt;p&gt;In the following code, I estimate the second term in the equation above – the additional variance due to the two-phase sampling design – assuming the random sub-sample is 1/20th the size or about 590 kids. If we just use the simple mean of our sub-sample as our estimator, the RMSE is about .021. (Just as we would expect for a SRS). If we use some of the other variables as auxiliary information, or RMSE is .0196. So, unfortunately, the auxiliary information didn’t buy us much in the way of improved precision here. (Though I should also point out that I pretty much just ran a regression with the first variables I could think of. A more sophisticated approach could potentially generate far better predictions.)&lt;/p&gt;
&lt;p&gt;Still, our estimated RMSE for the overall estimator &lt;span class=&#34;math inline&#34;&gt;\(\sqrt{.0084^2+.0196^2} = .0213\)&lt;/span&gt;. Not bad for a total sample size of 590 kids.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a new dataframe with only children 8-11 and all the variables we want to use in our analysis
kids &amp;lt;- ihds %&amp;gt;% 
  filter(!is.na(TA8B)) %&amp;gt;% 
  transmute(ASER4, TA4, RO3, URBAN2011, ASSETS, log_inc = log(INCOME+1), group = as_factor(GROUPS), RO5, HHEDUC, NPERSONS, log_pcc = log(COPC), private = ifelse((CS4 ==4), 1, 0))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Warning in log(INCOME + 1): NaNs produced&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Create a formula that we will use in our probit regression later
rx &amp;lt;- as.formula(&amp;quot;ASER4 ~ TA4 + log_pcc + RO3 + RO5 + HHEDUC + private + NPERSONS + URBAN2011 + log_inc + group&amp;quot;)

# 1/rho is the proportion of the main sample that we assume will be sub-sampled
rho &amp;lt;- 20

# set the seed so this is reproducible
set.seed(123456789)

# Create a 20-fold split. Note that ideally, I would randomly select households to be included/excluded in each split, but this is a pain and wouldn&amp;#39;t likely make much of a difference. (Since IHDS only surveyed kids 8-11 there are typically max 1 per hh.)
split_obj &amp;lt;- vfold_cv(kids, v = rho, repeats = 10)

# Calculate the true overall mean
true &amp;lt;- mean(kids$ASER4)

# Create a function which will take one of the splits from the split_obj and calculate 
# the error assuming we don&amp;#39;t use an auxiliary info and using auxiliary information.
# See here for more info https://rsample.tidymodels.org/articles/Working_with_rsets.html
ggreg_error &amp;lt;- function(splits){
  # Please note that I use the assessment data as the training data and the analysis test 
  # as the holdout data.  This is the exact opposite of what a normal ML work flow looks like.
  # In a normal workflow, you fit your model on the 1-1/rho of the data and 
  #then test it on the 1/rho portion of the data.  I want to do the exact opposite. 
  sample &amp;lt;- assessment(splits)
  other &amp;lt;- analysis(splits)
  
  # Fit the model  
  mod &amp;lt;- glm(rx, data = sample, family = binomial(link = &amp;quot;probit&amp;quot;))
  
  # Generate model predictions for both datasets
  preds_other &amp;lt;- augment(mod, newdata = other, type.predict = &amp;quot;response&amp;quot;)
  preds_sample &amp;lt;- augment(mod, newdata = sample, type.predict = &amp;quot;response&amp;quot;)

  # Calculate the generalized GREG (see Pfefferman eq 4.8 for details)
  ggreg &amp;lt;- mean(preds_other$.fitted, na.rm = TRUE)*((rho-1)/rho) + mean(sample$ASER4, na.rm = TRUE)/rho 

  # Return   
  return(list(true-ggreg, true- mean(sample$ASER4, na.rm=TRUE)))
}

# Map this function to the split object
split_obj$error &amp;lt;- map(split_obj$splits, ggreg_error)

# Get the root mean squared error for both the simple mean and the generalized GREG estimator
ggreg_error_vec &amp;lt;- map_dbl(split_obj$error, 1)
simple_error_vec &amp;lt;- map_dbl(split_obj$error, 2)

sqrt_mse_ggreg &amp;lt;- (mean(ggreg_error_vec^2))^.5
sqrt_mse_ggreg&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.01969&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;sqrt_mse_simple &amp;lt;- (mean(simple_error_vec^2))^.5
sqrt_mse_simple&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 0.02088427&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-lohr2019sampling&#34; class=&#34;csl-entry&#34;&gt;
Lohr, Sharon L. 2019. &lt;em&gt;Sampling: Design and Analysis: Design and Analysis&lt;/em&gt;. CRC Press.
&lt;/div&gt;
&lt;div id=&#34;ref-sarndal2003model&#34; class=&#34;csl-entry&#34;&gt;
Särndal, Carl-Erik, Bengt Swensson, and Jan Wretman. 2003. &lt;em&gt;Model Assisted Survey Sampling&lt;/em&gt;. Springer Science &amp;amp; Business Media.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Why India Should Focus on Educational TV rather than EdTech</title>
      <link>https://academic-demo.netlify.app/post/why-india-should-focus-on-educational-tv-rather-than-edtech/</link>
      <pubDate>Tue, 26 May 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/why-india-should-focus-on-educational-tv-rather-than-edtech/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;A couple of weeks ago, &lt;a href=&#34;https://www.linkedin.com/in/rob-sampson/?originalSubdomain=in&#34;&gt;Rob Sampson&lt;/a&gt; and I published an op-ed in Quartz India arguing that the central and state governments in India should use Educational TV rather than smartphone-based EdTech to reach students out of school due to the covid crisis. We use survey data to show that smartphone penetration in India is much lower than sales figures would lead you to believe. By contrast, TV ownership is relatively high even among the poor.&lt;/p&gt;
&lt;p&gt;The full article is &lt;a href=&#34;https://scroll.in/article/961937/small-screen-big-impact-educational-tv-could-be-indias-next-frontier-in-remote-learning&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As always with an op-ed, we ended up cutting several points due to space constraints. The great thing about a blog is that space constraints are not really an issue, so in this post I’ll flesh out some of those points that got cut.&lt;/p&gt;
&lt;p&gt;First, even if parents own a smartphone they often are reluctant to let their children use it for extended periods of time.&lt;/p&gt;
&lt;p&gt;Second, we believe TV is a better channel for instruction than radio due to relatively low radio ownership. During the Charcha2020 conference, Ashish Dhawan, founder of CSF, pointed out that the evidence for interactive radio is stronger than the evidence for educational TV. This is true, but, according to the most recent NFHS survey, only about 8% of Indian households own a radio. In addition, I think that the stronger evidence for interactive radio is most likely due to its longer history. Interactive radio has been around (and studied) since the mid-1970s. By contrast, if you exclude “edutainment” programs like Sesame Street, there are relatively few studies of ETV in developing countries. In terms of format, ETV can mimic interactive radio by including pauses for students to digest and react to content.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Estimating seroprevalence with data from an imperfect test on a convenience sample</title>
      <link>https://academic-demo.netlify.app/post/estimating-seroprevalence-with-data-from-an-imperfect-test-on-a-convenience-sample/</link>
      <pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/estimating-seroprevalence-with-data-from-an-imperfect-test-on-a-convenience-sample/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;&lt;em&gt;Update Jan, 2022: Since this post was published in May 2020, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gelmanBayesianAnalysisTests2020&#34; role=&#34;doc-biblioref&#34;&gt;Gelman and Carpenter&lt;/a&gt; (&lt;a href=&#34;#ref-gelmanBayesianAnalysisTests2020&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; have published a more comprehensive analysis on how to adjust for test imperfections using a Bayesian approach which goes beyond many of the ideas here. I recommend checking out their article.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Several recent studies have used data from antibody tests performed on a convenience sample to estimate seroprevalence of COVID-19 in a population. Estimating seroprevalence from this type of data presents two challenges. First, the analyst must take steps, through weighting or other measures, to deal with likely sample selection bias. Second, the analyst must take into account imperfections in the test itself. Addressing either of these challenges on their own is relatively straightforward to do using existing tools but addressing both at the same time is pretty tricky.&lt;/p&gt;
&lt;p&gt;In this blog post, I first describe the most commonly used tools for adjusting for test imperfections and performing inference on a convenience sample. I then describe two different ways of tackling both of these issues at once: a simple approach which sequentially applies the two simple approaches and a more complicated, but also more theoretically sound, Bayesian approach. I also report results from a quick Monte Carlo experiment I used to assess both approaches. A companion git &lt;a href=&#34;https://github.com/dougj892/sero_prevalence2&#34;&gt;repo&lt;/a&gt; includes code that (hopefully) can be relatively easily adapted to estimate seroprevalence from other studies using these approaches.&lt;/p&gt;
&lt;p&gt;The TLDR version of the results is that the naive approach seems to work fine for estimating overall population prevalence but that you should use the more sophisticated approach when generating estimates for subgroups.&lt;/p&gt;
&lt;div id=&#34;the-rogan-and-gladen-adjustment-to-account-for-test-imperfections&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The Rogan and Gladen Adjustment to Account for Test Imperfections&lt;/h2&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-roganEstimatingPrevalenceResults1978&#34; role=&#34;doc-biblioref&#34;&gt;Rogan and Gladen&lt;/a&gt; (&lt;a href=&#34;#ref-roganEstimatingPrevalenceResults1978&#34; role=&#34;doc-biblioref&#34;&gt;1978&lt;/a&gt;)&lt;/span&gt; developed a simple approach to adjust estimates of prevalence that takes into account test imperfections. If we define &lt;span class=&#34;math inline&#34;&gt;\(pt\)&lt;/span&gt; to be the true positive rate, &lt;span class=&#34;math inline&#34;&gt;\(se\)&lt;/span&gt; to be the test sensitivity (i.e. true positive rate), and &lt;span class=&#34;math inline&#34;&gt;\(sp\)&lt;/span&gt; to be the test specificity (i.e. the true negative rate), then the share of a population that will test positive is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ pa = se*pt+(1-sp)(1-pt) \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Rogan and Gladen approach to adjusting for test imperfections solves for &lt;span class=&#34;math inline&#34;&gt;\(pt\)&lt;/span&gt; in this equation and replaces the true values of each parameter with their estimated values.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{pt} = \frac{\widehat{pa}+\widehat{sp}-1}{\widehat{se}+\widehat{sp}-1} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Rogan and Gladen developed a few options for calculating standard errors of these estimates (and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-reiczigelExactConfidenceLimits2010&#34; role=&#34;doc-biblioref&#34;&gt;Reiczigel, Földi, and Ózsvári&lt;/a&gt; (&lt;a href=&#34;#ref-reiczigelExactConfidenceLimits2010&#34; role=&#34;doc-biblioref&#34;&gt;2010&lt;/a&gt;)&lt;/span&gt; developed more sophisticated confidence intervals). Rogan and Gladen’s simplest approach to estimating the standard errors (which I will use later) is to calculate the variance of &lt;span class=&#34;math inline&#34;&gt;\(\widehat{pt}\)&lt;/span&gt; using the formula for the variance of a proportion.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{Var({\widehat{pt}})}= \frac{\widehat{pa}(1-\widehat{pa})}{N(se+sp-1)^2}\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The R package “epiR” allows users to apply the Rogan and Gladen adjustment and calculate confidence intervals using a variety of approaches. (See the function “epi.prev” in this package.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;adjusting-a-convenience-sample-using-post-stratification&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Adjusting a Convenience Sample Using Post-Stratification&lt;/h2&gt;
&lt;p&gt;There are a lot of different ways to account for potential sample selection bias when analyzing data from a convenience sample but the simplest, and most commonly used, method is post-stratification. To apply post-stratification, we first divide up our samples into groups based on whatever demographic info we collected and calculate estimates for each group. We then weight the estimates for each of these groups according to the share of the total population that they represent. For example, if we sought to estimate the mean of some variable &lt;span class=&#34;math inline&#34;&gt;\(y\)&lt;/span&gt; for the total population our estimate would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{\bar{y}} = \sum_{h=1}^H{\frac{N_h*\bar{y_h}}{N}} \]&lt;/span&gt;
where &lt;span class=&#34;math inline&#34;&gt;\(\bar{y_h}\)&lt;/span&gt; is the estimate for group h, and &lt;span class=&#34;math inline&#34;&gt;\(\frac{N_h}{N}\)&lt;/span&gt; is the share of group h in the total population from, for example, census data.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-naive-approach-simple-poststratification-followed-by-a-rogan-gladen-adjustment&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The naive approach: simple poststratification followed by a Rogan Gladen adjustment&lt;/h2&gt;
&lt;p&gt;The simplest approach to adjusting for both test imperfections and potential sample selection bias arising from convenience sampling is to first estimate the apparent prevalence rate (without accounting for test imperfections) using poststratification and then apply the Rogan and Gladen adjustment.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;issues-with-the-naive-approach&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Issues with the naive approach&lt;/h2&gt;
&lt;p&gt;In theory, the naive approach shouldn’t work too well. To see why this is the case, suppose you have two strata of equal sample size but one stratum represents a much larger portion of the population than the other strata (i.e. if you were to use weights, the weights for observations from this stratum would be much higher than observations from other strata). Suppose also that true prevalence is very low. Due to random test error, you will likely have some false positives in your sample. If you happen to get a false positive in the stratum with high weights, then the naive approach will lead you to overestimate the overall true prevalence. On average, your estimate of the true prevalence will be Ok but it (in theory) will have pretty high variance. (I caveat these claims with the phrase “in theory” since, as we will see below, for the simulated data I create it isn’t actually that much of a problem.)&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;a-bayesian-approach-using-modified-mrp&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;A Bayesian Approach using Modified MRP&lt;/h2&gt;
&lt;p&gt;Theoretically, we should be able to improve on this approach by more carefully taking into account potential test imperfections. To use the example from above, if we saw that there was one positive test in the highly weighted stratum and 0 positive tests in the other stratum, we should adjust downward our overall estimate of the prevalence.&lt;/p&gt;
&lt;div id=&#34;quick-overview-of-mrp&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Quick overview of MRP&lt;/h3&gt;
&lt;p&gt;One way to do this is using a fully Bayesian approach built on multi-level regression and post-stratification (MRP). (For another Bayesian approach to this problem which doesn’t use MRP, see &lt;a href=&#34;https://www.medrxiv.org/content/10.1101/2020.04.15.20067066v1&#34;&gt;Larremore et al (2020)&lt;/a&gt;.)&lt;/p&gt;
&lt;p&gt;MRP is an approach to small area estimation in which the analyst first estimates the mean of each strata using a multi-level model and then weights up these estimates using the poststratification weights. For example, to estimate the overall proportion &lt;span class=&#34;math inline&#34;&gt;\(\theta\)&lt;/span&gt; in a population using data &lt;span class=&#34;math inline&#34;&gt;\(y_i\)&lt;/span&gt; for each individual, you might use a simple model as follows to first estimate, &lt;span class=&#34;math inline&#34;&gt;\(\theta_j\)&lt;/span&gt;, the proportion in each stratum j using stratum variables &lt;span class=&#34;math inline&#34;&gt;\(X_j\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \theta_j =logit^{-1}(X_j\beta); \beta\sim MVN(0,\sigma I); y_i \sim bernoulli(\theta_{j[i]}) \]&lt;/span&gt;
To derive your estimates of the total population, you just weight up. i.e. you calculate&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ \widehat{\theta} = \sum_{h=1}^H{\frac{N_h*\widehat{\theta_j}}{N}} \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;MRP is especially useful when you have a lot of different strata (which is often the case) since it allows you to more effectively “borrow strength” between strata compared to the approach where you simply model a different intercept for each stratum. (If you are simply modeling a separate intercept for each stratum, then there is no way for the model to know, for example, that a stratum for white men between 41 and 45 in Georgia and a stratum for white men between 46 and 50 are likely to be similar.) It is also, believe it or not, relatively straightforward compared to other approaches to small area estimate. For a more thorough overview of MRP, I highly recommend this &lt;a href=&#34;https://mc-stan.org/rstanarm/articles/mrp.html&#34;&gt;vignette&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;modified-mrp-to-account-for-test-imperfections&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Modified MRP to account for test imperfections&lt;/h3&gt;
&lt;p&gt;If implementing MRP using a Bayesian approach, it is fairly straightforward to modify the MRP model to take into account test error. As before, we use a multilevel model for the likelihood of the true prevalence. But in our likelihood of the test data, we use the apparent prevalence rate, which is the probability of a test being positive taking into account both prevalence and test imperfections, rather than the true prevalence. Lastly, we also model uncertainty in our estimates of the sensitivity and specificity using data on the number of true positives (tp), true negatives (tn), false positives (fp), and false negatives (fn) from a validation study of the antibody test.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ pt_j =logit^{-1}(X_j\beta); \beta\sim MVN(0,\sigma I)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ pa_j = se*pt_j+(1-sp)*(1-pt_j) \]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ se \sim binom(tp, tp+fn); sp \sim binom(tn, tn+fp)\]&lt;/span&gt;
&lt;span class=&#34;math display&#34;&gt;\[ y_i \sim bern(pa_{j[i]}) \]&lt;/span&gt;
For a complete Bayesian model, we also need to add priors for sensitivity and specificity.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;results-from-a-monte-carlo-simulation&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Results from a Monte Carlo Simulation&lt;/h2&gt;
&lt;p&gt;Theory is all well and good, but how do the two approaches compare when using data? To test this, I ran a simple Monte Carlo simulation using code borrowed from Kennedy and Gabry’s &lt;a href=&#34;https://cran.r-project.org/web/packages/rstanarm/vignettes/mrp.html&#34;&gt;MRP tutorial&lt;/a&gt;. (And big thanks to them for letting me copy their code!)&lt;/p&gt;
&lt;p&gt;Surprisingly, the naive approach actually did slightly better (in terms of average absolute deviation from the true seroprevalence) when it came to estimating overall seroprevalence. This is especially surprising since the data generating process used for the simulations is almost identical to my MRP model. The modified MRP process does much better when estimating subgroups (the Rogan and Gladen estimates for subgruops are often negative, which happens sometimes) but clearly, given the additional hassle of generating the code, the modified MRP approach is only worth it if you really want to estimate subgroup effects.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;for-people-interested-in-using-this-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;For people interested in using this code&lt;/h2&gt;
&lt;p&gt;All code for this analysis can be found &lt;a href=&#34;https://github.com/dougj892/sero_prevalence2&#34;&gt;here&lt;/a&gt;.
If you looking to copy and adapt the code, start with the R notebook “Estimate seroprevalence” in the above repo. In that notebook, I fit the modified MRP approach in two different ways: using raw Stan code and using the brms package (with some custom code to extend the package). If you would like to use the more complicated modified MRP approach, I strongly recommend you use the brms package. If you use the brms package, you should be able to copy and paste the code I created to define a “custom family” for the brms package and then modify the code in the main call to brm to suite your data. Since brms uses the lme4 syntax for defining multi-level models, customizing this code hopefully shouldn’t be too hard. By contrast, I find that modifying raw Stan code always takes quite a bit of time.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2 unnumbered&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-gelmanBayesianAnalysisTests2020&#34; class=&#34;csl-entry&#34;&gt;
Gelman, Andrew, and Bob Carpenter. 2020. &lt;span&gt;“Bayesian Analysis of Tests with Unknown Specificity and Sensitivity.”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 69 (5): 1269–83. &lt;a href=&#34;https://doi.org/10.1111/rssc.12435&#34;&gt;https://doi.org/10.1111/rssc.12435&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-reiczigelExactConfidenceLimits2010&#34; class=&#34;csl-entry&#34;&gt;
Reiczigel, J., J. Földi, and L. Ózsvári. 2010. &lt;span&gt;“Exact Confidence Limits for Prevalence of a Disease with an Imperfect Diagnostic Test.”&lt;/span&gt; &lt;em&gt;Epidemiology and Infection&lt;/em&gt; 138 (11): 1674–78. &lt;a href=&#34;https://doi.org/10.1017/S0950268810000385&#34;&gt;https://doi.org/10.1017/S0950268810000385&lt;/a&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-roganEstimatingPrevalenceResults1978&#34; class=&#34;csl-entry&#34;&gt;
Rogan, Walter, and Beth Gladen. 1978. &lt;span&gt;“Estimating &lt;span&gt;Prevalence&lt;/span&gt; from the &lt;span&gt;Results&lt;/span&gt; of a &lt;span&gt;Screening Test&lt;/span&gt;.”&lt;/span&gt; &lt;em&gt;American Journal of Epidemiology&lt;/em&gt; 107 (1): 71–76. &lt;a href=&#34;https://doi.org/10.1093/oxfordjournals.aje.a112510&#34;&gt;https://doi.org/10.1093/oxfordjournals.aje.a112510&lt;/a&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>An Alternative Approach to Power Calculations</title>
      <link>https://academic-demo.netlify.app/post/an-alternative-approach-to-power-calculations/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/an-alternative-approach-to-power-calculations/</guid>
      <description>
&lt;script src=&#34;https://academic-demo.netlify.app/rmarkdown-libs/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;p&gt;The typical approach to power calculations goes something like this: first, the evaluator estimates the smallest MDE for which the intervention would be cost-effective. Second, the evaluator calculates the sample required to detect that MDE. Third, the evaluator throws out the calculations from step two after realizing the evaluation would be way too expensive and instead estimates the MDE she can detect given her budget constraints.
The problem with this approach is that it doesn’t take into account the cost of the evaluation itself. In this blog post, I’ll show one way to design evaluations which takes into account the cost of the evaluation itself through the use of a simple toy example&lt;/p&gt;
&lt;div id=&#34;simple-example-with-a-perfectly-accurate-study&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Simple example with a perfectly accurate study&lt;/h1&gt;
&lt;p&gt;Suppose a funder has USD 1 million to spend and two options for how it can spend the money: it can either invest in a tried and true intervention which it knows will save 1,000 lives or it can invest in a new intervention. The funder believes that there is a 10% chance that the new intervention is significantly more effective than the tried and true intervention and that USD 1 million invested in the new intervention would save 5,000 lives. Unfortunately, they also believe that there is a 90% probability that the new intervention is less effective than the tried and true intervention and would only save 500 lives.
If the funder seeks to maximize expected lives saved, they would invest the entire USD 1 million in the tried in true intervention since the expected number of lives saved by investing in the new intervention is only $ .1&lt;em&gt;5000+.9&lt;/em&gt;500=950 $&lt;/p&gt;
&lt;p&gt;Now suppose the funder has the option of first funding a study which would reveal, with perfect accuracy, whether the new intervention is a block-bluster or a dud. To calculate the value of the study, we may estimate the “value of information” for the study defined as:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(I)=\sum_i P(i)EU(i)-EU \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Where I is the new information which we may obtain and EU(i) is the expected utility when I=i, P(i) is probability I=I, and EU is the expected utility without I (Kochenderfer, 2015). In other words, the value of information for a variable I is the increase in expected utility if that variable is observed.
In our case:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(study)=.1*5000+.9*1000-1000 = 400 \]&lt;/span&gt; lives&lt;/p&gt;
&lt;p&gt;If we additionally assume that the cost per life saved is constant for the two interventions (i.e. if you spent Y on the first intervention you would save Y/1,000,000*1000 lives), we may calculate the exact amount the funder would be willing to spend on the study. Assuming constant cost per life saved, the funder would be willing to spend up to $285,714 on this study. Note that up until this point we have not used Bayes’ theorem at all – just some simple algebra.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;removing-the-assumption-of-perfect-accuracy&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Removing the Assumption of Perfect Accuracy&lt;/h1&gt;
&lt;p&gt;Unfortunately, studies are never perfectly accurate. Suppose that if the intervention is a blockbuster there is a 95% probability that the study will say that it is a block buster. But if the intervention is a dud, there is still a 5% probability that the study will say that it is a blockbuster.&lt;br /&gt;
To calculate VOI for this new noisy study, we first calculate the probability that the study result is “blockbuster”. This probability is .9&lt;em&gt;.05+.1&lt;/em&gt;.95=.14. Next, we need to calculate the expected utility if the study result is positive and the expected utility if the result is negative. Here’s where things get a little trickier and where Bayes’ rule comes in handy. If we get a “blockbuster” result, our post-facto estimate of the probability that the intervention is really a blockbuster would be:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ P(bb|bb result)=(P(bb result |bb)*P(bb))/(P(bb result |bb)P(bb)+P(bb result|dud)P(dud))=.68 \]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Therefore, if the funder gets a “blockbuster” result from the study it would invest in the new intervention since the expected lives saved would be &lt;span class=&#34;math inline&#34;&gt;\(.68*5000+.32*500=3560\)&lt;/span&gt;.
I won’t calculate p(bb | dud result) since intuitively it seems fairly obvious that the funder would not invest in the new intervention if the study gave a “dud” result and all we care about for the VOI formula is the expected utility for each study result.
The value of information (in terms of lives saved) is:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[ VOI(noisy study)=P(bb result)*EU(bb result)+P(dud result)*EU(dud result)-1000=1358.4 \]&lt;/span&gt;
Thus, the noise in the study results reduces the value of the study information by 41.6 lives.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;more-complicated-effects-and-studies&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;More Complicated Effects and Studies&lt;/h1&gt;
&lt;p&gt;In the real world things are rarely binary: intervention effects and study estimates are typically continuous. For example, suppose we are investigating an intervention which seeks to reduce infant mortality. The funder probably doesn’t believe that the intervention is either a blockbuster or a dud. Rather, they probably believe that there is a small chance the intervention works great, a small chance is works well but not great, etc. Similarly, any potential study we perform on the intervention will spit out an estimated effect size and standard error rather than a simple up/down result. To calculate the value of information with continuous effect sizes / study results, we still apply the same formula as above but the calculations will get complicated very quickly so we will no longer be able to calculate things by hand. More on this later.&lt;/p&gt;
&lt;p&gt;[1]Kochenderfer, Mykel J. Decision making under uncertainty: theory and application. MIT press, 2015.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://academic-demo.netlify.app/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Three Stage Sampling</title>
      <link>https://academic-demo.netlify.app/post/3-stage/</link>
      <pubDate>Wed, 04 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/3-stage/</guid>
      <description>&lt;p&gt;One of IDinsight&amp;rsquo;s project teams is in the process of designing the sampling strategy for a large scale household survey and is considering using a three stage sampling design in which they would first select districts, then villages (or urban wards), and then households.  In addition, someone was asking about three stage clustering for an RCT somewhere on Slack (I can&amp;rsquo;t seem to find the slack post now) so I thought it might be useful to write a short post on three stage designs.&lt;/p&gt;
&lt;p&gt;In this post, I&amp;rsquo;ll try to answer four questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;When do you need to take into account both stages of clustering in a survey or evaluation?&lt;/li&gt;
&lt;li&gt;How do you properly account for a three stage design when performing sample size / power calculations?&lt;/li&gt;
&lt;li&gt;How should you estimate the inputs required for these calculations?&lt;/li&gt;
&lt;li&gt;How do you properly account for a three stage design when analyzing data?&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;when-you-do-you-need-to-take-into-both-stages&#34;&gt;When you do you need to take into both stages?&lt;/h1&gt;
&lt;p&gt;With an RCT, it&amp;rsquo;s pretty rare that you really need to take into account two stages of clustering. Remember that just because units exhibit some sort of clustering doesn&amp;rsquo;t mean that you need to adjust for clustering in your analysis.  For example, if you randomize at the student level it doesn&amp;rsquo;t matter that student learning outcomes exhibit clustering at the classroom level.  An example of when you might want to take into account two stages of clustering is when you randomize large clusters (e.g. schools) and then only collect data from units in a randomly sampled set of smaller clusters (e.g. kids in classrooms).  With surveys, anytime you have a three stage design you should theoretically take into account the clustering at both levels.&lt;/p&gt;
&lt;p&gt;Even when it makes sense in theory to take into account both stages of clustering, you can usually get by with just considering the most aggregate (highest) level of clustering.  We&amp;rsquo;ll see below why that makes sense.  In some cases, e.g. when you are trying to find the optimal survey design for a given budget, you do really need to take into account both stages of clustering.&lt;/p&gt;
&lt;h1 id=&#34;how-do-you-account-for-a-three-stage-design&#34;&gt;How do you account for a three stage design?&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;(The advice given below is tailored to someone performing sampling size calcs for a survey with a three stage design.  All of the advice holds true for power calcs as well.  You just need to multiply the final variance by 2 (since you have 2 groups &amp;ndash; treatment and control) and then use the standard adjustment to the standard error for power calcs &amp;ndash; i.e. instead of multiplying the standard error by +/-1.96 to create a 95% confidence interval you multiply by ~2.8 to calculate an MDE for alpha .05 and power .8.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s first recap how one stage of clustering affects the variance of your estimator.  Let&amp;rsquo;s say that you will use a two stage sampling strategy in which you will first randomly sample J clusters and then randomly sample K units from each cluster to estimate the mean of some variable y.  Further assume that the total number of units per cluster does not vary and is pretty large.  If values of y are correlated within each cluster, we can think of the values for y as being made up of a cluster component and an independent within-cluster component, i.e.&lt;/p&gt;
&lt;p&gt;$$y_{j,k}=\eta_j+\phi_{j,k}$$&lt;/p&gt;
&lt;p&gt;This allows us to calculate the variance the of y as:&lt;/p&gt;
&lt;p&gt;$$\sigma^2_y=\sigma^2_{\eta}+\sigma^2_{\phi}$$&lt;/p&gt;
&lt;p&gt;And the variance of the mean as:&lt;/p&gt;
&lt;p&gt;$$Var(\bar{y})=\frac{\sigma^2_{\eta}}{J}+\frac{\sigma^2_{\phi}}{JK}=\sigma^2_y\left(\frac{\rho}{J}+\frac{(1-\rho)}{JK}\right)$$&lt;/p&gt;
&lt;p&gt;Where \( \rho=\frac{\sigma^2_{\eta}}{\sigma^2_y} \).  It&amp;rsquo;s also useful to calculate the design effect, or the ratio of the variance of this estimator to the ratio of the estimator if the sample had been collected using simple random sampling (SRS). Since the variance under SRS would be \( \frac{\sigma^2_y}{JK} \) the design effect\(=1+(K-1)\rho\).&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s now suppose that we have a higher level sampling stage. We first pick Q mega-clusters, then J clusters from each mega-cluster, and then K households from each cluster.  Similarly, we can think of the values y as made of three components:&lt;/p&gt;
&lt;p&gt;$$y_{q,j,k}=\gamma_q+\eta_{q,j}+\phi_{q,j,k}$$&lt;/p&gt;
&lt;p&gt;The variance of y is then:&lt;/p&gt;
&lt;p&gt;$$\sigma^2_y=\sigma^2_{\gamma}+\sigma^2_{\eta}+\sigma^2_{\phi}$$&lt;/p&gt;
&lt;p&gt;And the variance of the mean is:&lt;/p&gt;
&lt;p&gt;$$Var(\bar{y})=\frac{\sigma^2_{\gamma}}{Q}+\frac{\sigma^2_{\eta}}{QJ}+\frac{\sigma^2_{\phi}}{QJK}=\sigma^2_y\left(  \frac{\rho_{\gamma}}{Q}+\frac{\rho_{\eta}}{QJ}+\frac{(1-\rho_{\gamma}-\rho_{\eta})}{QJK} \right)$$&lt;/p&gt;
&lt;p&gt;Where \(\rho_{\eta}=\frac{\sigma^2_{\eta}}{\sigma^2_y}\) and \(\rho_{\gamma}=\frac{\sigma^2_{\gamma}}{\sigma^2_y}\).  For our three stage sampling design, the design effect is:&lt;/p&gt;
&lt;p&gt;$$DEFF=1+(K-1)\rho_{\eta}+(JK-1)\rho_{\gamma}$$&lt;/p&gt;
&lt;p&gt;This also shows why just looking at the most aggregate level of clustering is usually pretty reasonable &amp;ndash; assuming the two ICCs are relatively similar in size, the adjustment to the variance will be driven primarily by the most aggregate level of clustering.&lt;/p&gt;
&lt;h1 id=&#34;how-should-you-estimate-the-inputs-required-for-these-calculations&#34;&gt;How should you estimate the inputs required for these calculations?&lt;/h1&gt;
&lt;p&gt;Now that we know how to adjust our estimate of the variance using the ICC at both levels, the next obvious questions is where to find the different values for the ICC.&lt;/p&gt;
&lt;p&gt;If you are lucky, you might find a dataset which includes both levels of clustering and the variable you are interested (or some similiar variable).  If so, then you can use Stata&amp;rsquo;s anova command to estimate the two ICCs.  I&amp;rsquo;m not really sure how to do this is Stata, but I think that it should be relatively straightforward if you search the help file for &amp;ldquo;nested anova.&amp;rdquo;  (And if you figure out how to do it please let me know!)&lt;/p&gt;
&lt;p&gt;Alternatively, you can resort to the typical hack of using a design effect calculated from another survey with a similar three stage design.  &lt;a href=&#34;https://unstats.un.org/unsd/hhsurveys/pdf/Chapter_7.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; are a few design effects from which to draw from. (As an aside, it would also be useful for us to start recording the design effects for key variables from our own surveys.  To get the design effect for a survey dataset in Stata first &amp;ldquo;svyset&amp;rdquo; your dataset, then estimate the population mean of a variable using &amp;ldquo;svy: mean &lt;variable&gt;&amp;rdquo;, and then run &amp;ldquo;estat effects&amp;rdquo; to get the design effect for the estimate.)&lt;/p&gt;
&lt;h1 id=&#34;how-do-you-properly-account-for-a-three-stage-design-when-analyzing-data&#34;&gt;How do you properly account for a three stage design when analyzing data?&lt;/h1&gt;
&lt;p&gt;Unfortunately, most Stata commands only allow for a single stage of clustering.  To account for two or more stages of clustering, you need to first &amp;ldquo;svyset&amp;rdquo; your data and then use the &amp;ldquo;svy&amp;rdquo; prefix before running any command.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simple Random Sampling vs. PPS Sampling</title>
      <link>https://academic-demo.netlify.app/post/srs-v-pps/</link>
      <pubDate>Fri, 21 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/srs-v-pps/</guid>
      <description>&lt;p&gt;A question came up on one of our evaluations on whether we should use simple random sampling (SRS) or probability proportional to size (PPS) sampling when selecting villages (our primary sampling units) for a matching study.  Under SRS, you randomly select primary sampling units (PSUs) until you reach your desired sample size.  With PPS sampling, you select your PSUs using some measure of size.  PPS is often used in a first stage of a two-stage sampling design because if you use PPS to select PSUs and then select a fixed number of units (households in our case) per PSU in the second stage of sampling, the probability of selection will be identical for all units.  (To see this, note that  the probability of selecting each PSU is \( n_1*w_i\)) where \( n_1\) is the number of PSUs you select and the probability of selecting a household in a village conditional on selecting the village in the first stage is \(\frac{n_2}{w_i}\) where \(n_2\) is the number of households sampled per village.  Note that this depends on your estimate of size being 100% accurate.  More details &lt;a href=&#34;https://en.wikipedia.org/wiki/Sampling_%28statistics%29#Probability-proportional-to-size_sampling&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.  Code to perform PPS without replacement &lt;a href=&#34;https://ideas.repec.org/c/boc/bocode/s454101.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;A quick way to answer this question is to use something called the &amp;ldquo;design effect.&amp;rdquo;  The design effect is defined as the ratio of the variance of your estimate under a given sampling scheme to the variance of your estimate under SRS of final units.  (Note that performing SRS on your final units, here households, is not the same as performing SRS of PSUs and then selecting a fixed number of units per PSU.) Design effects are typically used to estimate sample size requirements for population-based surveys that don&amp;rsquo;t use SRS.  For example, you may know that for a household survey in India looking at consumption expenditure and using a certain sampling strategy, the design effect is likely to be around 2. To estimate the required sample size for this survey, you first estimate the sample size required under SRS and then double it. (Remember that the variance of a mean of a sample drawn using SRS is \(\frac{\sigma^2}{N}\) so if you multiple the variance by X you also need to multiply the sample size by X to get the same variance.)&lt;/p&gt;
&lt;p&gt;If we used SRS to select villages and then selected a fixed number of households per village, we might want to weight each household by the number of households in the village, or \( \frac{w_i}{\sum{w_i}}  \), when performing our final analysis so that our estimates are representative of the entire population.  (I say &amp;ldquo;might&amp;rdquo; because there are differing opinions on this.  Stay tuned for a book club discussion on this topic.)  If we do use weights in our analysis, and  if we assume constant variance \( \sigma_y^2 \) per village and do a little hand waving, the variance of our estimate of the mean of a variable in the treatment group would be roughly:&lt;/p&gt;
&lt;p&gt;$$Var(\bar{y}_{w})=Var\left(\sum{\frac{w_iy_i}{\sum{w_i}}}\right)=\frac{\sum{w_i^2}}{\left(\sum{w_i}\right)^2}\sigma_y^2$$&lt;/p&gt;
&lt;p&gt;Where \( w_i \) is the size of village i.&lt;/p&gt;
&lt;p&gt;If we sample using PPS, our variance would be roughly the variance for an estimate under SRS of final units (which, again, is different from SRS of PSUs) which would just be \( \frac{\sigma_y^2}{N} \).  Thus, our design effect is&amp;hellip;&lt;/p&gt;
&lt;p&gt;$$\frac{\sum{w_i^2}}{\left(\sum{w_i}\right)^2}N$$&lt;/p&gt;
&lt;p&gt;To estimate the final sample size required if using weights, we first calculate the sample size required using standard power calculations and then multiply this by our estimate of the design effect. Note that this is a pretty rough calculation (for example, I&amp;rsquo;m not taking into account the fact that both sampling schemes involve two stages), but it gives you an approximate idea of how the sampling scheme will affect power.&lt;/p&gt;
&lt;p&gt;Another consideration in choosing between the two sampling schemes for this evaluation is that we have to do a full household listing in each village.  On average, the villages selected using PPS will be significantly larger than under simple random sampling (where the expected value of the village sizes would be equal to the average village size).  The formula for the expected size of the PSU under PPS (assuming we are just selecting one PSU or selecting with replacement) is&amp;hellip;&lt;/p&gt;
&lt;p&gt;$$E[w_i]=\frac{\sum{w_i^2}}{\sum{w_i}}$$&lt;/p&gt;
&lt;p&gt;To derive this, note that the expected value of a variable is the sum of the probability of selecting each value of the variable times the value.  Under PPS, the probability of selecting each village is \(\frac{w_i}{\sum{w_i}} \).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Web Scraping 101</title>
      <link>https://academic-demo.netlify.app/post/web-scraping/</link>
      <pubDate>Wed, 31 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/web-scraping/</guid>
      <description>&lt;p&gt;More and more organizations are publishing their data on the web. This is great, but often websites don’t offer an option to download a clean and complete dataset from the site.  In this situation, you have two options.  First, you (or some unlucky intern) can hunker down and spend a week wearing out the ‘c’ and ‘v’ keys on your keyboard as you cut and paste ad nauseum from the website to an Excel spreadsheet.  Second, you can use a tool like python to automatically “scrape” the data from the website. In this blog post, I’ll demonstrate how to use python to “scrape” the names of all IDinsight staff from our website.&lt;/p&gt;
&lt;h2 id=&#34;step-one--check-out-the-raw-html&#34;&gt;Step one — Check out the raw html&lt;/h2&gt;
&lt;p&gt;Often, the first step in scraping a website is to get familiar with the raw html code for the site.  Let’s do this for the IDinsight staff webpage.  Open “http://idinsight.org/about/team/staff/“ in a browser and then view the raw html for this webpage.  (To do this in Safari, select “show page source” from the “develop” menu.  If you don’t see the “develop” menu, follow &lt;a href=&#34;%22http://osxdaily.com/2011/11/03/enable-the-develop-menu-in-safari/%22&#34;&gt;these steps&lt;/a&gt; to add the &amp;ldquo;develop&amp;rdquo; menu.)&lt;/p&gt;
&lt;p&gt;When you look through the html, you will see that details for each staff member is coded in an html block similar to the following:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-html&#34;&gt;&amp;lt;div class=&amp;quot;bio-info&amp;quot;&amp;gt;
  &amp;lt;h3&amp;gt;Abhinav Gupta&amp;lt;/h3&amp;gt;
  &amp;lt;h4&amp;gt;Senior Finance and Operations Manager&amp;lt;/h4&amp;gt;
  &amp;lt;div class=&amp;quot;bio-details&amp;quot; class=&amp;quot;hide&amp;quot;&amp;gt;
    &amp;lt;h3&amp;gt;Abhinav Gupta&amp;lt;/h3&amp;gt;
    &amp;lt;p&amp;gt;
      &amp;lt;p&amp;gt;&amp;lt;a href=&amp;quot;mailto:abhinav.gupta@idinsight.org&amp;quot;&amp;gt;abhinav.gupta@IDinsight.org&amp;lt;/a&amp;gt;&amp;lt;/p&amp;gt;
      &amp;lt;p&amp;gt;Abhinav is a Senior Finance and Operations Manager at IDinsight’s India office. He brings more than eight years of financial advisory and consulting experience in India and U.K.&amp;lt;/p&amp;gt;
      &amp;lt;p&amp;gt;Prior to joining IDinsight, Abhinav was a Manager with the M&amp;amp;amp;A Transaction Services department of Deloitte in India. As a consultant, he advised clients across several sectors in infrastructure, telecommunications, retail and e-commerce.&amp;lt;/p&amp;gt;
      &amp;lt;p&amp;gt;Abhinav holds a master&amp;amp;#8217;s degree in Economics from University of Cambridge, UK and is a qualified Chartered Accountant from Institute of Chartered Accountants of England and Wales.&amp;lt;/p&amp;gt;
    &amp;lt;/p&amp;gt;
  &amp;lt;/div&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This suggests that if we want to get the names of each staff member, one strategy would be to search through the html code for each block of code starting with “&amp;lt;div class=&amp;ldquo;bio-info”&amp;gt;” and then search inside these code blocks for the text inside the html tags &lt;h3&gt; and &lt;/h3&gt;.&lt;/p&gt;
&lt;h2 id=&#34;step-2---use-beautifulsoup-to-parse-the-html&#34;&gt;Step 2 - Use BeautifulSoup to parse the html&lt;/h2&gt;
&lt;p&gt;The python package “requests” allows you to download the html code from a webpage and the python package “BeautifulSoup” parses the html from a webpage so that you can easily find specific code blocks.  The following python code downloads and parses the html for the IDinsight staff page.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# import the packages that we will need to scrape the website.
# The BeautifulSoup package helps parse html
from bs4 import BeautifulSoup
# The requests package just downloads the html
import requests

# download the html for the idinsight staff webpage
response = requests.get(&amp;quot;http://idinsight.org/about/team/staff/&amp;quot;)
# parse the html using the BeautifulSoup function
soup = BeautifulSoup(response.content, &amp;quot;lxml&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;step-3---use-beautifulsoup-to-search-the-parsed-html&#34;&gt;Step 3 - Use BeautifulSoup to search the parsed html&lt;/h2&gt;
&lt;p&gt;Now, we want to search the parsed code for code blocks starting with “&amp;lt;div class=&amp;ldquo;bio-info”&amp;gt;” and search inside these code blocks for the text inside the html tags &lt;h3&gt; and &lt;/h3&gt;.  The following python code performs these operations.  BeautifulSoup is a little tricky, so I’m not going to explain this code in detail, but as you can see it is quite powerful.  For more on the BeautifulSoup package see &lt;a href=&#34;%22https://www.crummy.com/software/BeautifulSoup/bs4/doc/%22&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# use the BeautifulSoup function &#39;findall&#39; to get a collection of div tags with the class &#39;bio-info&#39;
bios = soup.findAll(&#39;div&#39;, {&#39;class&#39;:&amp;quot;bio-info&amp;quot;})
# for each of the div tags with class &#39;bio-info&#39;, search for the h3 tag and get the contents of this tag
names = [bio.find(&#39;h3&#39;).contents[0] for bio in bios]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Lastly, we can use the following python code to check that we successfully scraped all staff names.&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# print names of all staff
for person in names:
  print(person)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Voila!  We&amp;rsquo;re done.  For a more advanced example of web scraping, this &lt;a href=&#34;%22https://github.com/dougj892/Jupyter-notebooks/blob/master/Scraping%203ie_v3.ipynb%22&#34;&gt;link&lt;/a&gt; includes code that downloads all the metadata for all of the studies in the 3ie impact evaluation repository.  (If you just want the full dataset of all the metadata, let me know and I can share it with you.)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multiple Hypothesis Testing</title>
      <link>https://academic-demo.netlify.app/post/mult-hypothesis/</link>
      <pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/mult-hypothesis/</guid>
      <description>&lt;p&gt;This week, I volunteered to read and summarize one of the articles for IDinsigh&amp;rsquo;s tech team&amp;rsquo;s book club. The topic for this week is multiple hypothesis testing and the article I volunteered to summarize is &amp;ldquo;Multiple Inference and Gender Differences in the Effects of Early Intervention: A Reevaluation of the Abecedarian, Perry Preschool, and Early Training Projects&amp;rdquo; by Michael Anderson.  You can find an ungated version of the article &lt;a href=&#34;http://ist-socrates.berkeley.edu/~raphael/IGERT/Workshop/Anderson%20Preschool.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Since not everyone at IDinsight has time to participate in these calls and I tend to forget anything that I don&amp;rsquo;t write down I thought I&amp;rsquo;d do a blog post on the article.  I&amp;rsquo;m not going to summarize the article itself, but rather just the key takeaways.&lt;/p&gt;
&lt;h1 id=&#34;the-problem-with-multiple-hypothesis-testing&#34;&gt;The problem with multiple hypothesis testing&lt;/h1&gt;
&lt;p&gt;The problem with conducting multiple hypothesis tests is simple: if you test conduct 100 hypothesis tests at the 5% level, even if all of the null hypotheses are true you would expect to reject around 5 hypotheses. This isn&amp;rsquo;t necessarily an issue if you keep in mind that you &lt;em&gt;should&lt;/em&gt; expect to see around 5 rejections, but it definitely is a problem if you go cherry picking for results like this&amp;hellip;&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;http://imgs.xkcd.com/comics/significant.png&#34; alt=&#34;XKCD on p hacking&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;There are two main approaches to dealing with this problem.  I&amp;rsquo;ll first talk about the most obvious way to deal with it which is to reduce the number of tests by aggregating your outcome variables.  Next, I&amp;rsquo;ll talk about an alternate approach which instead adjusts the p-values of each test to account for the multiple testing.  In addition to these two main approaches to the problem, I&amp;rsquo;ll also talk about my favorite approach: ignoring the problem.  (But we warned, you can only get away with this under certain conditions! Read below for more.)&lt;/p&gt;
&lt;h1 id=&#34;approach-1---reduce-the-number-of-tests&#34;&gt;Approach 1 - Reduce the number of tests&lt;/h1&gt;
&lt;p&gt;The most straightforward approach to dealing with the problem of multiple hypothesis tests is to reduce the number of tests by aggregating your outcome variables into an index.  In principle, you could aggregate your outcome variables into a single index however you wanted but some ways make more sense than others.  For example, it probably isn&amp;rsquo;t a good idea to take a simple average of different outcomes because the relative weighting of each variable would depend greatly on the scale on which each variable is measured.&lt;/p&gt;
&lt;p&gt;The approach Anderson uses is as follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Normalize each outcome variable by subtracting the mean and dividing by the standard deviation of the control group. In symbols, calculate:&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;$$\tilde{y}&lt;em&gt;{ik}=\frac{y&lt;/em&gt;{ik}-\bar{y}&lt;em&gt;k}{\sigma&lt;/em&gt;{k,c}}$$&lt;/p&gt;
&lt;p&gt;Where i indexes observations and k indexes outcomes.&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Calculate \( \hat{\Sigma} \), the sample variance covariance matrix for this vector of transformed outcomes&lt;/li&gt;
&lt;li&gt;Calculate \( \mathbf{s_i}=\hat{\Sigma}^{-1}\cdot \mathbf{\tilde{y_i}} \) the dot product of the inverse of the sample variance covariance matrix and the vector of outcomes for each observation&lt;/li&gt;
&lt;li&gt;Add up the elements of  \( \mathbf{s_i} \) to get your final index&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Unfortunately, I am unable to find code in Stata to do this. &lt;a href=&#34;http://nbviewer.jupyter.org/github/dougj892/Jupyter-notebooks/blob/master/Anderson%20summary%20index.ipynb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&lt;/a&gt; is some sample code showing how to do this in Python.  If anyone finds / writes code to do this, please let the rest of us know.&lt;/p&gt;
&lt;p&gt;Anderson&amp;rsquo;s approach seems like a good approach if there is no alternative principled way to aggregate the outcome variables.  As Anderson points out, this approach increases power by weighting those outcome variables that are not highly correlated with other outcome variables more. In some cases there might be a more natural theory based way of aggregating the outcome variables.  For example, if you think the various outcome variables all measure some latent variable you might use item response theory or something else to estimate the latent variable.&lt;/p&gt;
&lt;p&gt;As a sidenote, as great way to test for balance at baseline in an RCT is to use randomization inference on a single index constructed using basically the same approach.  See &lt;a href=&#34;http://projecteuclid.org/download/pdfview_1/euclid.ss/1219339114&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for more info.&lt;/p&gt;
&lt;h1 id=&#34;approach-2---adjust-your-p-values&#34;&gt;Approach 2 - Adjust your p-values&lt;/h1&gt;
&lt;p&gt;A second approach to dealing with multiple tests is to adjust your p-values to take into account the fact that you are conducting multiple tests at the same time.  The typical way to do this is to control the Family-Wise Error Rate (FWER), defined as the probability of making a single type 1 error (i.e. rejecting the null when the null is true).  (This isn&amp;rsquo;t the only thing you could control for though.  See &lt;a href=&#34;https://normaldeviate.wordpress.com/2012/10/04/testing-millions-of-hypotheses-fdr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt; for an alternative.) The easiest, but most conservative, way to control the FWER is to simply multiply your p-values by the number of tests you are conducting.  This method, called the Bonferroni method, works because the probability of making any type 1 error can&amp;rsquo;t be larger than the sum of the probabilities of making a type 1 error when all the nulls are true.  Due to its simplicity, the Bonferonni method is useful for performing power calculations.&lt;/p&gt;
&lt;p&gt;The Bonferroni method works, but, as I mentioned, it&amp;rsquo;s &lt;em&gt;really&lt;/em&gt; conservative.  There a couple of ways we can do better: by using a step-down method and by taking into account the correlation between test statistics.  Step down is a nifty trick in which you first order p-values smallest to largest and then sequentially reject null hypotheses using the smallest p-value first until you encounter a p-value that is too large after which you fail to reject all further hypotheses.  This &lt;a href=&#34;https://en.m.wikipedia.org/wiki/Holm%e2%80%93Bonferroni_method&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page&lt;/a&gt; has a good explanation of why this works for the step down equivalent of the Bonferroni method, the Holm-Bonferroni.&lt;/p&gt;
&lt;p&gt;The other way you can get more power is by taking into account the correlation between test statistics.  The basic idea here is that if your test statistics are all perfectly correlated you shouldn&amp;rsquo;t be adjusting your p-values at all because you should either reject all of the hypotheses or fail to reject all of them.&lt;/p&gt;
&lt;p&gt;Anderson uses a combination of these two tools to increase the power.  If you want more details of the intuition behind this technique, I found this &lt;a href=&#34;http://statistics.berkeley.edu/sites/default/files/tech-reports/633.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article&lt;/a&gt; helpful.&lt;/p&gt;
&lt;p&gt;Again, we were unable to find code in Stata to do this. If anyone has any suggestions, please let us know.&lt;/p&gt;
&lt;h1 id=&#34;approach-3---ignore-the-problem-see-disclaimer-below&#34;&gt;Approach 3 - Ignore the problem (see disclaimer below)&lt;/h1&gt;
&lt;p&gt;Now we come to my personal favorite approach &amp;ndash; ignoring the problem. But first, I should offer a strong disclaimer: this is only a reasonable option in certain circumstances.  To see why you sometimes might want to ignore the multiple hypothesis testing problem, consider a situation in which you happen to run 10 completely independent RCTs for a single client at the same time which eack look at completely different interventions targeting different outcomes. In this case, you almost certainly wouldn&amp;rsquo;t want to correct the p-value for one RCT to take into account the fact that you happen to be running another RCT at the same time.  Often the decision of whether one should correct for multiple hypothesis testing or not is subtle and depends on your perspective.  I don&amp;rsquo;t want to go into the full debate here, but I think that a useful approach for determining whether ignoring the problem is to first answer the following two questions.  First, are each of the hypothesis tests informing a separate independent decision or are they informing a single decision?  For example, is the purpose of the hypothesis tests to determine whether to individually continue each of ten programs or is the purpose to determine whether any of the ten programs?  Second, will the results from the evaluation be shared with the outside world or just with the client?  If the answer to the first question is &amp;ldquo;independent decisions&amp;rdquo; and the answer to the second is &amp;ldquo;just the client,&amp;rdquo; you might consider ignoring the problem.  (But make sure to run this by your TT point person first!)&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://academic-demo.netlify.app/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://academic-demo.netlify.app/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://academic-demo.netlify.app/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title>Response to Blattman&#39;s Post on Why What Works Is The Wrong Question</title>
      <link>https://academic-demo.netlify.app/post/blattman/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://academic-demo.netlify.app/post/blattman/</guid>
      <description>&lt;p&gt;Last week, Chris Blattman published a &lt;a href=&#34;http://chrisblattman.com/2016/07/19/14411/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;long blog post&lt;/a&gt; titled “Why ‘what works?’ is the wrong question: evaluating ideas not programs.”  In the blog post, which was adapted from a talk he gave at DFID, Blattman argues that a) impact evaluations should focus on deeper, theory-driven questions rather than just whether a program works or not and b) researchers should design impact evaluations to allow for generalizability by paying attention to context and running multiple evaluations in multiple contexts.&lt;/p&gt;
&lt;p&gt;There’s a lot to like in this post, but ultimately, it left me frustrated— not because I didn’t agree with the substance of the arguments, but because I think he squandered a great opportunity to push DFID in the direction of better evaluation.&lt;/p&gt;
&lt;p&gt;First, the bit I liked. Blattman’s argument that researchers should design impact evaluations with generalizability in mind struck a chord with me.  As Eva Vivalt and others have shown, context matters a lot for impact and extrapolating results from one context to another is really, really hard.  Thus, if your goal is the creation of general knowledge you should care just as much about external validity as you should about internal validity: an extremely rigorous result in one context is of no use if policymakers can’t figure out how the impact would like change if adapted to another context.  As Blattman points out, this implies a shift in how we go about doing impact evaluations that seek to create general knowledge.  Rather than one-off evaluations in one context, we should first think hard about context and then attempt to run multiple evaluations in multiple contexts all testing the same basic idea.&lt;/p&gt;
&lt;p&gt;Yet while I liked Blattman’s call for more attention to generalizability, I was disappointed that he didn’t more explicitly tell DFID how to make this happen.  Let me take a step back here and review how donors like USAID and DFID decide which programs to conduct an impact evaluation on (based on my, admittedly limited, experience).  First, they design a program. Then, if the person in charge of the program is a fan of impact evaluations, that person will set aside a portion of the budget to run an impact evaluation.  (There are exceptions to this rule, like DIV, but in general this is how it works.)&lt;/p&gt;
&lt;p&gt;In this context, calling for donors to conduct multiple trials in multiple places of the same idea is a bit of a pipe dream: an evaluator would have to have to simultaneously convince multiple program managers not only to participate in an impact evaluation but also to subjugate project design and scheduling considerations to the needs of the impact evaluation.  In my view, the way around this is for donors to clearly distinguish between evaluations whose main purpose is to improve the program being evaluated and evaluations whose main purpose is to create general knowledge.  (For more on that distinction, see &lt;a href=&#34;http://idinsight.org/idinsight-presents-at-3ie-evidence-week-on-the-future-of-impact-evaluation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.)  For the former, the existing method of leaving the evaluation decision up to program manager is, for the most part, fine.  In contrast, for the latter, donors should first identify the big questions that they want to answer and then identify which programs can help them answer these questions.&lt;/p&gt;
&lt;p&gt;Suggesting that an large bureaucracy should add yet another centralized, bureaucratic process is always a dangerous proposition but, in this case, I think it is necessary.  Currently, at donors like USAID and DFID  impact evaluations tend to be conducted on programs run by people who are sympathetic to impact evaluations rather than on programs for which there is little evidence or which should be a high priority for an evaluation for other reasons.  This not only means that the type of ambitious, multi-site trials that Blattman suggests are infeasible, but that most impact evaluations tend to be up-or-down assessments of large, complex programs which yield little in the way of useful results.  A more centralized system for identifying priority research questions to be address by KFEs would make these evaluations much more useful.&lt;/p&gt;
&lt;p&gt;Lastly, I was going to make one last point about Blattman’s call for more theory-driven evaluations, but this post is already getting way too long so will save that for another post!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
