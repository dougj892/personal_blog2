---
title: Did same language subtitling increase reading levels in Maharashtra?
author: ''
date: '2020-06-02'
slug: same-language-subtitling
categories: []
draft: true
tags:
  - Education
---
TLDR: Maybe, but we can’t say much from the data. In my next blog post, I argue that we should fund SLS anyway.

# Summary

Between 2013 and 2015, Zee Talkies, with the support of PlanetRead and USAID, added subtitles to all of the songs of many of the Marathi movies it broadcast. The organizers of the project hoped that by adding the subtitles, a strategy known as “same language subtitling,” they would get children to read along as they listened to the songs and thus improve their reading skills. 

A quick analysis of ASER scores from Maharashtra during that period of time suggests that the strategy may have been successful.  In 2014, 53.5% of rural children in grade 5 could read a standard 2 level text.  In 2016, 62.6% could. There was a similar (though smaller) improvement in grade 3 reading scores.  (There was no improvement in grade 8 reading scores.) In The Huffington Post, Brij Kothari, founder of Planet Read [argues](https://www.huffpost.com/entry/what-caused-maharashtras-leap-in-reading_b_589d1277e4b0e172783a9a8f) argues that this jump in reading scores was likely caused by the SLS.

In this blog post, I use a larger dataset of ASER data going back to 2007 (a year after ASER started) and the synthetic control approach to more rigorously test this claim. Unfortunately, the larger dataset and more sophisticated methods show that there is little evidence to either support or reject this claim.  

All code for this post is here. The ASER dataset is [here]("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%20trends%20over%20time.csv"). 

# Same Language Subtitling

The idea behind SLS is pretty simple: kids naturally follow the subtitles as they listen to a video and thus pick up on spelling.  For an example of SLS, check out [this video](https://www.youtube.com/watch?v=MHGSG1QmbgQ) from Planet Read.

The beauty of SLS is that the number of learners reached per dollar spent is, potentially, phenomenally high. In India, TV ownership is relatively high even among low income households.  And the costs of adding subtitles, while not negligible, are pretty modest.  (See the graph in my recent [Scroll article](https://scroll.in/article/961937/small-screen-big-impact-educational-tv-could-be-indias-next-frontier-in-remote-learning) with Rob for more data on TV ownership by socio-economic status.) Kothari estimates that adding SLS to all programming in India would cost approximately XXX.  In addition, these costs would likely come down over time as TV channels could simply demand scripts for new content. 

Despite the high potential of SLS, there isn’t a lot of evidence on its impact.  Several studies which track subject’s eye movements as they watch SLS videos (including one by PlanetRead of children in Rajasthan) have found that viewers pay attention to the subtitles in SLS videos.  But few studies have looked at the impact of SLS on reading ability.  According to a [review]( https://turnonthesubtitles.org/research/) of the evidence by turnonthesubtitles (a group dedicated to SLS), the evidence from Maharashtra provides the most conclusive evidence in favour of SLS’s impact on reading.  

# Basic analysis of ASER Data

Before performing more sophisticated analyses, it’s useful to first explore our data.  In this, and the rest of the analyses on this page, I focus on the 4 foundational literacy and numeracy scores included in the ASER trends over time report: portion of grade 3 children who can read a grade 1 text, portion of grade 5 children who can read a grade 2 text, portion of grade 3 children who can perform subtraction, and the portion of grade 5 children who can perform division.  While individual ASER reports from each year include other indicators, I have data for these indicators going back in time. 

First, let’s look at Maharashtra’s grade 5 reading scores over time. (The graph for grade 3 reading looks very similar.)

```{r setup, include=FALSE, message=FALSE} 
library(tidyverse); library(Synth)
aser <- read_csv("https://raw.githubusercontent.com/dougj892/public-datasets/master/ASER%20trends%20over%20time.csv") %>% 
  mutate_at(vars(-state_abbr, -State), as.numeric) %>% 
  select(year, State, state_abbr, std3_read = std3_read_std1_all, std5_read = std5_read_std2_all, std3_math  = std3_subtract_all, std5_math = std5_divis_all) %>%
  filter(year != 2018)
```

```{r echo =FALSE, message = FALSE}
ggplot(subset(aser, state_abbr == "MH"), aes(x = year, y = std5_read)) + 
  geom_line() +
  labs(title = "Maharashtra Grade 5 Reading", y = "% rural grade 5 children can read grade 2 text")
```
Now, let's look at the changes in Maharashtra between 2013 and 2016 on all of the ASER indicators and compare these changes to changes in other states so we can get a sense of whether the changes in Maharashtra are relatively large. 

```{r, echo = FALSE}
# Generate dateset of deltas from 2013 to 2016
diffs <- aser %>% filter((year == 2013) | (year == 2016)) %>% group_by(State) %>%
  mutate_at(vars(starts_with("std")), ~.x-lag(.x, order_by = year)) %>% 
  filter(year == 2016) %>% 
  ungroup()

diffs_long <- diffs %>% pivot_longer(starts_with("std"), names_to = "temp", values_to = "diff") %>% 
  mutate(class = str_sub(temp, 4,4), subject = str_extract(temp, "(math|read)"))


# plot the reading changes for 2016
ggplot(diffs_long, aes(x = State, y = diff, fill=factor(ifelse(State=="maharashtra","Highlighted","Normal")))) + 
  scale_fill_manual(name = "area", values=c("red","grey50"), guide= FALSE) +
  geom_bar(stat = "identity")+
  facet_grid(subject ~ class) +
  coord_flip() +
  labs(y = "Change from 2014 to 2016", x =element_blank(), title = "Change in ASER score 2014-2016 by state, subject and grade")

```
 
The figures reveal a few things. First, the jump in grade 5 reading levels in Maharashtra between 2014 and 2016 was large compared to other states.  In fact, Maharashtra experienced the largest gain in grade 5 literacy of any state. In addition, the gain in grade 5 math was much more modest. Both of these facts suggest that something caused an increase in Maharashtra's literacy levels.  On the other hand, gains in grade 3 reading were much more modest and were similar to gains in grade 3 math.  In addition, looking at reading scores over time it seems like Maharashtra may just be bouncing back from one particularly bad year.

My takeaway from these figures is that Kothari's argument is plausible but by no means certain. The reading gains are large but, on the other hand, the data are noisy. 

# The Synthetic Control Method (SCM)

The synthetic control method offers a more rigorous approach to the classic single unit case study.  Traditionally, when attempting to estimate the impact of a policy implemented in only one state or region, researchers would pick one other state or region to use as the control This is in fact what Kothari does in his article in The Huffington Post. (He uses Gujarat as the control.) This approach is obviously fraught.  States may vary significantly in their policies and trends over time and thus it is questionable whether any one state would serve as a good control for another state. 

The synthetic control method seeks to create a better control for the state in which the policy or intervention has been implemented by using a weighted average of the outcomes for several other states which display similar trends in the outcome prior to the intervention and have similar covariate values. [Abadie, Diamond, and Hainmuller (2011)](https://www.tandfonline.com/doi/pdf/10.1198/jasa.2009.ap08746) show that the if this "synthetic control" matches the intervention state in terms of pre-intervention outcomes and covariates, and outcomes are determined by a factor model (see article for more details), the synthetic control will likely do a good job of predicting the outcome in the intervention state after the implementation of the intervention. 

A second advantage of the synthetic control approach is that it provides a natural way to test the reliability of our estimate through the use of placebo tests. For example, we may conduct a set of placebo test by dropping the intervention state from our sample and then calculating the "effect" for each of the other states. By comparing the size of the effect for the actual intervention state with the size of the placebo effects for the other states, we may get a sense of how robust our estimate is. 

[Abadie (2019)](https://economics.mit.edu/files/17847) provides an excellent overview of the method. 

# Applying SCM to the case of SLS in Maharashtra

I use SCM to estimate the impact of SLS on grade 5 reading levels in Maharashtra between 2014 and 2016. I start with grade 5 reading levels for two reasons.  First, based on the figures above, it seems like I am much more likely to see an impact on grade 5 reading than grade 3 reading.  Thus, if I don't find a clear effect on grade 5 reading (which I don't) it is safe to give up. Second, if I use grade 5 reading levels as the outcome I can use grade 3 reading levels from the previous round as a covariate which should increase my precision substantially. 

There is now a cottage industry of synthetic control approaches. I use the original synthetic control approach as described in Abadie, Diamond, and Hainmuller due to the availability of the R Synth package to apply this approach. 

To apply the synthetic control approach, we must specify a set of predictors and a set of linear combinations of the pre-intervention outcomes variables to generate the synthetic control match. For predictors, I use grade 3 reading scores in 2014 and grade 5 math scores in 2016.  (It is Ok to use "predictors" measured after the intervention as long as the intervention doesn't effect those predictors.) For linear combinations of the pre-intervention outcome variables, I use the mean from 2007 to 2014 and the mean in 2013 and 2014.  

I also specify a vector which controls how much weight to assign each of these predictors in the matching process. (In most contexts, you can let the Synth package do this by figuring out which predictors best predict the outcome variable prior to the intervention.  In our case, this wouldn't work since, for example, 2016 grade 5 math scores would not do a good job of predicting 2008 grade 5 reading.) See the code for details of this vector.

```{r include = FALSE}
# Change 2016 to 2015. Otherwise, the lack of 2015 throws an error.
aser$year[aser$year ==2016] <- 2015

# list the states with any NAs for std3_read or std5_read and then drop them. 
#  I also drop 2006 data because for some reason that is giving me the unbalanced error
states_w_na <- unique(aser$state_abbr[is.na(aser$std3_read) | is.na(aser$std5_read)])
aser <- aser %>% filter(!(state_abbr %in% states_w_na)) %>% 
  filter(year != 2006) 
  
# Create index for state (this is required by the Synth package)
aser <- aser %>% group_by(state_abbr) %>% mutate(id = group_indices())
```

```{r include = FALSE}
# Estimate SCM just for MH
v <- c(.15,.2,.2,.2,.25)

dataprep.mh <- dataprep(foo = as.data.frame(aser),
           predictors = c("std3_read", "std5_read"),
           predictors.op = "mean",
           time.predictors.prior = 2007:2013,
           special.predictors = list(
             list("std5_read", 2012:2013, "mean"),
             list("std3_read", 2013, "mean"),
             list("std5_math", 2014:2015, "mean")
           ),
           dependent = "std5_read",
           unit.variable = "id",
           unit.names.variable = "state_abbr",
           time.variable = "year",
           treatment.identifier = 11,
           controls.identifier = seq(1,max(aser$id),1)[-11],
           time.optimize.ssr = 2007:2013,
           time.plot = 2007:2015)
mh <- synth(data.prep.obj = dataprep.mh, custom.v = v)
mh.tables <- synth.tab(mh, dataprep.mh)

# Estimate placebo effect for each state. Technically, i should drop MH first but it doesn't make much of a difference
placebo_effects <- function(i) {
  dataprep.temp <- dataprep(foo = as.data.frame(aser),
           predictors = c("std3_read", "std5_read"),
           predictors.op = "mean",
           time.predictors.prior = 2007:2013,
           special.predictors = list(
             list("std5_read", 2012:2013, "mean"),
             list("std3_read", 2013, "mean"),
             list("std5_math", 2014:2015, "mean")
           ),
           dependent = "std5_read",
           unit.variable = "id",
           unit.names.variable = "state_abbr",
           time.variable = "year",
           treatment.identifier = 11,
           controls.identifier = seq(1,max(aser$id),1)[-11],
           time.optimize.ssr = 2007:2013,
           time.plot = 2007:2015)
  synth.out <- synth(data.prep.obj = dataprep.temp, custom.v = v)
  gaps <- dataprep.temp$Y1plot - (dataprep.temp$Y0plot %*% synth.out$solution.w)
  return(gaps[length(gaps)])
}
effects <- map_dbl(1:max(aser$id), placebo_effects)
```

Create a graph showing the effects.

```{r}
results <- aser %>% count(state_abbr, id) %>% 
  arrange(id) %>% select(-n) %>% 
  cbind(effects) %>% rename("effect" =...3 )
```















